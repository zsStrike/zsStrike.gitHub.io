---
title: 通过降维压缩数据
date: 2020-02-04 17:28:09
tags: ["机器学习"]
mathjax: true
---

在本章中，我们将会学习到三种**特征提取**的方法，它们都可以将原始数据集变换到一个维度更低的新的特征子空间。

<!-- More -->

## 无监督数据降维技术之主成分分析

**主成分分析**（PCA）是一种广泛应用于不同领域的无监督线性数据转换技术，突出作用是降维。PCA的目标是在高维数据中找到最大方差的方向，并且将数据映射到一个维度不大于原始数据的新的子空间上。

如果使用PCA技术，我们需要构建一个$ d * k $维的转换矩阵$ W $，从而将原来的d维特征向量转换为k维特征向量（k<d）。PCA算法的步骤如下：

1. 对原始d维数据做标准化处理
2. 构造样本的协方差矩阵
3. 计算协方差矩阵的特征值和相应的特征向量
4. 选择前k个最大特征对应的特征向量（k为新的特征空间维度）
5. 通过前k个特征向量构建映射矩阵$ W $
6. 将原始的d维特征$ x $通过$ W $转换为新的k维特征$ x' $

### 总体方差和贡献方差

这一小节完成PCA的前四个步骤。

首先，使用前面用到的葡萄酒数据集：

```python
import pandas as pd
df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)
```

接着，将数据集划分为训练集和测试集，同时使用`StandardScaler`来将其标准化：

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
sc = StandardScaler()
X_train_std = sc.fit_transform(X_train)
X_test_std = sc.fit_transform(X_test)
```

接下来构造协方差矩阵，同时求解协方差矩阵的特征值和特征向量：

```python
import numpy as np
cov_mat = np.cov(X_train_std.T)
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
print(eigen_vals)
>> [4.8923083  2.46635032 1.42809973 1.01233462 0.84906459 0.60181514
>> 0.52251546 0.08414846 0.33051429 0.29595018 0.16831254 0.21432212
>> 0.2399553 ]
```

通过使用`np.linalg.elg`函数，可以得到一个包含有13个特征值的向量（eigen_vals）和一个13 * 13的特征矩阵（eigen_vecs），其中，特征向量以列的方式存在于特征矩阵中。

由于我们需要将数据压缩到一个新的特征子空间上实现降维，我们只需要选择那些包含最多信息的特征向量组成的子集。在此衡量函数是特征值$ \lambda_j $的方差贡献率：
$$
\frac{\lambda_j}{\sum_{i=1}^{d}j}
$$
接下来看一下不同特征值对应的方差贡献率：

```python
tot = sum(eigen_vals)
var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]
print(var_exp)
>> [0.3732964772349068, 0.18818926106599568, 0.10896790724757796, 0.07724389477124863, 0.0647859460182618, 0.045920138114781475, 0.03986935597634714, 0.025219142607261574, 0.022581806817679666, 0.01830924471952691, 0.016353362655051454, 0.01284270583749274, 0.006420756933868311]
```

可以知道，第一主成分占方差总和的$ 40\% $左右。

### 特征转换

接下来继续执行PCA方法的最后三个步骤。

首先，按照特征值的降序排列特征对：

```python
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]
eigen_pairs.sort(reverse=True)
```

接下来，我们只选择两个对应的最大的特征向量：

```python
w = np.hstack((eigen_pairs[0][1][:, np.newaxis],
              eigen_pairs[1][1][:, np.newaxis]))
print(w)
>> [[ 0.14669811  0.50417079]
>>  [-0.24224554  0.24216889]
>>  [-0.02993442  0.28698484]
>>  [-0.25519002 -0.06468718]
>>  [ 0.12079772  0.22995385]
>>  [ 0.38934455  0.09363991]
>>  [ 0.42326486  0.01088622]
>>  [-0.30634956  0.01870216]
>>  [ 0.30572219  0.03040352]
>>  [-0.09869191  0.54527081]
>>  [ 0.30032535 -0.27924322]
>>  [ 0.36821154 -0.174365  ]
>>  [ 0.29259713  0.36315461]]
```

从而我们现在得到了一个13*2的映射矩阵$ W $。接下来转换原始的数据集：

```python
X_train_pca = X_train_std.dot(w)
```

最后，新的数据集被保存在124*2的矩阵中，接下来对其进行可视化：

```python
import matplotlib.pyplot as plt
colors = ['r', 'b', 'g']
markers = ['s', 'x', 'o']
for l, c, m in zip(np.unique(y_train), colors, markers):
    plt.scatter(X_train_pca[y_train==l, 0], X_train_pca[y_train==l, 1], c=c, label=l, marker=m)
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.legend()
plt.show()
```

得到的图像如下：

![img](./Tue,%2004%20Feb%202020%20184309.png)

从上图可以很直观的看到，线性分类器能够对其有很好的划分。

### 使用scikit-learn进行主成分分析

我们先使用PCA对葡萄酒数据做预处理，然后再使用逻辑斯蒂回归模型对转换后的数据进行分类，最后绘制出散点图：

