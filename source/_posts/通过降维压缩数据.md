---
title: 通过降维压缩数据
date: 2020-02-04 17:28:09
tags: ["机器学习"]
mathjax: true
---

在本章中，我们将会学习到三种**特征提取**的方法，它们都可以将原始数据集变换到一个维度更低的新的特征子空间。

<!-- More -->

## 无监督数据降维技术之主成分分析

**主成分分析**（PCA）是一种广泛应用于不同领域的无监督线性数据转换技术，突出作用是降维。PCA的目标是在高维数据中找到最大方差的方向，并且将数据映射到一个维度不大于原始数据的新的子空间上。

如果使用PCA技术，我们需要构建一个$ d * k $维的转换矩阵$ W $，从而将原来的d维特征向量转换为k维特征向量（k<d）。PCA算法的步骤如下：

1. 对原始d维数据做标准化处理
2. 构造样本的协方差矩阵
3. 计算协方差矩阵的特征值和相应的特征向量
4. 选择前k个最大特征对应的特征向量（k为新的特征空间维度）
5. 通过前k个特征向量构建映射矩阵$ W $
6. 将原始的d维特征$ x $通过$ W $转换为新的k维特征$ x' $

### 总体方差和贡献方差

这一小节完成PCA的前四个步骤。

首先，使用前面用到的葡萄酒数据集：

```python
import pandas as pd
df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)
```

接着，将数据集划分为训练集和测试集，同时使用`StandardScaler`来将其标准化：

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
sc = StandardScaler()
X_train_std = sc.fit_transform(X_train)
X_test_std = sc.fit_transform(X_test)
```

接下来构造协方差矩阵，同时求解协方差矩阵的特征值和特征向量：

```python
import numpy as np
cov_mat = np.cov(X_train_std.T)
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
print(eigen_vals)
>> [4.8923083  2.46635032 1.42809973 1.01233462 0.84906459 0.60181514
>> 0.52251546 0.08414846 0.33051429 0.29595018 0.16831254 0.21432212
>> 0.2399553 ]
```

通过使用`np.linalg.elg`函数，可以得到一个包含有13个特征值的向量（eigen_vals）和一个13 * 13的特征矩阵（eigen_vecs），其中，特征向量以列的方式存在于特征矩阵中。

由于我们需要将数据压缩到一个新的特征子空间上实现降维，我们只需要选择那些包含最多信息的特征向量组成的子集。在此衡量函数是特征值$ \lambda_j $的方差贡献率：
$$
\frac{\lambda_j}{\sum_{i=1}^{d}j}
$$
接下来看一下不同特征值对应的方差贡献率：

```python
tot = sum(eigen_vals)
var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]
print(var_exp)
>> [0.3732964772349068, 0.18818926106599568, 0.10896790724757796, 0.07724389477124863, 0.0647859460182618, 0.045920138114781475, 0.03986935597634714, 0.025219142607261574, 0.022581806817679666, 0.01830924471952691, 0.016353362655051454, 0.01284270583749274, 0.006420756933868311]
```

可以知道，第一主成分占方差总和的$ 40\% $左右。

### 特征转换

接下来继续执行PCA方法的最后三个步骤。

首先，按照特征值的降序排列特征对：

```python
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]
eigen_pairs.sort(reverse=True)
```

接下来，我们只选择两个对应的最大的特征向量：

```python
w = np.hstack((eigen_pairs[0][1][:, np.newaxis],
              eigen_pairs[1][1][:, np.newaxis]))
print(w)
>> [[ 0.14669811  0.50417079]
>>  [-0.24224554  0.24216889]
>>  [-0.02993442  0.28698484]
>>  [-0.25519002 -0.06468718]
>>  [ 0.12079772  0.22995385]
>>  [ 0.38934455  0.09363991]
>>  [ 0.42326486  0.01088622]
>>  [-0.30634956  0.01870216]
>>  [ 0.30572219  0.03040352]
>>  [-0.09869191  0.54527081]
>>  [ 0.30032535 -0.27924322]
>>  [ 0.36821154 -0.174365  ]
>>  [ 0.29259713  0.36315461]]
```

从而我们现在得到了一个13*2的映射矩阵$ W $。接下来转换原始的数据集：

```python
X_train_pca = X_train_std.dot(w)
```

最后，新的数据集被保存在124*2的矩阵中，接下来对其进行可视化：

```python
import matplotlib.pyplot as plt
colors = ['r', 'b', 'g']
markers = ['s', 'x', 'o']
for l, c, m in zip(np.unique(y_train), colors, markers):
    plt.scatter(X_train_pca[y_train==l, 0], X_train_pca[y_train==l, 1], c=c, label=l, marker=m)
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.legend()
plt.show()
```

得到的图像如下：

![img](./Tue,%2004%20Feb%202020%20184309.png)

从上图可以很直观的看到，线性分类器能够对其有很好的划分。

### 使用scikit-learn进行主成分分析

我们先使用PCA对葡萄酒数据做预处理，然后再使用逻辑斯蒂回归模型对转换后的数据进行分类，最后绘制出散点图：

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_std)
X_test_pca = pca.transform(X_test_std)

import matplotlib.pyplot as plt
colors = ['r', 'b', 'g']
markers = ['s', 'x', 'o']
for l, c, m in zip(np.unique(y_train), colors, markers):
    plt.scatter(X_train_pca[y_train==l, 0], X_train_pca[y_train==l, 1], c=c, label=l, marker=m)
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.legend()
plt.show()
```

得到的图像如下：

![img](./Tue,%2004%20Feb%202020%20193047.png)

比较该图和上一节中的图像，可以发现上图实际上就是我们自己完成的PCA图沿着PC1轴翻转的结果。出现此差异的原因在于特征分析方法：特征向量为正或者为负。

接下来使用逻辑斯蒂回归模型进行训练，并且得到训练结果：

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

lr = LogisticRegression()
lr.fit(X_train_pca, y_train)
y_pred = lr.predict(X_test_pca)
accuracy_score(y_test, y_pred)
>> 0.9814814814814815
```

可以发现的逻辑斯蒂回归模型的拟合率很优良。

## 通过线性判别分析压缩无监督数据

**线性判别分析**（LDA）是一种可作为特征抽取的技术，它可以提高数据分析过程中的计算效率，同时，对于不适用于正则化的模型，它可以降低因维度灾难带来的过拟合。

LDA方法的步骤如下：

1. 对d为数据集进行标准化处理
2. 对于每一类别，计算d维的均值向量
3. 构造类间的散布矩阵$ S_{B} $以及类内的散布举证$ S_{W} $
4. 计算矩阵$ s_{W}^{-1}S_{B} $的特征值及对应的特征向量
5. 选取前k个特征值对应的特征向量，构造一个d*k维的转换矩阵$ W $
6. 使用转换矩阵$ W $将样本映射到新的特征子空间中

### 计算散布矩阵

葡萄酒数据我们已经经过标准化处理，接下来求解均值向量$ m_i $：

```python
np.set_printoptions(precision=4)
mean_vecs = []
for label in range(1, 4):
    mean_vecs.append(np.mean(X_train_std[y_train==label], axis=0))
print(mean_vecs)
>> [array([ 0.9259, -0.3091,  0.2592, -0.7989,  0.3039,  0.9608,  1.0515,
>>         -0.6306,  0.5354,  0.2209,  0.4855,  0.798 ,  1.2017]),
>>  array([-0.8727, -0.3854, -0.4437,  0.2481, -0.2409, -0.1059,  0.0187,
>>         -0.0164,  0.1095, -0.8796,  0.4392,  0.2776, -0.7016]),
>>  array([ 0.1637,  0.8929,  0.3249,  0.5658, -0.01  , -0.9499, -1.228 ,
>>          0.7436, -0.7652,  0.979 , -1.1698, -1.3007, -0.3912])]
```

通过均值向量，我们计算一下类内散布矩阵$ S_W $:
$$
S_W = \sum_{i=1}^cS_i
$$
这可以通过累加各类别i的散步矩阵$ S_i $来计算：
$$
S_i = \sum_{x \in D_i}^{c}(x-m_i)(x-m_i)^T
$$

```python
d = 13
S_W = np.zeros((d, d))
for label, mv in zip(range(1, 4), mean_vecs):
    class_scater = np.zeros((d, d))
    for row in X[y==label]:
        row, mv = row.reshape(d, 1), mv.reshape(d, 1)
        class_scater += (row - mv).dot((row - mv).T)
    S_W += class_scater
S_W.shape
>> (13, 13)
```

此前，我们假定对散步矩阵计算时，曾假设训练集的类标是均匀分布的，但是，通过以下程序，我们发现其不遵守这个假设：

```python
np.bincount(y_train)
>> array([ 0, 40, 49, 35], dtype=int64)
```

因此，在我们通过累加方式计算散布矩阵$ S_{W} $前，需要对各类别的散步矩阵$ S_i $做缩放处理。但采用此种方式时，此时散布矩阵和协方差矩阵计算方式相同。协方差矩阵可以看作是归一化的散布矩阵：
$$
\frac{1}{N_i}S_{W} = \frac{1}{N_i}\sum_{x \in D_i}^c(x-m_i)(x-m_i)^T
$$

```python
d = 13
S_W = np.zeros((d, d))
for label, mv in zip(range(1, 4), mean_vecs):
    class_scater = np.cov(X_train_std[y_train==label].T)
    S_W += class_scater
S_W.shape
>> (13, 13)
```

接下来计算类间散布矩阵$ S_B $:
$$
S_B = \sum_{i=1}^cN_i(m_i-m)(m_i-m)^T
$$
其中，m是全局均值，他在计算时用到了所有类别中的全部样本：

```python
mean_overall = np.mean(X_train_std, axis=0)
d = 13
S_B = np.zeros((d, d))
for i, mean_vec in enumerate(mean_vecs):
    n = X[y==i+1, :].shape[0]
    mean_vec = mean_vec.reshape(d, 1)
    mean_overall = mean_overall.reshape(d, 1)
    S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)
S_B.shape
>> (13, 13)
```

### 在新特征子空间上选取线性判别算法

LDA余下的步骤和PCA的步骤相似：

```python
eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]
eigen_pairs = sorted(eigen_pairs, key=lambda k: k[0], reverse=True)
for eigen_pair in eigen_pairs:
    print(eigen_pair[0])
```

得到的结果如下：

```python
643.0153843460517
225.08698185416256
8.002675183788468e-14
5.757534614184537e-14
3.5105079604736804e-14
3.4638958368304884e-14
2.587811510007498e-14
2.587811510007498e-14
2.4449817310582036e-14
1.6532199129716054e-14
8.331225171347768e-15
2.3238388797036527e-15
6.522430076120113e-16
```

从上述输出来看，我们只得到了两个非零特征值（实际得到的3-13个特征值并未严格为0，这是由numpy的浮点数运算导致的），说明只有前面两个特征值对应的特征几乎包含了葡萄酒训练数据集中的全部有用信息。

接下来构造转换矩阵：

```python
w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real, eigen_pairs[1][1][:, np.newaxis].real))
w
>> array([[-0.0707,  0.3778],
>>        [ 0.0359,  0.2223],
>>        [-0.0263,  0.3813],
>>        [ 0.1875, -0.2955],
>>        [-0.0033, -0.0143],
>>        [ 0.2328, -0.0151],
>>        [-0.7719, -0.2149],
>>        [-0.0803, -0.0726],
>>        [ 0.0896, -0.1767],
>>        [ 0.1815,  0.2909],
>>        [-0.0631, -0.2376],
>>        [-0.3794, -0.0867],
>>        [-0.3355,  0.586 ]])
```

### 将样本映射到新的特征空间

通过上一节中构建的转换矩阵$ W $，我们来对原始数据进行转换：

```python
X_train_lda = X_train_std.dot(w)
colors = ['r', 'b', 'g']
markers = ['s', 'x', 'o']
for l, c, m in zip(np.unique(y_train), colors, markers):
    plt.scatter(X_train_lda[y_train==l, 0], X_train_lda[y_train==l, 1], c=c, label=l, marker=m)
plt.xlabel('LD 1')
plt.ylabel('LD 2')
plt.legend()
plt.show()
```

得到的图像如下：

![img](./Tue,%2004%20Feb%202020%20211842.png)

通过图像可知，三个葡萄酒类在新的特征子空间上是线性可分的。

### 使用scikit-laern进行LDA分析

接下来，看一下scikit-laern中对LDA类的实现：

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=2)
X_train_lda = lda.fit_transform(X_train_std, y_train)
colors = ['r', 'b', 'g']
markers = ['s', 'x', 'o']
for l, c, m in zip(np.unique(y_train), colors, markers):
    plt.scatter(X_train_lda[y_train==l, 0], X_train_lda[y_train==l, 1], c=c, label=l, marker=m)
plt.xlabel('LD 1')
plt.ylabel('LD 2')
plt.legend()
plt.show()
```

得到的图像如下：

![img](./Tue,%2004%20Feb%202020%20212606.png)

此时看一下逻辑斯蒂回归模型的预测准确度：

```python
lr = LogisticRegression()
lr = lr.fit(X_train_lda, y_train)
X_test_lda = lda.fit_transform(X_test_std, y_test)
y_pred = lr.predict(X_test_lda)
accuracy_score(y_pred, y_test)
>> 1.0
```

可以看到，逻辑斯蒂回归模型在测试数据集上对样本分类可谓完美。