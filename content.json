{"meta":{"title":"zsStrike","subtitle":null,"description":null,"author":"zsStrike","url":"http://blog.zsstrike.top","root":"/"},"pages":[{"title":"所有分类","date":"2020-12-17T10:23:43.753Z","updated":"2020-12-17T10:23:43.753Z","comments":true,"path":"categories/index.html","permalink":"http://blog.zsstrike.top/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2020-12-17T10:23:43.754Z","updated":"2020-12-17T10:23:43.754Z","comments":true,"path":"tags/index.html","permalink":"http://blog.zsstrike.top/tags/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2020-12-17T10:23:43.754Z","updated":"2020-12-17T10:23:43.754Z","comments":true,"path":"friends/index.html","permalink":"http://blog.zsstrike.top/friends/index.html","excerpt":"这里写友链上方的内容。","text":"这里写友链上方的内容。 这里可以写友链页面下方的文字备注，例如自己的友链规范、示例等。"},{"title":"Projects","date":"2020-12-17T10:23:43.754Z","updated":"2020-12-17T10:23:43.754Z","comments":true,"path":"projects/index.html","permalink":"http://blog.zsstrike.top/projects/index.html","excerpt":"","text":"Foresyn本项目是参与国际基因工程机器大赛（IGEM）创建的，目的是整合生物学相关数据库（如BiGG），优化用户交互界面，最终实现一个Web端应用，为使用者提供生物实验相关数据以及对实验进行预测。期间担任前端组组长，主要负责设计和实现Web界面，同时同后端人员沟通设计API为前端提供必要的数据。 项目地址：https://github.com/USTCSoftware2019/foresyn Activity-Planning本项目主要是构建一个Web站点用于展示中科大社团活动安排表，供学生们提前了解相关活动的信息，以此提前进行时间安排。该项目数据库使用MongoDB，后端用Nodejs搭配Express框架，使用模板语法渲染视图。 项目地址：https://github.com/zsStrike/activity-planing Rust-FreeRTOSFreeRTOS由C语言编写，鉴于C语言的不安全性，选择相对更安全的Rust语言来重构FreeRTOS系统，以此提供更加安全和稳定的嵌入式实时操作系统。在该项目中主要完成对链表的重构任务，以及协同小组其他成员测试链表结构的正确性和完成其他重构的其他任务。 项目地址：https://github.com/OSH-2019/x-rust-freertos"},{"title":"","date":"2020-12-17T10:23:43.750Z","updated":"2020-12-17T10:23:43.750Z","comments":true,"path":"about/index.html","permalink":"http://blog.zsstrike.top/about/index.html","excerpt":"","text":"我是一个有钻研精神的学生，在校期间专业课程成绩都在A以上。我喜欢探索和追问，对于不了解的东西能够坚持询问和探索。我具有良好的团队合作精神，善于分析和解决问题，对于创新技术有着强烈的求知欲。除此之外，我热爱生活，性格开朗，喜欢交朋友。"},{"title":"","date":"2020-12-17T10:23:43.484Z","updated":"2020-12-17T10:23:43.484Z","comments":true,"path":"404.html","permalink":"http://blog.zsstrike.top/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"","date":"2020-12-17T10:23:43.753Z","updated":"2020-12-17T10:23:43.753Z","comments":true,"path":"assets/css/style.css","permalink":"http://blog.zsstrike.top/assets/css/style.css","excerpt":"","text":"@charset \"utf-8\"; @font-face { font-family: 'Varela Round'; src: url(https://cdn.jsdelivr.net/gh/xaoxuu/cdn-fonts@19.1.7/VarelaRound/VarelaRound-Regular.ttf); font-weight: normal; font-style: normal; } @font-face { font-family: 'Source Sans Pro'; src: url(https://cdn.jsdelivr.net/gh/xaoxuu/cdn-fonts@master/SourceSansPro/SourceSansPro-Regular.ttf); font-weight: normal; font-style: normal; } /*! normalize.css v3.0.2 | MIT License | git.io/normalize */ /** * 1. Set default font family to sans-serif. * 2. Prevent iOS text size adjust after orientation change, without disabling * user zoom. */ html { font-family: sans-serif; /* 1 */ -ms-text-size-adjust: 100%; /* 2 */ -webkit-text-size-adjust: 100%; /* 2 */ } /** * Remove default margin. */ body { margin: 0; } /* HTML5 display definitions ========================================================================== */ /** * Correct `block` display not defined for any HTML5 element in IE 8/9. * Correct `block` display not defined for `details` or `summary` in IE 10/11 * and Firefox. * Correct `block` display not defined for `main` in IE 11. */ article, aside, details, figcaption, figure, footer, header, hgroup, main, menu, nav, section, summary { display: block; } /** * 1. Correct `inline-block` display not defined in IE 8/9. * 2. Normalize vertical alignment of `progress` in Chrome, Firefox, and Opera. */ audio, canvas, progress, video { display: inline-block; /* 1 */ vertical-align: baseline; /* 2 */ } /** * Prevent modern browsers from displaying `audio` without controls. * Remove excess height in iOS 5 devices. */ audio:not([controls]) { display: none; height: 0; } /** * Address `[hidden]` styling not present in IE 8/9/10. * Hide the `template` element in IE 8/9/11, Safari, and Firefox < 22. */ [hidden], template { display: none; } /* Links ========================================================================== */ /** * Remove the gray background color from active links in IE 10. */ a { background-color: transparent; } /** * Improve readability when focused and also mouse hovered in all browsers. */ a:active, a:hover { outline: 0; } /* Text-level semantics ========================================================================== */ /** * Address styling not present in IE 8/9/10/11, Safari, and Chrome. */ abbr[title] { border-bottom: 1px dotted; } /** * Address style set to `bolder` in Firefox 4+, Safari, and Chrome. */ b, strong { font-weight: bold; } /** * Address styling not present in Safari and Chrome. */ dfn { font-style: italic; } /** * Address variable `h1` font-size and margin within `section` and `article` * contexts in Firefox 4+, Safari, and Chrome. */ h1 { font-size: 2em; margin: 0.67em 0; } /** * Address styling not present in IE 8/9. */ mark { background: #ff0; color: #000; } /** * Address inconsistent and variable font size in all browsers. */ small { font-size: 80%; } /** * Prevent `sub` and `sup` affecting `line-height` in all browsers. */ sub, sup { font-size: 75%; line-height: 0; position: relative; vertical-align: baseline; } sup { top: -0.5em; } sub { bottom: -0.25em; } /* Embedded content ========================================================================== */ /** * Remove border when inside `a` element in IE 8/9/10. */ img { border: 0; } /** * Correct overflow not hidden in IE 9/10/11. */ svg:not(:root) { overflow: hidden; } /* Grouping content ========================================================================== */ /** * Address margin not present in IE 8/9 and Safari. */ figure { margin: 1em 40px; } /** * Address differences between Firefox and other browsers. */ hr { -moz-box-sizing: content-box; box-sizing: content-box; height: 0; border: 0; border-radius: 1px; border-bottom: 1px solid rgba(51, 51, 51, 0.1); } /** * Contain overflow in all browsers. */ pre { overflow: auto; } /** * Address odd `em`-unit font size rendering in all browsers. */ code, kbd, pre, samp { font-family: monospace, monospace; font-size: 1em; } /* Forms ========================================================================== */ /** * Known limitation: by default, Chrome and Safari on OS X allow very limited * styling of `select`, unless a `border` property is set. */ /** * 1. Correct color not being inherited. * Known issue: affects color of disabled elements. * 2. Correct font properties not being inherited. * 3. Address margins set differently in Firefox 4+, Safari, and Chrome. */ button, input, optgroup, select, textarea { color: inherit; /* 1 */ font: inherit; /* 2 */ margin: 0; /* 3 */ } /** * Address `overflow` set to `hidden` in IE 8/9/10/11. */ button { overflow: visible; } /** * Address inconsistent `text-transform` inheritance for `button` and `select`. * All other form control elements do not inherit `text-transform` values. * Correct `button` style inheritance in Firefox, IE 8/9/10/11, and Opera. * Correct `select` style inheritance in Firefox. */ button, select { text-transform: none; } /** * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio` * and `video` controls. * 2. Correct inability to style clickable `input` types in iOS. * 3. Improve usability and consistency of cursor style between image-type * `input` and others. */ button, html input[type=\"button\"], input[type=\"reset\"], input[type=\"submit\"] { -webkit-appearance: button; /* 2 */ cursor: pointer; /* 3 */ } /** * Re-set default cursor for disabled elements. */ button[disabled], html input[disabled] { cursor: default; } /** * Remove inner padding and border in Firefox 4+. */ button::-moz-focus-inner, input::-moz-focus-inner { border: 0; padding: 0; } /** * Address Firefox 4+ setting `line-height` on `input` using `!important` in * the UA stylesheet. */ input { line-height: normal; } /** * It's recommended that you don't attempt to style these elements. * Firefox's implementation doesn't respect box-sizing, padding, or width. * * 1. Address box sizing set to `content-box` in IE 8/9/10. * 2. Remove excess padding in IE 8/9/10. */ input[type=\"checkbox\"], input[type=\"radio\"] { box-sizing: border-box; /* 1 */ padding: 0; /* 2 */ } /** * Fix the cursor style for Chrome's increment/decrement buttons. For certain * `font-size` values of the `input`, it causes the cursor style of the * decrement button to change from `default` to `text`. */ input[type=\"number\"]::-webkit-inner-spin-button, input[type=\"number\"]::-webkit-outer-spin-button { height: auto; } /** * 1. Address `appearance` set to `searchfield` in Safari and Chrome. * 2. Address `box-sizing` set to `border-box` in Safari and Chrome * (include `-moz` to future-proof). */ input[type=\"search\"] { -webkit-appearance: textfield; /* 1 */ -moz-box-sizing: content-box; -webkit-box-sizing: content-box; /* 2 */ box-sizing: content-box; } /** * Remove inner padding and search cancel button in Safari and Chrome on OS X. * Safari (but not Chrome) clips the cancel button when the search input has * padding (and `textfield` appearance). */ input[type=\"search\"]::-webkit-search-cancel-button, input[type=\"search\"]::-webkit-search-decoration { -webkit-appearance: none; } /** * Define consistent border, margin, and padding. */ fieldset { border: 1px solid #c0c0c0; margin: 0 2px; padding: 0.35em 0.625em 0.75em; } /** * 1. Correct `color` not being inherited in IE 8/9/10/11. * 2. Remove padding so people aren't caught out if they zero out fieldsets. */ legend { border: 0; /* 1 */ padding: 0; /* 2 */ } /** * Remove default vertical scrollbar in IE 8/9/10/11. */ textarea { overflow: auto; } /** * Don't inherit the `font-weight` (applied by a rule above). * NOTE: the default cannot safely be changed in Chrome and Safari on OS X. */ optgroup { font-weight: bold; } /* Tables ========================================================================== */ /** * Remove most spacing between table cells. */ table { border-collapse: collapse; width: 100%; } table th { background-color: #f7f7f7; } table td, table th { text-align: justify; padding: 4px 8px; border: 1px solid #F4F4F4; } td, th { padding: 0; } /* Basic Settings */ * { box-sizing: border-box; outline: none; margin: 0; padding: 0; } /* My Base */ html { color: #333333; width: 100%; height: 100%; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; font-size: 16px; line-height: 1.5rem; -webkit-text-size-adjust: 100%; -ms-text-size-adjust: 100%; text-rendering: optimizelegibility; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); } body { background-color: #F4F4F4; } body.modal-active { overflow: hidden; } @media (max-width: 680px) { body.modal-active { position: fixed; top: 0; right: 0; bottom: 0; left: 0; } } body.z_menu-open .menu-phone { transform: translate3d(-16px, 0, 0); } fancybox { display: flex; justify-content: center; } .cover-wrapper { padding-bottom: 2px; } .cover-wrapper .cover { top: 0; left: 0; max-width: 100%; height: calc(100vh); display: flex; flex-wrap: nowrap; flex-direction: column; align-items: center; align-self: center; align-content: center; } .cover-wrapper .cover .title, .cover-wrapper .cover .logo { font-size: 48px; margin-top: calc(28vh - 2*16px); text-align: center; font-weight: bold; } .cover-wrapper .cover .title { line-height: calc(24px*2 + 2*16px); } .cover-wrapper .cover .logo { max-height: 100px; max-width: calc(100% - 4*16px); } @media (max-width: 580px) { .cover-wrapper .cover .title, .cover-wrapper .cover .logo { font-size: 48px; line-height: 52.8px; } } .cover-wrapper .cover .m_search { margin-top: calc(2vh + 2*16px); position: relative; max-width: calc(100% - 1*16px); width: 340px; line-height: 48px; vertical-align: middle; } .cover-wrapper .cover .m_search .form { position: relative; display: block; width: 100%; } .cover-wrapper .cover .m_search .icon, .cover-wrapper .cover .m_search .input { transition: all 0.3s ease; -moz-transition: all 0.3s ease; -webkit-transition: all 0.3s ease; -o-transition: all 0.3s ease; } .cover-wrapper .cover .m_search .icon { position: absolute; display: block; line-height: 44px; height: 44px; width: 32px; top: 0; left: 5px; font-size: 16px; color: rgba(51, 51, 51, 0.6); } .cover-wrapper .cover .m_search .input { display: block; font-size: 16px; line-height: 16px; height: 44px; width: 100%; color: #333333; box-shadow: none; box-sizing: border-box; -webkit-appearance: none; padding-left: 36px; border-radius: 64px; background: #ffffff; border: 1px dashed transparent; } @media (max-width: 580px) { .cover-wrapper .cover .m_search .input { padding-left: 36px; } } .cover-wrapper .cover .m_search .input::-webkit-input-placeholder { padding-top: 2px; color: rgba(51, 51, 51, 0.6); } .cover-wrapper .cover .m_search .input:-moz-placeholder { padding-top: 2px; color: rgba(51, 51, 51, 0.6); } .cover-wrapper .cover .m_search .input::-moz-placeholder { padding-top: 2px; color: rgba(51, 51, 51, 0.6); } .cover-wrapper .cover .m_search .input:-ms-input-placeholder { padding-top: 2px; color: rgba(51, 51, 51, 0.6); } .cover-wrapper .cover .m_search .input:hover ~ .icon { color: #1BC3FB; } .cover-wrapper .cover .m_search .input:focus { border: 1px solid #1BC3FB; } .cover-wrapper .cover .m_search .input:focus ~ .icon { color: #1BC3FB; } .cover-wrapper .cover.half { height: calc(60vh - 16px - 64px); } .cover-wrapper .cover.half .title, .cover-wrapper .cover.half .logo { margin-top: calc(22vh - 4*16px); } @media (max-width: 580px) { .cover-wrapper .cover.half { height: calc(45vh - 16px - 64px); } .cover-wrapper .cover.half .title, .cover-wrapper .cover.half .logo { margin-top: calc(24vh - 6*16px); } } .cover-wrapper .cover.half .m_search { margin-top: 16px; } .cover-wrapper .cover, .cover-wrapper .cover a { color: #1BC3FB; } .cover-wrapper .cover .menu { margin-top: 16px; } .cover-wrapper .cover .menu ul { display: flex; flex-wrap: wrap; align-items: baseline; justify-content: center; } .cover-wrapper .cover .menu ul li { display: flex; flex-wrap: wrap; align-items: center; padding: 0; height: auto; } .cover-wrapper .cover .menu ul > li > a { font-size: 14px; padding: 2px; margin: 0 4px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; color: rgba(51, 51, 51, 0.85); border-bottom: 1px solid transparent; } .cover-wrapper .cover .menu ul > li > a:hover, .cover-wrapper .cover .menu ul > li > a.active { color: #1BC3FB; border-bottom: 1px solid #1BC3FB; } .cover-wrapper .cover .switcher > li a:hover { background: rgba(27, 195, 251, 0.15); } .z-depth-nav, .l_header, #u-search .modal .modal-header { box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.24), 0 3px 6px 0px rgba(0, 0, 0, 0.1); } .z-depth-nav-raised, #u-search .modal .modal-header:hover { box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } .z-depth-main, .l_main .post, .widget { box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } .z-depth-main-raised { box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } .z-depth-0 { box-shadow: 0 1px 4px 0 rgba(0, 0, 0, 0.07); } .z-depth-1 { box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.08), 0 2px 4px 0 rgba(0, 0, 0, 0.1); } .z-depth-1-half { box-shadow: 0 2px 3px 0px rgba(0, 0, 0, 0.4), 0 0px 8px 0px rgba(0, 0, 0, 0.2); } .z-depth-2 { box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.1), 0 3px 10px 0 rgba(0, 0, 0, 0.1); } .z-depth-3 { box-shadow: 0 6px 10px 0 rgba(0, 0, 0, 0.12), 0 8px 25px 0 rgba(0, 0, 0, 0.1); } .z-depth-4 { box-shadow: 0 8px 14px 0 rgba(0, 0, 0, 0.11), 0 12px 22px 0 rgba(0, 0, 0, 0.11); } .z-depth-5 { box-shadow: 0 12px 12px 0 rgba(0, 0, 0, 0.1), 0 20px 33px 0 rgba(0, 0, 0, 0.11); } .z-depth-0 { box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.08), 0 2px 4px 0 rgba(0, 0, 0, 0.08); } .hoverable { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; box-shadow: 0; } .hoverable:hover { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; box-shadow: 0 8px 17px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19); } ::-moz-selection { background: rgba(33, 150, 243, 0.2); } ::selection { background: rgba(33, 150, 243, 0.2); } h1, h2, h3, h4, h5, h6 { -webkit-font-feature-settings: 'dlig' 1, 'liga' 1, 'lnum' 1, 'kern' 1; -moz-font-feature-settings: 'dlig' 1, 'liga' 1, 'lnum' 1, 'kern' 1; -o-font-feature-settings: 'dlig' 1, 'liga' 1, 'lnum' 1, 'kern' 1; text-rendering: geometricPrecision; margin: 0 0 0.4em 0; } h1 { font-size: 24px; } h2 { font-size: 24px; } h3 { font-size: 20.8px; } h4 { font-size: 18.4px; } h5 { font-size: 16px; } h6 { font-size: 14px; } a { color: #444444; cursor: pointer; text-decoration: none; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } a:hover { text-decoration: none; } pre { tab-size: 4; -moz-tab-size: 4; -o-tab-size: 4; -webkit-tab-size: 4; } img { max-width: 100%; } /** * Util */ .clearfix { zoom: 1; } .clearfix:before, .clearfix:after { content: \" \"; display: table; } .clearfix:after { clear: both; } .hidden { text-indent: -9999px; visibility: hidden; display: none; } .inner { position: relative; width: 80%; max-width: 710px; margin: 0 auto; } .vertical { display: table-cell; vertical-align: middle; } .right { float: right; } .left { float: left; } .disable-trans { -moz-transition: none !important; -webkit-transition: none !important; transition: none !important; } .txt-ellipsis, .widget .content ul.entry a .name, .widget .content ul.popular-posts a .name { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; } ul, ol { padding-left: 0; } li { list-style: none; } .mark { position: relative; } .mark a { color: #444444; display: inline-block; padding: 0 8px; border-left: 4px solid transparent; background: transparent; border-radius: 4px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .mark a:hover { background: rgba(27, 195, 251, 0.1); border-left: 4px solid #1BC3FB; padding: 8px; } .mark a:active { border-left: 8px solid #1BC3FB; } ul.h-list { display: flex; align-items: center; height: 100%; } ul.h-list > li { height: 100%; justify-content: center; } /** * Loading bar */ #loading-bar-wrapper { position: fixed; top: 62px; left: 0; width: 100%; z-index: 99999; } #loading-bar { position: fixed; width: 0; height: 2px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; background-color: rgba(255, 255, 255, 0.5); } #loading-bar.pure { background-color: rgba(27, 195, 251, 0.5); } .body-wrapper { position: relative; display: flex; width: 100%; max-width: 1080px; margin: 0 auto; flex-wrap: wrap; justify-content: space-between; align-items: stretch; } .container--flex { display: flex; flex-wrap: nowrap; justify-content: space-between; align-items: center; } .l_body { position: relative; margin: 16px; margin-top: 16px; } .l_body.nocover { margin-top: 80px; } @media (max-width: 580px) { .l_body { margin: 80px 0 16px; border-radius: 0; } } .l_body .s-top { transition: all 0.6s ease; -moz-transition: all 0.6s ease; -webkit-transition: all 0.6s ease; -o-transition: all 0.6s ease; z-index: 9; position: fixed; width: 48px; height: 48px; line-height: 48px; border-radius: 100%; bottom: 32px; right: 32px; transform: translateY(100px) scale(0); transform-origin: bottom; color: #333333; } @media (max-width: 768px) { .l_body .s-top { right: 16px; } } .l_body .s-top.show { transform: translateY(0) scale(1); } .l_body .s-top.show.hl { background: #1BC3FB; color: white; box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } @media (min-width: 768px) { .l_body .s-top:hover { transform: scale(1.2); border-radius: 25%; background: #1BC3FB; color: white; box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } .l_body .s-top:hover.hl { box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } } .l_header { position: fixed; z-index: 9999; top: 0; width: 100%; font-size: 16px; line-height: 64px; height: 64px; overflow: hidden; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; padding: 0 16px; margin-bottom: 16px; background: #1BC3FB; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_header .wrapper { padding: auto 16px; max-width: 1080px; margin: auto; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_header .wrapper a.logo { color: #262626; } .l_header.no_sidebar .wrapper { max-width: 768px; margin: auto; } .l_header .wrapper.sub { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; transform: translateY(-64px); } @media (max-width: 580px) { .l_header .wrapper.sub .logo { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; padding-left: 16px; padding-right: 0; font-size: 16px; } } .l_header .nav--main, .l_header .nav-sub { height: 64px; } .l_header.hide { transform: translateY(100px) scale(0); } .l_header.show { transform: translateY(0) scale(1); } .l_header, .l_header a { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; height: 64px; line-height: 64px; color: white; } .l_header .logo { padding: 0 24px; font-size: 19.2px; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; letter-spacing: 0; } @media (max-width: 580px) { .l_header .logo { padding: 0 16px; } } .l_header .logo.img { padding: 0 16px 0 0; } .l_header .logo img { height: 100%; } .l_header img.logo { padding: 4px 0; } .l_header .nav-sub .logo { padding: 0 24px; font-size: 16px; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; } @media (max-width: 580px) { .l_header .nav-sub .logo { letter-spacing: -0.5px; padding-top: 1px; } } .l_header .menu { position: relative; flex: 1 0 auto; height: 64px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; margin: 0 16px 0 0; } .l_header .menu ul > li > a { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; display: block; font-size: 16px; color: rgba(255, 255, 255, 0.7); padding: 0 8px; } .l_header .menu ul > li > a:hover { color: white; border-bottom: 2px solid white; background: rgba(255, 255, 255, 0.1); } .l_header .menu ul > li > a:active, .l_header .menu ul > li > a.active { color: white; border-bottom: 2px solid white; } @media (max-width: 580px) { .l_header .menu { display: none; } } .l_header .switcher { display: none; font-size: 16px; line-height: 64px; } .l_header .switcher .s-toc { display: none; } @media (max-width: 768px) { .l_header .switcher .s-toc { display: block; } } .l_header .switcher > li { height: 48px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; margin: 2px; } @media (max-width: 580px) { .l_header .switcher > li { margin: 0; height: 48px; } } .l_header .switcher > li a { display: flex; justify-content: center; align-items: center; width: 48px; height: 48px; border-radius: 100px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_header .switcher > li a:hover { background: rgba(255, 255, 255, 0.3); } @media (max-width: 580px) { .l_header .switcher > li a { width: 32px; height: 48px; } } @media (max-width: 580px) { .l_header .switcher { display: flex; padding-left: 8px; padding-right: 10px; } } .l_header .nav-sub .switcher { display: flex; } .l_header .m_search { position: relative; display: flex; width: 285px; height: 64px; } @media (max-width: 1350px) { .l_header .m_search { width: 240px; } } .l_header .m_search .form { position: relative; display: block; width: 100%; margin: auto; } .l_header .m_search .icon, .l_header .m_search .input { transition: all 0.3s ease; -moz-transition: all 0.3s ease; -webkit-transition: all 0.3s ease; -o-transition: all 0.3s ease; } .l_header .m_search .icon { position: absolute; display: block; line-height: 40px; height: 40px; width: 32px; top: 0; left: 5px; font-size: 16px; color: rgba(255, 255, 255, 0.6); } .l_header .m_search .input { display: block; font-size: 16px; line-height: 16px; height: 40px; width: 100%; color: rgba(255, 255, 255, 0.6); box-shadow: none; box-sizing: border-box; -webkit-appearance: none; padding-left: 36px; border-radius: 8px; background: rgba(255, 255, 255, 0.15); border: 1px dashed transparent; } @media (max-width: 580px) { .l_header .m_search .input { padding-left: 36px; } } .l_header .m_search .input::-webkit-input-placeholder { padding-top: 2px; color: rgba(255, 255, 255, 0.6); } .l_header .m_search .input:-moz-placeholder { padding-top: 2px; color: rgba(255, 255, 255, 0.6); } .l_header .m_search .input::-moz-placeholder { padding-top: 2px; color: rgba(255, 255, 255, 0.6); } .l_header .m_search .input:-ms-input-placeholder { padding-top: 2px; color: rgba(255, 255, 255, 0.6); } .l_header .m_search .input:hover { color: white; border: 1px solid rgba(255, 255, 255, 0.6); } .l_header .m_search .input:focus { color: white; border: 1px solid white; } .l_header .m_search .input:focus ~ .icon { color: white; } .l_header.pure { background: white; box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } .l_header.pure, .l_header.pure a { color: #1BC3FB; } .l_header.pure .menu ul > li > a { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; color: rgba(51, 51, 51, 0.85); } .l_header.pure .menu ul > li > a.current { border-bottom: 2px solid rgba(27, 195, 251, 0.8); } .l_header.pure .menu ul > li > a:hover { color: #1BC3FB; border-bottom: 2px solid #1BC3FB; background: rgba(27, 195, 251, 0.1); } .l_header.pure .menu ul > li > a:active, .l_header.pure .menu ul > li > a.active { color: #1BC3FB; border-bottom: 2px solid #1BC3FB; } .l_header.pure .switcher > li a:hover { background: rgba(27, 195, 251, 0.15); } .l_header.pure .m_search .icon { color: rgba(51, 51, 51, 0.6); } .l_header.pure .m_search .input { color: #333333; background: #F4F4F4; } .l_header.pure .m_search .input::-webkit-input-placeholder { color: rgba(51, 51, 51, 0.6); } .l_header.pure .m_search .input:-moz-placeholder { color: rgba(51, 51, 51, 0.6); } .l_header.pure .m_search .input::-moz-placeholder { color: rgba(51, 51, 51, 0.6); } .l_header.pure .m_search .input:-ms-input-placeholder { color: rgba(51, 51, 51, 0.6); } .l_header.pure .m_search .input:hover { border: 1px solid rgba(27, 195, 251, 0.6); } .l_header.pure .m_search .input:hover ~ .icon { color: rgba(27, 195, 251, 0.8); } .l_header.pure .m_search .input:focus { color: #333333; background: rgba(27, 195, 251, 0.15); border: 1px solid #1BC3FB; } .l_header.pure .m_search .input:focus ~ .icon { color: #1BC3FB; } @media (max-width: 580px) { .l_header { padding: 0; } .l_header .m_search { width: 0; overflow: hidden; position: absolute; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; margin: 0 8px; } .l_header.z_search-open .logo { opacity: 0; } .l_header.z_search-open .m_search { width: calc(100vw - 2*16px - 2*32px); } } .menu-phone { position: fixed; top: 80px; right: 0; z-index: 10000; line-height: 32px; background: white; border-right: 0; box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); border-radius: 12px; transform: translate3d(-40px, -40px, 0) scale(0, 0); transform-origin: right top; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .menu-phone .header { border-top-left-radius: 12px; border-top-right-radius: 12px; background-color: rgba(27, 195, 251, 0.9); color: white; font-size: 16px; line-height: 1.8em; padding: 8px 22px; } .menu-phone:hover { box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } .menu-phone:active { box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } .menu-phone nav { padding: 8px 0px; } .menu-phone nav .nav { height: 36px; line-height: 36px; position: relative; display: block; color: #444444; padding: 2px 20px; border-left: 4px solid transparent; border-right: 4px solid transparent; } .menu-phone nav .nav:hover, .menu-phone nav .nav.active { border-left: 4px solid #1BC3FB; background: rgba(27, 195, 251, 0.1); } .cover-wrapper .l_header { transition: all 0.5s ease; -moz-transition: all 0.5s ease; -webkit-transition: all 0.5s ease; -o-transition: all 0.5s ease; transform: translateY(-96px); } .cover-wrapper .l_header.show { transform: translateY(0); } .l_main { width: calc(100% - 1 * 285px); padding-right: 16px; float: left; } @media (max-width: 1350px) { .l_main { width: calc(100% - 1 * 240px); } } @media (max-width: 768px) { .l_main { width: 100%; } } .l_main.no_sidebar { width: 100%; padding-right: 0; max-width: 768px; margin: auto; } .l_main.no_sidebar ~ .l_side { display: none; } .l_main .post-list { position: relative; margin: 0px auto; column-gap: 0; } @media (max-width: 580px) { .l_main .post-list { margin: 0; } } .l_main ul.popular-posts h3 { padding: 0; margin: 0; font-size: 16px; } .l_main #comments { position: relative; } @media (max-width: 580px) { } .l_main #comments #valine_container p { line-height: 1.7; } .l_main #comments #valine_container p img { max-height: 28px; } .l_main #comments #valine_container img { display: inline; } .l_main #comments #valine_container .vwrap { border-radius: 12px; border-style: dashed; border: 1px dashed rgba(51, 51, 51, 0.3); transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_main #comments #valine_container .vwrap:hover { border: 1px dashed #1bc3fb; } .l_main #comments #valine_container .vwrap .vheader .vinput { border-radius: 0; border-bottom: 1px dashed rgba(51, 51, 51, 0.3); } .l_main #comments #valine_container .vwrap .vheader .vinput:hover { border-bottom: 1px dashed #1BC3FB; } .l_main #comments #valine_container .vwrap .vheader .vinput:focus { border-bottom: 1px solid #1BC3FB; } .l_main #comments #valine_container .vwrap .vedit .vctrl span { color: #1BC3FB; padding: 0; margin: 10px; } .l_main #comments #valine_container button { border: none; padding-left: 2.4em; padding-right: 2.4em; font-weight: bold; background-color: #1BC3FB; color: white; border-radius: 6px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_main #comments #valine_container button:hover { background: #04a8df; } .l_main #comments #valine_container blockquote { padding: 16px; border-left: 4px solid #1BC3FB; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_main #comments #valine_container blockquote p { text-align: left; word-wrap: normal; margin: 0; font-size: 14px; line-height: 21px; } .l_main #comments #valine_container pre code { border: none; } .l_main #comments #valine_container code { font-family: 'Ubuntu', Menlo, Monaco, courier, \"Lucida Console\", monospace, 'Source Code Pro', \"Microsoft YaHei\", Helvetica, Arial, sans-serif; font-size: 12.8px; color: rgba(51, 51, 51, 0.9); } .l_main #comments #valine_container a, .l_main #comments #valine_container .vemoji-btn, .l_main #comments #valine_container .vpreview-btn { color: #1BC3FB; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_main #comments #valine_container a:hover, .l_main #comments #valine_container .vemoji-btn:hover, .l_main #comments #valine_container .vpreview-btn:hover { color: #ff5722; text-decoration: underline; } .l_main #comments #valine_container a:active, .l_main #comments #valine_container .vemoji-btn:active, .l_main #comments #valine_container .vpreview-btn:active { color: #a22700; } .l_main #comments #valine_container .vhead span.vnick { color: rgba(51, 51, 51, 0.9); } .l_main #comments #valine_container .vhead a.vnick { color: #ff9800; font-weight: bold; } .l_main #comments #valine_container .vhead a.vnick:hover { color: #ff5722; text-decoration: underline; } .l_main #comments #valine_container .vhead .vsys { margin: 2px; padding: 1px 8px; background-color: rgba(51, 51, 51, 0.1); } .l_main #comments #valine_container .vcard .vquote { border-left: none; } .l_main #comments #valine_container .vcard .vh { border-bottom: 1px dashed rgba(51, 51, 51, 0.1); } .l_main #comments #valine_container .vmeta .vat { font-weight: bold; color: #1BC3FB; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_main #comments #valine_container .vmeta .vat:hover { color: #ff5722; text-decoration: underline; } .l_main #comments #valine_container .vmeta .vat:active { color: #a22700; } .l_main #comments #valine_container .vinput { color: #333333; } .l_main #comments #valine_container p { color: #333333; } .l_main #comments .vemojis { justify-content: space-between; } .l_main #comments .vemojis i { width: auto; height: 36px; padding: 0; margin: 8px 8px 0 8px; } .l_main #comments .vemojis i .emoji { height: 24px; margin-top: 6px; background: transparent; } .l_main #comments p .emoji { display: inline; height: 28px; background: transparent; } .l_main .post-wrapper { column-break-inside: avoid; break-inside: avoid-column; } .l_main .post-wrapper { margin-bottom: 16px; } .l_main .post-wrapper .post .meta { margin-bottom: 16px; } .l_main .post-wrapper .post .meta .title { font-size: 24px; } .l_main .post-wrapper .post .meta .title a { font-size: 24px; } .l_main .post-wrapper .post .full-width { margin-left: -24px; margin-right: -24px; width: calc(100% + 3 * 16px); } .l_main .post-wrapper .post .auto-padding { padding-left: 24px; padding-right: 24px; border-bottom-left-radius: 12px; border-bottom-right-radius: 12px; overflow: auto; } .l_main .post-wrapper .tags { margin-bottom: -32px; } @media (max-width: 580px) { .l_main .post-wrapper .tags { margin-bottom: -24px; } .l_main .post-wrapper .post .meta { margin-bottom: 16px; } .l_main .post-wrapper .post .meta .title { font-size: 24px; } .l_main .post-wrapper .post .full-width { margin-left: -16px; margin-right: -16px; padding-left: 16px; padding-right: 16px; width: calc(100% + 2 * 16px); } .l_main .post-wrapper .post .auto-padding { padding-left: 16px; padding-right: 16px; border-bottom-left-radius: 12px; border-bottom-right-radius: 12px; overflow: auto; } .l_main .post-wrapper .post .highlight { margin-left: 0px; margin-right: 0px; width: calc(100% - 0 * 16px); } } @media (max-width: 580px) and (max-width: 580px) { .l_main .post-wrapper .post { padding: 24px 16px; } .l_main .post-wrapper .post .highlight { margin-left: 0px; margin-right: 0px; width: calc(100% - 0 * 16px); } .l_main .post-wrapper .post .auto-padding { border-bottom-left-radius: 0; border-bottom-right-radius: 0; } } @media (max-width: 580px) { .l_main .widget { border-radius: 0; margin-left: 0; margin-right: 0; width: auto; } .l_main .widget:hover { box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } } .l_main .post { position: relative; margin: 16px auto; padding: 32px 24px; background: white; border-radius: 12px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_main .post h1 { font-weight: normal; font-size: 28.8px; line-height: 1.7; color: #333333; } .l_main .post:hover { box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } .l_main .post:active { box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } @media (max-width: 580px) { .l_main .post { border-radius: 0; } .l_main .post:hover { box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } } .l_main .post .tags a { color: rgba(51, 51, 51, 0.7); } .l_main .post .meta { color: rgba(51, 51, 51, 0.7); font-size: 13.3px; } .l_main .post .meta#header-meta { margin-top: 0; margin-bottom: 16px; } .l_main .post .meta#footer-meta { margin-top: 32px; margin-bottom: 8px; } .l_main .post .meta .aplayer, .l_main .post .meta .thumbnail { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; width: 65px; height: 65px; border-radius: 100%; float: right; margin: 4px; box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } .l_main .post .meta .aplayer:hover, .l_main .post .meta .thumbnail:hover { border-radius: 25%; transform: scale(1.1); box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } @media (max-width: 580px) { .l_main .post .meta .aplayer:hover, .l_main .post .meta .thumbnail:hover { border-radius: 100%; transform: scale(1); box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } } .l_main .post .meta .thumbnail { width: auto; border-radius: 4px; box-shadow: none; } .l_main .post .meta .thumbnail:hover { border-radius: 4px; transform: scale(1.1) rotate(4deg); box-shadow: none; } .l_main .post .meta .title { text-align: left; font-size: 28.8px; margin: 0; } @media (max-width: 580px) { .l_main .post .meta .title { font-size: 24px; } } .l_main .post .meta .title a { display: inline; line-height: 1.7; font-weight: normal; color: #333333; text-decoration: none; font-size: 28.8px; } @media (max-width: 580px) { .l_main .post .meta .title a { font-size: 24px; } } .l_main .post .meta .title a:hover { color: #ff5722; } .l_main .post .meta .new-meta-box { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; padding-top: 4px; padding-bottom: 8px; display: flex; align-items: center; flex-wrap: wrap; } .l_main .post .meta .new-meta-box .new-meta-item { color: rgba(51, 51, 51, 0.7); font-size: 14px; line-height: 24px; display: flex; align-items: center; justify-content: center; padding: 2px; margin: 0 8px 0 0; border-radius: 4px; } .l_main .post .meta .new-meta-box .new-meta-item .notlink { cursor: default; } .l_main .post .meta .new-meta-box .new-meta-item .notlink:hover { color: rgba(51, 51, 51, 0.7); } .l_main .post .meta .new-meta-box .new-meta-item .notlink:hover p { color: rgba(51, 51, 51, 0.7); } .l_main .post .meta .new-meta-box .new-meta-item:last-child { margin-right: 0; } .l_main .post .meta .new-meta-box .new-meta-item img, .l_main .post .meta .new-meta-box .new-meta-item i { border-radius: 100%; display: inline; } .l_main .post .meta .new-meta-box .new-meta-item i { margin-right: 4px; border-radius: 0; } .l_main .post .meta .new-meta-box .new-meta-item i.fa-hashtag { margin-right: 1px; } .l_main .post .meta .new-meta-box .new-meta-item p, .l_main .post .meta .new-meta-box .new-meta-item a { color: rgba(51, 51, 51, 0.7); padding-left: 0; padding-right: 4px; } .l_main .post .meta .new-meta-box .new-meta-item a { font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; display: flex; justify-content: center; align-items: center; } .l_main .post .meta .new-meta-box .new-meta-item a img { height: 17px; width: 17px; margin-right: 5px; transform: translateY(-1px); } .l_main .post .meta .new-meta-box .new-meta-item a p { margin: 0; padding-top: 2px; font-weight: normal; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_main .post .meta .new-meta-box .new-meta-item a:hover { color: #ff5722; text-decoration: none; } .l_main .post .meta .new-meta-box .new-meta-item a:hover p { color: #ff5722; } .l_main .post .meta .new-meta-box .share-body { height: 22px; display: flex; } .l_main .post .meta .new-meta-box .share-body a { padding: 0; margin-right: 4px; } .l_main .post .meta .new-meta-box .share-body a img { height: 22px; width: auto; background: transparent; } .l_main .post .full-width, .l_main .post .highlight { margin-left: 0px; margin-right: 0px; width: calc(100% - 0 * 16px); } .l_main .post img { display: flex; justify-content: center; align-items: center; max-width: 100%; border-radius: 4px; background: none; } .l_main .post span img { display: inline-block; } .l_main .post a img { display: inline; } @media (max-width: 768px) { .l_main { padding-right: 0; } .l_main .post .meta { margin-bottom: 16px; } .l_main .post .meta .title { font-size: 24px; } .l_main .post .full-width { margin-left: -16px; margin-right: -16px; padding-left: 16px; padding-right: 16px; width: calc(100% + 2 * 16px); } .l_main .post .auto-padding { padding-left: 16px; padding-right: 16px; border-bottom-left-radius: 12px; border-bottom-right-radius: 12px; overflow: auto; } .l_main .post .highlight { margin-left: 0px; margin-right: 0px; width: calc(100% - 0 * 16px); } } @media (max-width: 768px) and (max-width: 580px) { .l_main { width: 100%; } } @media (max-width: 768px) and (max-width: 580px) { .l_main .post { padding: 24px 16px; } .l_main .post .highlight { margin-left: 0px; margin-right: 0px; width: calc(100% - 0 * 16px); } .l_main .post .auto-padding { border-bottom-left-radius: 0; border-bottom-right-radius: 0; } } .l_main .prev-next { width: 100%; display: flex; justify-content: space-between; align-items: baseline; color: rgba(51, 51, 51, 0.5); margin: 0; } .l_main .prev-next .prev { text-align: left; border-top-right-radius: 32px; border-bottom-right-radius: 32px; } .l_main .prev-next .next { text-align: right; border-top-left-radius: 32px; border-bottom-left-radius: 32px; } .l_main .prev-next p { margin: 16px; } .l_main .prev-next a { color: rgba(27, 195, 251, 0.9); } .l_main .prev-next section { color: rgba(51, 51, 51, 0.8); padding: 16px; border-radius: 12px; } .l_main .prev-next section:hover { color: #ff5722; } @media (max-width: 580px) { .l_main .prev-next section { border-radius: 0; } } .alert { display: none; position: fixed; top: 50%; left: 50%; transform: translate(-50%, -50%); z-index: 99999; text-align: center; padding: 24px 36px; border-radius: 4px; box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; font-weight: bold; font-size: 16px; } .alert.alert-success { color: #3c763d; background-color: #dff0d8; border-color: #d6e9c6; } .alert.alert-info { color: #31708f; background-color: #d9edf7; border-color: #bce8f1; } .alert.alert-warning { color: #8a6d3b; background-color: #fcf8e3; border-color: #faebcc; } .alert.alert-danger { color: #a94442; background-color: #f2dede; border-color: #ebccd1; } .l_side { width: 285px; float: right; position: relative; display: flex; flex-direction: column; } @media (max-width: 1350px) { .l_side { width: 240px; } } @media (max-width: 768px) { .l_side { width: 100%; } } .widget { z-index: 0; background: white; margin-top: 16px; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; font-size: 16px; border-radius: 12px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; width: 100%; max-height: calc(100% - 64px - 4 * 16px); } .widget:hover { box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } .widget:active { box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } @media (max-width: 580px) { .widget { width: calc(100% - 2 * 16px); margin: 16px 16px 0 16px; } } .widget header { display: flex; justify-content: space-between; border-top-left-radius: 12px; border-top-right-radius: 12px; background-color: #1BC3FB; color: white; font-weight: bold; line-height: 1.5em; padding: 8px 16px; } .widget header .rightBtn { color: white; } .widget header .rightBtn:hover { color: #037094; } .widget header .rightBtn:hover.rotate90 { transform: rotate(90deg); } .widget header.pure { background-color: white; color: #1BC3FB; padding-top: 14px; padding-bottom: 14px; } .widget header.pure .rightBtn { color: #1BC3FB; } .widget header.pure .rightBtn:hover { color: #037094; } .widget .content { text-align: justify; padding: 8px; max-height: calc(100% - 64px - 12.5 * 16px); } .widget .content ul > li a { color: rgba(51, 51, 51, 0.8); padding: 0 16px; line-height: 36px; display: flex; justify-content: space-between; align-content: center; border-left: 2px solid transparent; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .widget .content ul.entry a .name, .widget .content ul.popular-posts a .name { flex: auto; color: rgba(51, 51, 51, 0.8); } .widget .content ul.entry a .badge, .widget .content ul.popular-posts a .badge { flex: none; font-weight: normal; font-size: 14px; color: rgba(51, 51, 51, 0.7); } .widget .content ul.entry a:hover, .widget .content ul.popular-posts a:hover { border-left: 4px solid #1BC3FB; background: rgba(27, 195, 251, 0.1); } .widget .content ul.entry a:active, .widget .content ul.popular-posts a:active { border-left: 8px solid #1BC3FB; } .widget .content ul.entry a.child, .widget .content ul.popular-posts a.child { padding-left: 32px; } .widget.author { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .widget.author .content { padding: 0; } .widget.author .content div.avatar { display: flex; justify-content: center; } .widget.author .content img { padding: 0; margin: 0; display: flex; justify-content: center; width: 285px; height: 285px; border-top-left-radius: 12px; border-top-right-radius: 12px; } @media (max-width: 1350px) { .widget.author .content img { width: 240px; height: 240px; } } @media (max-width: 768px) { .widget.author .content img { width: 96px; height: 96px; border-radius: 100%; margin-top: 8px; padding: 8px; } } .widget.author .content h2 { text-align: center; font-weight: bold; margin: 8px; } @media (max-width: 768px) { .widget.author .content h2 { margin: 8px; } } .widget.author .content p { font-size: 16px; font-weight: bold; text-align: center; margin: 8px 8px 0 8px; empty-cells: hide; } .widget.author .content .social-wrapper { display: flex; justify-content: space-between; flex-wrap: wrap; margin: 4px 8px; } .widget.author .content .social-wrapper a { color: rgba(51, 51, 51, 0.7); padding: 0; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .widget.author .content .social-wrapper a:hover { color: #ff5722; } .widget.author .content .social-wrapper a.social { display: flex; justify-content: center; align-items: center; width: 32px; height: 32px; margin: 4px; border-radius: 100px; } .widget.author .content .social-wrapper a.social:hover { background: rgba(27, 195, 251, 0.1); color: #1BC3FB; } @media (max-width: 768px) { .widget.author .content .social-wrapper { justify-content: center; display: none; } } @media (max-width: 768px) { .widget.author { box-shadow: none; background: #F4F4F4; margin-top: 32px; } } .widget.plain .content { font-size: 14px; font-weight: bold; word-break: break-all; padding: 8px 16px; line-height: 22px; } .widget.plain .content.pure { padding: 0 16px 16px 16px; } .widget.plain .content a { color: #1BC3FB; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .widget.plain .content a:hover { color: #ff5722; text-decoration: underline; } .widget.plain .content a:active { color: #a22700; } .widget.list .content { padding: 8px 0; } .widget.list .content.pure { padding-top: 0; } .widget.list .content a { font-size: 14px; font-weight: bold; } .widget.list .content a:hover { text-decoration: none; } .widget.list .content a i { color: rgba(51, 51, 51, 0.7); line-height: 36px; margin-right: 3px; } .widget.list .content a img { display: inline; vertical-align: middle; height: 18px; width: 18px; margin-bottom: 4px; } .widget.list .content a img#round { border-radius: 100%; } .widget.grid .content { padding: 8px 0; } .widget.grid .content.pure { padding-top: 0; } .widget.grid .content ul.grid { border: none; display: flex; flex-wrap: wrap; justify-content: space-around; padding: 0 16px; } .widget.grid .content ul.grid a { text-align: center; border-radius: 12px; margin: 4px 0; padding: 4px 8px; display: flex; flex-direction: column; align-items: center; font-size: 12.6px; font-weight: bold; line-height: 18.2px; color: rgba(51, 51, 51, 0.7); border: 1px solid transparent; } .widget.grid .content ul.grid a i { margin-top: 0.3em; margin-bottom: 0.3em; font-size: 1.8em; } .widget.grid .content ul.grid a img { display: inline; vertical-align: middle; margin-bottom: 4px; } .widget.grid .content ul.grid a img#round { border-radius: 100%; } .widget.grid .content ul.grid a:hover { color: #1BC3FB; background: rgba(27, 195, 251, 0.1); border-radius: 4px; } .widget.grid .content ul.grid a:active { color: #1BC3FB; } .widget.grid .content ul.grid a.active { color: #1BC3FB; border: 1px solid #1BC3FB; } .widget.category .content { padding: 8px 0; font-size: 14px; font-weight: bold; } .widget.category .content.pure { padding-top: 0; } .widget.tagcloud .content { text-align: justify; padding: 8px 16px; } .widget.tagcloud .content.pure { padding: 0 16px 16px 16px; } .widget.tagcloud .content a { display: inline-block; transition: all 0.1s ease; -moz-transition: all 0.1s ease; -webkit-transition: all 0.1s ease; -o-transition: all 0.1s ease; line-height: 1.6em; } .widget.tagcloud .content a:hover { color: #ff5722 !important; text-decoration: underline; } .widget.music header.pure { padding-bottom: 4px; } .widget.music .content { padding: 12px; padding-top: 8px; } .widget.music .content.pure { padding-top: 4px; } .widget.music .content .aplayer { border-radius: 4px; color: #666; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; } .widget.related_posts .content { padding: 8px 0; font-size: 14px; font-weight: bold; } .widget.related_posts .content.pure { padding-top: 0; } .widget.related_posts .content h3 { font-size: 14px; font-weight: bold; margin: 0; } .widget.related_posts .content h3 a { line-height: inherit; padding-top: 4px; padding-bottom: 4px; } .l_side .toc-wrapper { z-index: 1; overflow: hidden; border-radius: 12px; position: sticky; top: 80px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_side .toc-wrapper header { position: sticky; width: 100%; top: 0; } .l_side .toc-wrapper .content { padding: 8px 0 16px 0; max-height: 500px; overflow: auto; } .l_side .toc-wrapper .content.pure { padding-top: 0; } .l_side .toc-wrapper .content a { border-left: 4px solid transparent; } .l_side .toc-wrapper .content a:hover { color: #333333; border-left: 4px solid #1BC3FB; } .l_side .toc-wrapper .content a:active { border-left: 8px solid #1BC3FB; } .l_side .toc-wrapper .content a.active { color: #333333; border-left: 4px solid #1BC3FB; background: rgba(27, 195, 251, 0.1); } .l_side .toc-wrapper.active { position: fixed; box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); top: 64px; width: 285px; } .l_side .toc-wrapper.active header .s-toc { transform: rotate(30deg); } @media (max-width: 1350px) { .l_side .toc-wrapper.active { width: 240px; } } @media (max-width: 768px) { .l_side .toc-wrapper.active { width: calc(100% - 2 * 16px); } } @media (max-width: 768px) { .l_side .toc-wrapper { position: fixed; max-height: 1000px; width: calc(100% - 2 * 16px); top: 64px; box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); visibility: hidden; transform: scale(0, 0); transform-origin: right top; } .l_side .toc-wrapper .rightBtn { display: none; } .l_side .toc-wrapper.active { visibility: visible; transform: scale(1, 1); } } .l_side .toc-wrapper a { padding-left: 8px; color: rgba(51, 51, 51, 0.6); font-size: 14px; display: inline-block; } .l_side .toc-wrapper ol .toc-item.toc-level-1 .toc-child a { padding-left: 12.8px; font-weight: normal; } .l_side .toc-wrapper ol .toc-item.toc-level-2 .toc-child a { padding-left: 25.6px; font-weight: normal; } .l_side .toc-wrapper ol .toc-item.toc-level-3 .toc-child a { padding-left: 38.4px; font-weight: normal; } .l_side .toc-wrapper ol .toc-item.toc-level-4 .toc-child a { padding-left: 51.2px; font-weight: normal; } .l_side .toc-wrapper ol li { width: auto; text-align: left; } .l_side .toc-wrapper ol li a { padding: 0 8px 0 11px; font-weight: bold; width: 100%; } .l_side .toc-wrapper:empty { display: none; } #archive-page { margin-bottom: 32px; } #archive-page .archive { position: relative; } #archive-page .archive .archive-year { font-size: 16px; margin-top: 4em; margin-bottom: 1em; } #archive-page .archive .archive-year:first-child { margin-top: 0em; padding-top: 0; } #archive-page .archive .archive-year h2 { margin-top: 1em; } #archive-page .archive .archive-year a { color: #333333; text-decoration: none; } #archive-page .archive .archive-post a { width: 100%; display: inline-flex; flex-flow: row nowrap; justify-content: flex-start; align-items: flex-start; text-decoration: none; } #archive-page .archive .archive-post a.child { padding-left: 32px; } #archive-page .archive .archive-post time { color: #333333; flex: none; font-size: 14px; padding: 0.5em 0.5em 0.5em 3em; } @media (max-width: 580px) { #archive-page .archive .archive-post time { padding: 0.5em 0.5em 0.5em 0; } } #archive-page .archive .archive-post .title { flex: auto; padding: 0.5em; font-size: 14px; color: #333333; } #archive-page .archive .archive-post .title i { color: #1BC3FB; } #archive-page .archive .archive-post .title i.music { color: #ff5722; } #archive-page .archive .archive-post .title i.red { color: #FE5F58; } #archive-page .archive .archive-post .title i.green { color: #3DC550; } #archive-page .archive .archive-post .title i.yellow { color: #FFBD2B; } #archive-page .archive .archive-post .title i.blue { color: #1BCDFC; } #archive-page .archive .archive-post .title i.theme { color: #1BC3FB; } #archive-page .archive .archive-post .title i.accent { color: #ff5722; } #archive-page .archive .archive-post .title i.orange { color: #ff5722; } .article { color: #333333; font-size: 16px; line-height: 1.7; word-break: break-all; word-wrap: break-word; } .article img { position: relative; margin: 0 auto; background: white; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } @media (max-width: 580px) { .article img { box-shadow: none; } } .article span img { display: inline; margin: auto; } .article .aplayer { margin: 0; display: inline-block; width: 400px; max-width: 100%; border-radius: 4px; color: #666; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; } .article p.small-img img, .article div.small-img img { width: auto; max-width: 100%; margin: 0; box-shadow: none; } .article p { margin-top: 0.5em; margin-bottom: 1em; max-width: 100%; overflow: auto; } .article p strong { color: #333333; padding-left: 2px; padding-right: 2px; } .article p .mjx-math { font-family: 'Ubuntu', Menlo, Monaco, courier, \"Lucida Console\", monospace, 'Source Code Pro', \"Microsoft YaHei\", Helvetica, Arial, sans-serif; background: rgba(244, 244, 244, 0.5); padding: 8px; border-radius: 4px; } .article ul, .article ol { font-size: 15.2px; list-style: initial; padding-left: 10px; margin-left: 10px; margin-bottom: 1em; } .article ul.center, .article ol.center { justify-content: center; } .article ul.pure, .article ol.pure { margin: 0; padding: 0; display: flex; flex-wrap: wrap; align-items: stretch; } .article ul.pure br, .article ol.pure br { display: none; } @media screen and (max-width: 900px) { .article ul.pure, .article ol.pure { justify-content: space-between; } } .article ul.pure li, .article ol.pure li { margin: 8px; display: flex; width: 75px; flex-direction: column; align-items: stretch; vertical-align: middle; text-align: center; font-size: 0.8em; line-height: 1.2em; overflow: hidden; } .article ul.pure li a, .article ol.pure li a { display: flex; flex-direction: column; align-items: center; text-align: center; } .article ul.pure li img, .article ol.pure li img { margin-bottom: 8px; } .article ul.pure.rounded img, .article ol.pure.rounded img { border-radius: 25%; } .article ul.pure.circle img, .article ol.pure.circle img { border-radius: 50%; } @media screen and (max-width: 900px) { .article ul.pure.about, .article ol.pure.about { justify-content: center; } } .article ul > li { list-style: initial; } .article ol > li { margin-left: 10px; list-style: decimal; } .article a { color: #1BC3FB; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .article a:before { display: none; } .article a:hover { color: #ff5722; text-decoration: underline; } .article a:active { color: #a22700; } .article h1, .article h2, .article h3, .article h4, .article h5, .article h6 { position: relative; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; font-weight: normal; margin-top: 1.5em; margin-bottom: 1em; } .article h1.title, .article h2.title, .article h3.title, .article h4.title, .article h5.title, .article h6.title { left: 0; } .article h1.title:before, .article h2.title:before, .article h3.title:before, .article h4.title:before, .article h5.title:before, .article h6.title:before { content: none; } .article h1, .article h2 { color: #262626; margin-top: 3em; border-bottom: 1px solid rgba(51, 51, 51, 0.1); padding-bottom: 0.2em; } .article h3:first-child, .article h4:first-child, .article h5:first-child, .article h6:first-child { margin-top: 0; padding-top: 0; } .article h1 { font-size: 24px; } .article h2 { font-size: 24px; text-align: left; } .article h3 { font-size: 20.8px; color: #262626; text-align: left; } .article h4 { font-weight: bold; font-size: 18.4px; } .article h5 { font-weight: bold; color: #333333; font-size: 16px; } .article h6 { color: rgba(51, 51, 51, 0.75); font-size: 14px; } .article .subtitle h6 { color: rgba(51, 51, 51, 0.9); } .article figure figcaption span { display: inline-block; margin-right: 5px; } .article blockquote { position: relative; width: 100%; font-size: 14px; background: rgba(27, 195, 251, 0.1); margin: 1em 0; padding: 16px; border-left: 4px solid #1BC3FB; border-radius: 4px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .article blockquote p { text-align: left; word-wrap: normal; margin: 0; font-size: 14px; line-height: 21px; } .article blockquote footer strong { margin-right: 7px; } .article blockquote.pullquote.right { border-left: none; border-right: 4px solid #1BC3FB; } .article blockquote.pullquote.right p { text-align: right; } .article pre { display: block; -moz-box-sizing: border-box; box-sizing: border-box; font-family: 'Ubuntu', Menlo, Monaco, courier, \"Lucida Console\", monospace, 'Source Code Pro', \"Microsoft YaHei\", Helvetica, Arial, sans-serif; color: #333333; } .article code { font-family: 'Ubuntu', Menlo, Monaco, courier, \"Lucida Console\", monospace, 'Source Code Pro', \"Microsoft YaHei\", Helvetica, Arial, sans-serif; padding: 3px 3px 0px 3px; margin: 0px 2px; vertical-align: center; border-radius: 2px; border: 1px solid rgba(27, 195, 251, 0.5); font-size: 12.8px; background: rgba(27, 195, 251, 0.1); } @media (max-width: 580px) { .article code { font-size: 12.16px; } } .article .readmore { font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; font-size: 0.8em; letter-spacing: 0.1em; margin-top: 16px; } .article .readmore a { text-decoration: none; display: inline-block; vertical-align: middle; line-height: 2rem; font-weight: bold; background-color: #1BC3FB; padding: 0.2em 2.4em; color: white; border-radius: 6px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .article .readmore a:hover { background: #04a8df; } .article .tags { position: relative; padding-top: 8px; padding-bottom: 8px; font-size: 14px; line-height: 1.7; margin-top: 16px; background: rgba(231, 231, 231, 0.5); word-spacing: 8px; } .article .tags a { color: #333333; position: relative; display: inline-block; word-spacing: 0; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .article .tags a:hover { color: #ff5722; background: transparent; text-decoration: none; } .article table:not('.highlight table') { width: 100%; } .article table:not('.highlight table') td, .article table:not('.highlight table') th { padding: 12px 24px; } @media (max-width: 580px) { .article ul, .article ol { font-size: 15.2px; } .article figure { font-size: 13px; line-height: 1.6em; } } .article .prev-next { width: 100%; display: flex; justify-content: space-between; align-content: flex-start; } .article .prev-next section { width: 100%; padding: 8px; color: rgba(51, 51, 51, 0.7); background-color: rgba(244, 244, 244, 0.5); border-radius: 12px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .article .prev-next section p { font-size: 16px; line-height: 1.7; margin: 0; } .article .prev-next section h4 { margin-top: 8px; margin-bottom: 8px; position: relative; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; font-weight: bold; font-size: 16px; } @media (max-width: 580px) { .article .prev-next section h4 { letter-spacing: -1px; } } .article .prev-next section h6 { margin: 0; word-spacing: normal; } .article .prev-next section .tags { background: transparent; padding: 0; margin-top: 8px; margin-bottom: 0; font-size: 12.6px; word-spacing: 4px; } .article .prev-next section:first-child { margin-left: 0; margin-right: 0; } .article .prev-next .prev { text-align: left; margin-left: 0; margin-right: 8px; border-top-right-radius: 12px; border-bottom-right-radius: 12px; } .article .prev-next .next { text-align: right; margin-left: 8px; margin-right: 0; border-top-left-radius: 12px; border-bottom-left-radius: 12px; } .highlight { position: relative; width: 100%; margin-top: 1em; margin-bottom: 1.2em; overflow: auto; display: block; background: #F4FAFE; font-size: 13.3px; font-family: 'Ubuntu', Menlo, Monaco, courier, \"Lucida Console\", monospace, 'Source Code Pro', \"Microsoft YaHei\", Helvetica, Arial, sans-serif; line-height: 1.7; border-radius: 4px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; /* Handle */ } .highlight figcaption { padding: 4px 8px; background-color: #e6f4fd; } .highlight table td, .highlight table th { padding: 0; } .highlight .gutter { width: 24px; padding: 0 12px; text-align: right; border-width: 0; margin-left: 0; background-color: #e6f4fd; } .highlight .gutter pre { color: rgba(51, 51, 51, 0.8); } .highlight .code { padding: 16px; vertical-align: top; border: 0px solid #efefef; } .highlight .code:before { content: \"\"; position: absolute; top: 0; right: 0; color: rgba(51, 51, 51, 0.8); font-size: 11.2px; padding: 4px 8px 0; line-height: 1.7; } .highlight.html .code:before { content: \"HTML\"; } .highlight.js .code:before { content: \"JS\"; } .highlight.bash .code:before { content: \"BASH\"; } .highlight.shell .code:before { content: \"SHELL\"; } .highlight.css .code:before { content: \"CSS\"; } .highlight.less .code:before { content: \"LESS\"; } .highlight.swift .code:before { content: \"SWIFT\"; } .highlight.objc .code:before { content: \"OBJECTIVE-C\"; } .highlight.c .code:before { content: \"C\"; } .highlight.java .code:before { content: \"JAVA\"; } .highlight.python .code:before { content: \"PYTHON\"; } .highlight.plain .code:before { content: \"\"; } .highlight::-webkit-scrollbar { height: 4px; width: 4px; } .highlight::-webkit-scrollbar-track-piece { background: transparent; } .highlight::-webkit-scrollbar-thumb { background: #ddeffc; cursor: pointer; border-radius: 4px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .highlight::-webkit-scrollbar-thumb:hover { background: #bce0f9; } @media (max-width: 580px) { .article .highlight { font-size: 12.635px; } } .art-item-footer { height: 40px; line-height: 1.7; font-size: 14px; } .art-item-footer .art-item-left, .art-item-footer .art-item-right { width: 50%; height: 40px; line-height: 40px; text-overflow: ellipsis; white-space: nowrap; overflow: hidden; } .art-item-footer .art-item-left { float: left; text-align: left; } .art-item-footer .art-item-right { float: right; text-align: right; } @media (max-width: 580px) { .art-item-footer { font-size: 12.635px; } } pre .line { color: rgba(51, 51, 51, 0.9); } pre .marked { background-color: rgba(255, 189, 43, 0.2); border-radius: 2px; border: 1px solid rgba(255, 189, 43, 0.4); } pre .title { color: #3f51b5; } pre .comment { color: rgba(61, 139, 64, 0.7); } pre .keyword, pre .javascript .function, pre .attr { color: #9c27b0; } pre .type, pre .built_in, pre .tag .name { color: #4BA7EE; } pre .variable, pre .attribute, pre .regexp, pre .ruby .constant, pre .xml .tag .title, pre .xml .pi, pre .xml .doctype, pre .html .doctype, pre .css .id, pre .css .class, pre .css .pseudo { color: #FD8607; } pre .number, pre .preprocessor, pre .literal, pre .params, pre .constant { color: #FD8607; } pre .class, pre .ruby .class .title, pre .css .rules .attribute { color: #ff9800; } pre .string { color: #3d8b40; } pre .value, pre .inheritance, pre .header, pre .ruby .symbol, pre .xml .cdata { color: #4caf50; } pre .css .hexcolor { color: #66cccc; } pre .function, pre .python .decorator, pre .python .title, pre .ruby .function .title, pre .ruby .title .keyword, pre .perl .sub, pre .javascript .title, pre .coffeescript .title { color: #6699cc; } .html .tag .name { color: #EE2B29; } .highlight { position: relative; } .btn-copy { display: inline-block; cursor: pointer; background-color: #FCFCFC; background-image: linear-gradient(#fcfcfc, #eee); border: 1px solid #d5d5d5; border-radius: 2px; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; -webkit-appearance: none; font-size: 13px; font-weight: 700; line-height: 20px; color: #666; padding: 2px 6px; position: absolute; right: 5px; top: 5px; opacity: 0; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .btn-copy:hover { color: #444; } .btn-copy span { margin-left: 5px; } .highlight:hover .btn-copy { opacity: 1; } /* Pagination */ #page-nav { position: relative; width: 100%; padding: 20px 0px; } #page-nav .page-number, #page-nav .space { display: none; } #page-nav .next, #page-nav .prev { font-size: 0.8125em; font-weight: normal; color: #aaaaaa; border-radius: 2px; } #page-nav .next:hover, #page-nav .prev:hover { color: #444444; } #page-nav .next span, #page-nav .prev span { line-height: 20px; vertical-align: middle; } #page-nav .next span.icon, #page-nav .prev span.icon { position: relative; top: 1px; } #page-nav .next { float: right; padding: 0 7px 2px 10px; } #page-nav .prev { float: left; padding: 0 10px 2px 7px; } #u-search { display: none; position: fixed; top: 0; left: 0; width: 100%; height: 100%; padding: 60px 20px; z-index: 999999; } @media (max-width: 680px) { #u-search { padding: 0px; } } #u-search .modal { position: fixed; height: 80%; width: 100%; max-width: 640px; left: 50%; top: 0; margin: 64px 0px 0px -320px; background: #fff; box-shadow: 0 7px 8px -4px rgba(0, 0, 0, 0.2), 0 13px 19px 2px rgba(0, 0, 0, 0.14), 0 5px 24px 4px rgba(0, 0, 0, 0.12); z-index: 3; border-radius: 12px; overflow: hidden; } @media (max-width: 680px) { #u-search .modal { box-shadow: none; max-width: none; top: 0; left: 0; margin: 0; height: 100%; border-radius: 0; } } #u-search .modal .modal-ajax-content { opacity: 0; visibility: hidden; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } #u-search .modal .modal-ajax-content.loaded { opacity: 1; visibility: visible; } #u-search .modal .modal-header { position: relative; width: 100%; height: 64px; background-color: #1BC3FB; z-index: 3; border-top-left-radius: 12px; border-top-right-radius: 12px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } @media (max-width: 680px) { #u-search .modal .modal-header { padding: 0px; border-radius: 0; } } #u-search .modal .modal-header .btn-close { display: block; position: absolute; width: 55px; height: 64px; top: 0; right: 0; color: white; cursor: pointer; text-align: center; line-height: 64px; vertical-align: middle; font-size: 1.3em; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; z-index: 2; } #u-search .modal .modal-header .btn-close:hover { transform: rotate(90deg); } #u-search .modal .modal-header .modal-loading { position: absolute; bottom: 0; left: 0; width: 100%; height: 2px; background: transparent; z-index: 1; } #u-search .modal .modal-header .modal-loading .modal-loading-bar { display: block; position: relative; width: 0%; height: 100%; background: #ffffff; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } #u-search .modal .modal-header #u-search-modal-form { position: relative; width: 100%; height: 100%; z-index: 2; } #u-search .modal .modal-header #u-search-modal-form #u-search-modal-input { width: 100%; padding: 0px 50px; height: 64px; font-size: 16px; line-height: 1.7; vertical-align: middle; color: white; border: none; background: transparent; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; font-weight: thin; appearance: none; box-shadow: none; } #u-search .modal .modal-header #u-search-modal-form #u-search-modal-input:focus { border-top-left-radius: 12px; border-top-right-radius: 12px; } #u-search .modal .modal-header #u-search-modal-btn-submit { position: absolute; top: 0; left: 0; padding-left: 5px; padding-top: 2px; background: transparent; border: none; width: 50px; height: 64px; vertical-align: middle; font-size: 1.3em; color: white; z-index: 2; } #u-search .modal .modal-footer { position: absolute; bottom: 0; left: 0; width: 100%; height: 50px; padding: 0px 15px; background: #fff; border-top: 1px solid #dddddd; } #u-search .modal .modal-footer .logo { position: absolute; top: 0; left: 0; width: 100%; height: 100%; text-align: center; z-index: 0; } #u-search .modal .modal-footer .logo a { display: inline-block; } #u-search .modal .modal-footer .logo.google img { height: 24px; margin-top: 13px; } #u-search .modal .modal-footer .logo.baidu img { height: 22px; margin-top: 14px; } #u-search .modal .modal-footer .logo img { position: relative; display: inline-block; width: auto; height: 18px; margin-top: 16px; } #u-search .modal .modal-footer .modal-error { position: relative; float: left; vertical-align: middle; line-height: 50px; font-size: 13px; z-index: 1; } #u-search .modal .modal-footer .modal-metadata { position: relative; float: left; vertical-align: middle; line-height: 50px; font-size: 13px; z-index: 1; } #u-search .modal .modal-footer .nav { position: relative; display: block; float: right; vertical-align: middle; font-size: 13px; font-weight: 500; line-height: 50px; color: #828282; cursor: pointer; z-index: 1; } #u-search .modal .modal-footer .nav:hover { color: #444444; } #u-search .modal .modal-footer .nav.btn-next { margin-left: 10px; } #u-search .modal .modal-footer .nav .icon { font-size: 12px; } #u-search .modal .modal-body { position: absolute; padding: 64px 50px 80px 50px; width: 100%; height: 100%; top: 0; left: 0; overflow-y: scroll; -webkit-overflow-scrolling: touch; background-color: white; border-radius: 12px; } @media (max-width: 680px) { #u-search .modal .modal-body { padding: 60px 20px 80px 20px; } } #u-search .modal .modal-body .modal-results { list-style: none; } #u-search .modal .modal-body .modal-results li { border-bottom: 1px solid #e6e8ea; } #u-search .modal .modal-body .modal-results li:last-child { border-bottom: none; } #u-search .modal .modal-body .modal-results .result { position: relative; display: block; padding: 15px 30px 15px 0px; text-decoration: none; } #u-search .modal .modal-body .modal-results .result:hover .title { color: #ff5722; } #u-search .modal .modal-body .modal-results .result .title { display: inline-block; max-width: 100%; color: #4d4d4d; font-size: 16px; font-weight: bold; padding: 1px; margin-bottom: 2px; line-height: 1.7; white-space: normal; overflow: hidden; text-overflow: ellipsis; } #u-search .modal .modal-body .modal-results .result .digest { display: block; white-space: pre-wrap; overflow: scroll; text-overflow: ellipsis; font-size: 14px; line-height: 1.7; color: #808080; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } #u-search .modal .modal-body .modal-results .result .icon { position: absolute; top: 50%; right: 0; margin-top: -4px; font-size: 11px; color: #828282; } #u-search .modal-overlay { position: absolute; top: 0; left: 0; width: 100%; height: 100%; background: rgba(0, 0, 0, 0.7); z-index: 1; } #footer { position: relative; padding: 40px 10px 120px 10px; width: 100%; color: rgba(51, 51, 51, 0.5); margin: 0px auto; font-size: 14px; overflow: hidden; text-align: center; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; } #footer .licenses { color: rgba(51, 51, 51, 0.5); text-decoration: underline; } #footer .codename { text-decoration: underline; } #footer .social-wrapper { display: flex; justify-content: center; flex-wrap: wrap; margin: 4px 8px; } #footer a { color: rgba(51, 51, 51, 0.7); padding: 0; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } #footer a:hover { color: #ff5722; } #footer a.social { position: relative; display: inline-block; text-align: center; display: flex; justify-content: center; align-items: center; width: 32px; height: 32px; margin: 4px; border-radius: 100px; } #footer a.social:hover { background: rgba(27, 195, 251, 0.1); color: #1BC3FB; } @media (max-width: 768px) { #footer { justify-content: center; } } .article.typo.l_friends .friends-group h2 { font-size: 20.8px; } .article.typo.l_friends .friends-group .friend-content { display: flex; flex-wrap: wrap; margin: -8px; border-radius: 12px; } .article.typo.l_friends .friends-group .friend-content .friend-card { display: flex; border-radius: 12px; box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); background: #eee; margin: 8px; color: rgba(51, 51, 51, 0.8); justify-content: flex-start; align-content: flex-start; flex-direction: column; border: 1px solid transparent; width: calc(100%/3 - 16px); } @media (max-width: 1024px) { .article.typo.l_friends .friends-group .friend-content .friend-card { width: calc(100%/3 - 16px); } } @media (max-width: 768px) { .article.typo.l_friends .friends-group .friend-content .friend-card { width: calc(100%/2 - 16px); } } @media (max-width: 580px) { .article.typo.l_friends .friends-group .friend-content .friend-card { width: 100%; margin: 0 4px; border-radius: 0; flex-direction: row; } .article.typo.l_friends .friends-group .friend-content .friend-card:first-child { border-top-left-radius: 12px; border-top-right-radius: 12px; } .article.typo.l_friends .friends-group .friend-content .friend-card:last-child { border-bottom-left-radius: 12px; border-bottom-right-radius: 12px; } } .article.typo.l_friends .friends-group .friend-content .friend-card:hover { text-decoration: none; box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1), 0 32px 64px 0px rgba(0, 0, 0, 0.1); transform: scale(1.05); border-radius: 12px; } @media (max-width: 580px) { .article.typo.l_friends .friends-group .friend-content .friend-card:hover { transform: scale(1.02); margin: 8px 0; } } .article.typo.l_friends .friends-group .friend-content .friend-card:hover .friend-left .avatar { transform: scale(1.2) rotate(12deg); box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-left { display: flex; align-self: center; } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-left .avatar { width: 64px; height: 64px; min-width: 64px; min-height: 64px; margin: 16px 8px 4px 8px; border-radius: 100%; border: 0px solid transparent; } @media (max-width: 580px) { .article.typo.l_friends .friends-group .friend-content .friend-card .friend-left .avatar { margin: 16px; } } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right { margin: 4px 8px; display: flex; flex-direction: column; text-align: center; } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right p { text-align: center; } @media (max-width: 580px) { .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right { margin: 16px 16px 16px 0; text-align: left; } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right p { text-align: left; } } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right .friend-tags-wrapper { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; margin-left: -2px; word-break: break-all; margin-bottom: 8px; } @media (max-width: 580px) { .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right .friend-tags-wrapper { margin-bottom: 0; } } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right p { margin: 0; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.15); } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right p.friend-name { font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; font-size: 16px; font-weight: bold; padding-top: 4px; } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right p.tags { font-size: 11.9px; display: inline; background: none; word-wrap: break-word; padding-right: 4px; }"}],"posts":[{"title":"《操作系统概念》笔记","slug":"《操作系统概念》笔记","date":"2021-03-31T13:30:49.000Z","updated":"2021-05-30T06:36:53.068Z","comments":true,"path":"2021/03/31/《操作系统概念》笔记/","link":"","permalink":"http://blog.zsstrike.top/2021/03/31/%E3%80%8A%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%A6%82%E5%BF%B5%E3%80%8B%E7%AC%94%E8%AE%B0/","excerpt":"本文总结了《操作系统概念》中的知识点，以便查阅。","text":"本文总结了《操作系统概念》中的知识点，以便查阅。 第二章 操作系统结构操作系统的服务：主要包括以下功能： 用户界面 程序执行 IO 操作 文件系统操作 通信：通过共享内存或者消息交换实现 错误检测 资源分配 记账：记录用户使用资源类型和数量 保护和安全 系统调用：提供操作系统的服务调用接口，该部分通常使用 C 或者 C++ 实现，当然，对某些底层硬件操作可能需要使用汇编。向操作系统传参的方法有：寄存器传参，栈传参，表传参。 image-20210331220938403 系统调用类型：进程控制，文件管理，设备管理，信息维护，通信，保护。 操作系统的结构： 简单结构：如 MS-DOS，并没有仔细划分为模块 image-20210331221537719 分层方法：每层上调用的接口是不同的，用于区分用户接口和不同等级的系统接口 image-20210331221737990 微内核：主要功能在于为客户端程序和运行在用户空间的各种服务提供通信，通信是通过消息传递实现的 image-20210331221924584 模块：最佳方法是可加载的内核模块，常见于现代 Linux 系统中 image-20210331222025099 操作系统的引导：加载内核以启动计算机的过程，称为系统引导。大多数的操作系统都有一个引导程序，改程序能够定位内核，并将其加载到内存以开始执行。当 CPU 收到一个重置事件时，指令寄存器会加载某个预先定义的内存位置，并从该位置执行，该位置实际上就是引导程序所在位置。 第三章 进程进程概念：进程是执行的程序，包括代码段，数据段，程序计数器的值，堆和栈等。程序是被动实体，而进程是活动实体。 进程状态：就绪，运行，等待，新建，终止。 image-20210529144702669 进程控制块（PCB）：包含有和进程相关的信息，如进程状态，进程 id，程序计数器，内存管理信息等。 调度队列：进程在进入系统的时候，会被加入到进程队列，该队列包含有所有进程。驻留在内存中，就绪的进程则保存在就绪队列上。通常采用双向链表实现，链表指向不同进程的 PCB。 上下文切换：切换 CPU 到另外一个进程需要保存当期进程的状态和恢复另外一个进程的状态。 进程间通信（IPC）：提供进程间通信的好处有信息共享，计算加速和模块化。有两种基本模型：共享内存和消息传递。 客户机-服务器通信： 套接字：常用且高效，但是输入分布式进程间一种低级形式的通信 远程服务调用（RPC）：和 IPC 的消息不同，RPC 通信交换的信息具有明确的结构，不再仅仅是数据包。 管道：分为单向和双向（半双工和全双工），是否允许非父子关系的进程之间相互通信。 第四章 多线程编程线程：是 CPU 调度的基本单位，包含有 ID，程序计数器，寄存器组等，和同一进程下的其他线程共享代码段，数据段和其他资源。 image-20210530095726061 多线程编程的优点：响应性，资源共享，经济和可伸缩性。 并发和并行：某个时间段内多个任务交替执行，叫做并发；同一时间点，有不同的任务运行，叫做并行。 多线程模型：有用户线程和内核线程，用户线程位于内核之上，管理无需内核支持，内核线程由操作系统支持和管理，常见的有三种模型： 多对一模型：一个线程执行阻塞调用，整个进程都会阻塞，另外，由于任一时间只有一个线程可以访问内核，不能很好地在多处理核系统上运行。 一对一模型：优点是可以在多个核上运行，缺点是每次创建一个用户线程都需要创建一个内核线程，不能创建太多的线程。 image-20210530101216129 多对多模型：解决了多对一模型和一对一模型中的缺点。多路复用多个用户线程到相同数量或者更少的内核线程，但也允许绑定某个用户线程到一个内核线程，该变种有时称为双层模型。 image-20210530101724125 轻量级进程（LWP）：在多对多模型或者双层模型之间的一个数据结构，对于用户级线程，LWP 表现为虚拟处理器，每个 LWP 和一个内核线程相连，只有内核线程才能通过系统调度以便运行于物理处理器。 image-20210530102005673 第五章 进程调度基本概念：进程的执行包括有 CPU 执行和 IO 等待过程，IO 密集型程序表现为大量短 CPU 执行，CPU 密集型程序只有少量长 CPU 执行。每当 CPU 空闲的时候，操作系统就会从就绪队列中选择一个进程来执行。在此过程中，调度程序需要完成上下文切换，切换到用户模式等。 调度方案：抢占式和非抢占式的。非抢占式调度下，一旦某个进程分配到 CPU，就会一直使用 CPU，知道它变为终止或者等待状态。 调度准则： CPU 使用率 吞吐量 周转时间：从进程提交到进程完成的时间段 等待时间：在就绪队列中等待的时间 响应时间：从提交请求到第一次响应的时间 调度算法：如何从就绪队列中选择进程以便为其分配 CPU 先到先服务调度（FCFS） 最短作业优先调度（SJF） 优先级调度（PS）：低优先级无穷等待解决方案之一是老化，即逐渐增加在系统中等待时间长的进程的优先级 轮转调度（RR）：性能很大程度上取决于时间片的大小 多级队列调度：将就绪队列分成多个单独队列，每个队列可以采用不同的调度算法 image-20210530112104285 多级反馈队列调度：在多级队列调度的时候，进程只会在某个队列中，队列之间不允许迁移，而多级反馈队列允许进程在队列之间切换。 image-20210530112241461 第六章 同步生产者消费者问题：两个不同的进程对 counter=4 分别进行加一和减一。如果这两个进程并发执行，得到的结果可能是 3,4,5。语句执行方式为 load，add/sub，store。像这样得到不正确的状态的过程称为竞争。 临界区问题：进程在临界区内可能会修改公共变量，更新文件内容等。临界区问题的解决方案应该满足如下要求： 互斥：只允许单个进程在临界区内运行 进步：如果没有进程在临界区，那么可以选择某个进程进入 有限等待 互斥锁（mutex lock）：在进入临界区应该得到锁，在退出的时候释放锁。accquire 和 release 两个操作都是原子执行的。如果在获取的时候忙等待，那么这样的锁也被称为自旋锁。不过自旋锁也有一个优点，当进程在等待锁的时候，没有上下文的切换。当使用锁的时间较短的时候，自旋锁还是可用的。 信号量（semaphore）：信号量 S 是一个整形变量，通过 wait 和 signal 操作。二进制信号量类似于互斥锁。 死锁与饥饿：死锁指的是多个进程间相互等待资源，饥饿则是无限等待信号量。 经典同步问题：有界缓冲问题，读者作者问题，哲学家就餐问题。","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://blog.zsstrike.top/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"《深入浅出MySQL》笔记","slug":"《深入浅出MySQL》笔记","date":"2020-12-20T02:42:23.000Z","updated":"2021-03-25T11:51:22.394Z","comments":true,"path":"2020/12/20/《深入浅出MySQL》笔记/","link":"","permalink":"http://blog.zsstrike.top/2020/12/20/%E3%80%8A%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAMySQL%E3%80%8B%E7%AC%94%E8%AE%B0/","excerpt":"","text":"本文用于记录《深入浅出MySQL》里面的知识要点，以备再次查阅。 第二章 SQL基础MySQL使用入门： SQL语句分类：DDL，DML，DCL。 DDL：数据定义语言，对数据库内部的对象进行创建，删除，修改等操作。 123456789101112131415161718&#x2F;&#x2F; 数据库CREATE DATABASE dbname;SHOW DATABASES;USE dbname;DROP DATABASE dbname;&#x2F;&#x2F; 表CREATE TABLE tablename ( column_name1, type1 constraints, ... column_namen, typen constraints);DESC tablename; SHOW CREATE TABLE tablename;DROP TBALE database;ALTER TABLE tablename MODIFY [COLUMN] column_definition [FIRST | AFTER col_name];ALTER TABLE tablename ADD [COLUMN] column_difinition [FIRST | AFTER col_name];ALTER TABLE tablename DROP [COLUMN] col_name;ALTER TABLE tablename CHANGE [COLUMN] old_col_name column_definition;ALTER TABLE tablename RENAME new_tablename; DML：数据操作，主要包括表记录的增删查改。 1234567INSERT INTO tablename(field1, field2, ..., fieldn)VALUES(value1, value2, ..., valuen),(value1, value2, ..., valuen);UPDATE tablename SET field1&#x3D;value1 [WHERE CONDITION];DELETE FROM tablename [WHERE CONDITION];SELECT * FROM tablename [WHERE CONDITION] [LIMIT offset_start, row_count]; 查询不重复记录：distinct 条件查询：WHERE CONDITION 排序和限制：ORDER BY 和 LIMIT offset_start, row_count 聚合：GROUP BY column_name HAVING condition 表连接：内连接，外连接（左连接，右连接） 子查询：可能需要 in，not in，exists 等 记录联合：UNION，UNION ALL DCL：DBA 用来管理系统中的对象权限时使用。 12GRANT SELECT, INSERT on dbname.* to &#39;z1&#39;@&#39;localhost&#39; identified by &#39;123&#39;;REVOKE INSERT on dbname.* from &#39;z1&#39;@&#39;localhost&#39;; 帮助的使用： 按照层次查看帮助：? contents 快速查阅帮助：? select 第三章 MySQL支持的数据类型数值类型： 类型 大小 范围（有符号） 范围（无符号） TINYINT 1 byte (-128，127) (0，255) SMALLINT 2 bytes (-32 768，32 767) (0，65 535) MEDIUMINT 3 bytes (-8 388 608，8 388 607) (0，16 777 215) INT或INTEGER 4 bytes (-2 147 483 648，2 147 483 647) (0，4 294 967 295) BIGINT 8 bytes (-9,223,372,036,854,775,808，9 223 372 036 854 775 807) (0，18 446 744 073 709 551 615) FLOAT 4 bytes (-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466 E+38) DOUBLE 8 bytes (-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) DECIMAL 对DECIMAL(M,D) ，如果M&gt;D，为M+2否则为D+2 依赖于M和D的值 依赖于M和D的值 对于整形数据，MySQL还支持在类型名称后面的小括号内指定显式宽度。 日期时间类型： 类型 大小 ( bytes) 范围 格式 用途 DATE 3 1000-01-01/9999-12-31 YYYY-MM-DD 日期值 TIME 3 ‘-838:59:59’/‘838:59:59’ HH:MM:SS 时间值或持续时间 YEAR 1 1901/2155 YYYY 年份值 DATETIME 8 1000-01-01 00:00:00/9999-12-31 23:59:59 YYYY-MM-DD HH:MM:SS 混合日期和时间值 TIMESTAMP 4 1970-01-01 00:00:00/2038 YYYYMMDD HHMMSS 混合日期和时间值，时间戳 字符串类型： 类型 大小 用途 CHAR 0-255 bytes 定长字符串 VARCHAR 0-65535 bytes 变长字符串 TINYBLOB 0-255 bytes 不超过 255 个字符的二进制字符串 TINYTEXT 0-255 bytes 短文本字符串 BLOB 0-65 535 bytes 二进制形式的长文本数据 TEXT 0-65 535 bytes 长文本数据 MEDIUMBLOB 0-16 777 215 bytes 二进制形式的中等长度文本数据 MEDIUMTEXT 0-16 777 215 bytes 中等长度文本数据 LONGBLOB 0-4 294 967 295 bytes 二进制形式的极大文本数据 LONGTEXT 0-4 294 967 295 bytes 极大文本数据 ENUM：值范围需要在创建表时通过枚举方式显式指定，对于插入不在 ENUM指定范围内的值时，并没有返回警告，而是插入了 enum 定义中的第一个值 SET：从允许值集合中选择任意1个或多个元素进行组合来赋值 第四章 MySQL中的运算符算术运算符： 运算符 作用 + 加法 - 减法 * 乘法 / 或 DIV 除法 % 或 MOD 取余 在除法运算和模运算中，如果除数为0，将是非法除数，返回结果为NULL。 比较运算符： = 等于 &lt;&gt;, != 不等于 &gt; 大于 &lt; 小于 &lt;= 小于等于 &gt;= 大于等于 BETWEEN 在两值之间 &gt;=min&amp;&amp;&lt;=max NOT BETWEEN 不在两值之间 IN 在集合中 NOT IN 不在集合中 &lt;=&gt; 严格比较两个NULL值是否相等 两个操作码均为NULL时，其所得值为1；而当一个操作码为NULL时，其所得值为0 LIKE 模糊匹配 REGEXP 或 RLIKE 正则式匹配 IS NULL 为空 IS NOT NULL 不为空 逻辑运算符： 运算符号 作用 NOT 或 ! 逻辑非 AND 逻辑与 OR 逻辑或 XOR 逻辑异或 位运算符： 运算符号 作用 &amp; 按位与 | 按位或 ^ 按位异或 ! 取反 &lt;&lt; 左移 &gt;&gt; 右移 第五章 常用函数字符串函数： 编号 函数名 作用 1 LEFT(s,n) 返回字符串s前n个字符 2 RIGHT(s,n) 返回字符串s后n个字符 3 LENGTH(s) 返回字符串s的长度 4 LOCATE(s1,s2) 从字符串 s2 中获取 子串s1 的开始位置 5 LOWER(s) 大写转小写 6 UPPER(s) 小写转大写 7 LTRIM(s) 去掉字符串s左面的空格 8 RTRIM(s) 去掉字符串s右面的空格 9 TRIM(s) 去掉字符串s两边的空格 10 ASCII(s) 返回字符串s的第一个字符的 ASCII 码 11 CONCAT(s1,s2…sn) 字符串 s1,s2 等多个字符串合并为一个字符串 12 FIND_IN_SET(s1,s2) 返回在字符串s2中与s1匹配的字符串的位置(多句话) 13 FORMAT(x,n) 可以将数字 x 进行格式化 “#,###.##”, 将 x 保留到小数点后 n 位，最后一位四舍五入 14 INSERT(s1,x,len,s2) 字符串 s2 替换 s1 的 x 位置开始长度为 len 的字符串 15 SUBSTR(s, start, length) 从字符串 s 的 start 位置截取长度为 length 的子字符串 16 POSITION(s1 IN s) 从字符串 s 中获取 s1 的开始位置 17 REPEAT(s,n) 将字符串 s 重复 n 次 18 REVERSE(s) 将字符串s的顺序反过来 19 STRCMP(s1,s2) 比较字符串 s1 和 s2，如果 s1 与 s2 相等返回 0 ，如果 s1&gt;s2 返回 1，如果 s1&lt;s2 返回 -1（比较的是字符串首字母的 ASCII 码） 20 REPLACE (s1,s2,s3) 替换字符串；将s1中的s2内容替换为s3 数值函数： 编号 函数名 作用 1 ABS(x) 返回x的绝对值 2 AVG(expression) 返回一个表达式的平均值，expression 是一个字段 3 CEIL(x)/CEILING(x) 返回大于或等于 x 的最小整数 4 FLOOR(x) 返回小于或等于 x 的最大整数 5 EXP(x) 返回 e 的 x 次方 6 GREATEST(expr1, expr2, expr3, …) 返回列表中的最大值 7 LEAST(expr1, expr2, expr3, …) 返回列表中的最小值 8 LN 返回数字的自然对数 9 LOG(x) 返回自然对数(以 e 为底的对数) 10 MAX(expression) 返回字段 expression 中的最大值 11 MIN(expression) 返回字段 expression 中的最大值 12 POW(x,y)/POWER(x,y) 返回 x 的 y 次方 13 RAND() 返回 0 到 1 的随机数 14 ROUND(x) 返回离 x 最近的整数 15 SIGN(x) 返回 x 的符号，x 是负数、0、正数分别返回 -1、0 和 1 16 SQRT(x) 返回x的平方根 17 SUM(expression) 返回指定字段的总和 18 TRUNCATE(x,y) 返回数值 x 保留到小数点后 y 位的值（与 ROUND 最大的区别是不会进行四舍五入） 日期和时间函数： 编号 函数名 作用 1 CURDATE()/CURRENT_DATE() 返回当前日期 2 CURRENT_TIME()/CURTIME() 返回当前时间 3 CURRENT_TIMESTAMP() 返回当前日期和时间 4 ADDDATE(d,n) 计算起始日期 d 加上 n 天的日期 5 ADDTIME(t,n) 时间 t 加上 n 秒的时间 6 DATE() 从日期或日期时间表达式中提取日期值 7 DAY(d) 返回日期值 d 的日期部分 8 DATEDIFF(d1,d2) 计算日期 d1-&gt;d2 之间相隔的天数 9 DATE_FORMAT 按表达式 f的要求显示日期 d 10 DAYNAME(d) 返回日期 d 是星期几，如 Monday,Tuesday 11 DAYOFMONTH(d) 计算日期 d 是本月的第几天 12 DAYOFWEEK(d) 日期 d 今天是星期几，1 星期日，2 星期一，以此类推 13 DAYOFYEAR(d) 计算日期 d 是本年的第几天 14 UNIX_TIMESTAMP() 得到时间戳 15 FROM_UNIXTIME() 时间戳转日期 16 NOW() 返回当前的日期和时间 17 STR_TO_DATE() 将日期格式的字符转换成指定格式的日期 18 DATE_FORMAT() 将日期转换成字符(支持：- . /分割年月日) 流程函数： 函数 功能 IF(cond, t, f) 如果 cond 为真，返回 t，否则fanhui f CASE cond WHEN value1 THEN result … END 多重选择 其他常用函数： 函数 功能 DATABASE() 返回当前数据库名 VERSION() 返回数据库版本 USER() 返回当前登录用户名 INET_ATON(IP) 返回 IP 代表的 num INET_NTOA(num) 返回 num 代表的 IP PASSWORD() 返回加密版本 MD5() 返回 MD5 的值 第六章 图形化工具的使用MySQL Workbench： SQL 开发 数据建模 服务器管理 MySQL Utilities phpMyAdmin： 数据库管理 数据库对象管理 权限管理 导入导出数据 第七章 存储引擎（表类型）的选择存储引擎概述：根据不同领域的需要选择合适的存储引擎，可以更好地提高数据库的效率。在诸多的引擎中，支持事务安全的只有 InnoDB 和 BDB。默认的存储引擎可以通过 default-table-type 配置。使用 SHOW ENGINES 可以查看当前数据库支持的引擎。 各种存储引擎的特性： image-20201220142420583 MyISAM：不支持事务、也不支持外键，其优势是访问的速度快，对事务完整性没有要求或者以 SELECT、INSERT 为主的应用基本上都可以使用这个引擎创建表。 InnoDB：提供了具有提交、回滚和崩溃恢复能力的事务安全。但是对比MyISAM的存储引擎，InnoDB写的处理效率差一些，并且会占用更多的磁盘空间以保留数据和索引。 MEMORY：使用存在于内存中的内容来创建表。每个MEMORY 表只实际对应一个磁盘文件，格式是.frm。MEMORY 类型的表访问非常地快，因为它的数据是放在内存中的，并且默认使用 HASH 索引，但是一旦服务关闭，表中的数据就会丢失掉。 MERGE：是一组 MyISAM 表的组合，这些 MyISAM 表必须结构完全相同，MERGE 表本身并没有数据，对 MERGE 类型的表可以进行查询、更新、删除操作，这些操作实际上是对内部 MyISAM 表进行的。 TokuDB：第三方引擎，是一个高性能、支持事务处理的MySQL和MariaDB的存储引擎，具有高扩展性、高压缩率、高效的写入性能，支持大多数在线DDL操作。 第八章 选择合适的数据类型CHAR 与 VARCHAR：下表是它们之间的对比，最后一行只适用于 MySQL 运行在非严格模式下面。 image-20201220145107981 CHAR 长度固定，处理速度快，但是浪费空间存储。对于 MyISAM 和 MEMORY 来说，首选 CHAR，而对于 InnoDB 来说，建议使用VARCHAR类型。 TEXT 与 BLOB：在保存较大文本时，通常会选择使用TEXT或者BLOB。二者之间的主要差别是BLOB能用来保存二进制数据，比如照片；而TEXT只能保存字符数据，比如一篇文章或者日记。 BLOB 和 TEXT 值会造成性能问题，在执行删除操作之后，会在数据表中留下很大的空洞，可以定期使用 OPTIMIZE TABLE 来进行碎片整理。 使用合成索引来提高大文本字段的查询性能。合成索引就是根据大文本字段的内容建立一个散列值，并把这个值存储在单独的数据列中，接下来就可以通过检索散列值找到数据行了。注意只能用于精确匹配。 在不必要的时候避免检索大型的 BLOB 或 TEXT 值。 把 BLOB 或 TEXT 列分离到单独的表中，以减少主表的碎片 浮点数与定点数：浮点数不是精确的，定点数更加精确。float，doble 都是浮点数，decimal 则是定点数。 日期类型选择：根据实际需要选择能够满足应用的最小存储的日期类型。如果记录的日期需要让不同时区的用户使用，那么最好使用 TIMESTAMP，因为日期类型中只有它能够和实际时区相对应。 第九章 字符集常用字符集比较： image-20201220170248401 选择字符集标准： 如果应用要处理各种各样的文字，首选 utf-8 如果应用中涉及已有数据的导入，就要充分考虑数据库字符集对已有数据的兼容性 如果数据库只需要支持一般中文，数据量很大，性能要求也很高，那就应该选择双字节定长编码的中文字符集，比如 GBK 如果数据库需要做大量的字符运算，如比较、排序等，那么选择定长字符集可能更好 MySQL 支持的字符集简介：查看所有可用的字符集的命令是 show character set，MySQL 的字符集包含字符集（CHARACTER）和校对规则（COLLATION）两个概念。其中字符集用来定义 MySQL 存储字符串的方式，校对规则用来定义比较字符串的方式。校对规则命名约定：以其相关的字符集名开始，通常包括一个语言名，并且以_ci（大小写不敏感）、_cs（大小写敏感）或_bin（比较是基于字符编码的值而与language无关）结束。 MySQL 字符集设置：有4个级别的默认设置：服务器级、数据库级、表级和字段级。 服务器字符集：可以在 my.cnf 中配置character-set-server来设置。 数据库字符集：可以在创建数据库的时候指定，也可以在创建完数据库后通过“alter database”命令进行修改。后者并不能修改之前已经插入的数据的字符集。 表字符集：可以在创建表的时候指定，可以通过 alter table 命令进行修改，同样，如果表中已有记录，修改字符集对原有的记录并没有影响，不会按照新的字符集进行存放。 列字符集：可以定义列级别的字符集和校对规则，主要是针对相同的表不同字段需要使用不同的字符集的情况。 字符集的修改步骤：如果原来的数据库中已经存在数据，那么通过 alter database 或者 alter tablename 的方式并不能修改之前已经插入的数据的字符集。最好先使用 mysqldump 导出表定义，然后手动修改数据集将SET NAMES character，最后再次导入数据。 第十章 索引的设计和使用索引概述：索引用于快速找出在某个列中有一特定值的行，对相关列使用索引能提高 SELECT 操作性能的最佳途径，MySQL 支持前缀索引，还支持全文索引。 创建索引： 123CREATE [UNIQUE|FULLTEXT|SPATIAL] INDEX index_name[USING index_type]ON tbl_name (index_col_name,. .); 删除索引： 1DROP INDEX index_name ON tbl_name; 索引设计原则： 最适合索引的列是出现在 WHERE 子句中的列，或连接子句中指定的列 使用唯一索引 使用短索引 不过度使用索引 BTREE 索引与 HASH 索引： 使用 HASH 索引的时候，只能用于 = 或者 &lt;&gt; 操作符比较，MySQL 不能确定两个值之间大约有多少行数据 对于 BTREE 索引，当使用 &gt;、&lt;、&gt;=、&lt;=、BETWEEN、!= 或者 &lt;&gt;，或者LIKE ‘pattern’ 操作符时，都可以使用相关列上的索引 第十一章 视图视图：一种虚拟存在的表，对于使用视图的用户来说透明，视图相对于表的优点有：简单，安全和数据独立。 视图操作： 12345678910111213&#x2F;&#x2F; 创建视图CREATE [OR REPLACE] VIEW view_name [(column_list)]AS select_statement[WITH [CASCADED | LOCAL] CHECK OPTION]&#x2F;&#x2F; 修改视图ALTER VIEW view_name [(column_list)]AS select_statement[WITH [CASCADED | LOCAL] CHECK OPTION]&#x2F;&#x2F; 删除视图DROP VIEW [IF EXISTS] view_name [, view_name] . .[RESTRICT | CASCADE]&#x2F;&#x2F; 查看视图SHOW TABLES;SHOW CREATE VIEW view_name; WITH [CASCADED | LOCAL] CHECK OPTION决定了是否允许更新数据使记录不再满足视图的条件，其中LOCAL只要满足本视图的条件就可以更新，而CASCADED则必须满足所有针对该视图的所有视图的条件才可以更新。 第十二章 存储过程和函数存储过程和函数：它们都是一段 SQL 语句的集合，不同之处在于函数必须有返回值，并且其参数只能是 IN 类型的，合理使用它们可以减少数据传输量，但是在服务器上进行大量的运算也会占用服务器的 CPU，需要综合考虑。 存储过程和函数的相关操作： 1234567891011121314151617181920&#x2F;&#x2F; 创建CREATE PROCUDURE p_name ([proc_parameter[,...]])[characteristic ..] routine_bodyCREATE FUNCTION f_name ([func_parameter[,. .]])RETURNS type[characteristic ..] routine_body&#x2F;&#x2F; 修改ALTER &#123;PROCEDURE | FUNCTION&#125; sp_name [characteristic . .]&#x2F;&#x2F; 调用CALL sp_name([parameter[,...]])&#x2F;&#x2F; 删除DROP &#123;PROCEDURE | FUNCTION&#125; [IF EXISTS] sp_name&#x2F;&#x2F; 查看SHOW &#123;PROCEDURE | FUNCTION&#125; STATUS [LIKE &#39;pattern&#39;] 通常，routine_body包含多条语句，为了不出现错误，我们可以使用DELIMITER $$命令将语句的结束符从“;”修改成其他符号（$$）。 characteristic特征值说明如下： LANGUAGE SQL：说明 BODY 是使用 SQL 语言编写的 [NOT] DETERMINISTIC：DETERMINISTIC确定的,即每次输入一样输出也一样的程序，NOT DETERMINISTIC非确定的，默认是非确定的 { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA }：提供额外信息给服务器 SQL SECURITY { DEFINER | INVOKER }：可以用来指定子程序该用创建子程序者的许可来执行，还是使用调用者的许可来执行。默认值是DEFINER 变量的使用： 123456&#x2F;&#x2F; 定义DECLARE var_name[,. .] type [DEFAULT value]&#x2F;&#x2F; 赋值SET var_name &#x3D; expr [, var_name &#x3D; expr] ..SELECT col_name[,. .] INTO var_name[,. .] table_expr 条件的使用： 12345678910111213&#x2F;&#x2F; 定义DECLARE condition_name CONDITION FOR condition_valuecondition_value: SQLSTATE [VALUE] sqlstate_value | mysql_error_code&#x2F;&#x2F; 条件处理DECLARE handler_type HANDLER FOR condition_value[,...] sp_statementhandler_type: CONTINUE | EXIT | UNDOcondition_value: SQLSTATE [VALUE] sqlstate_value| condition_name| SQLWARNING| NOT FOUND| SQLEXCEPTION| mysql_error_code 光标使用： 1234567&#x2F;&#x2F; 声明DECLARE cursor_name CURSOR FOR select_statement&#x2F;&#x2F; OPEN -&gt; FETCH -&gt; CLOSEOPEN cursor_nameFETCH cursor_name INTO var_name[, var_name]..CLOSE cursor_name 流程控制： 123456789101112131415161718192021222324252627282930&#x2F;&#x2F; IFIF condition THEN statement_list[ELSEIF condition THEN statement_list] ...[ELSE statement_list]END IF&#x2F;&#x2F; CASECASE case_valueWHEN when_value THEN statement_list[WHEN when_value THEN statement_list] ...[ELSE statement_list]END CASE&#x2F;&#x2F; LOOP，通常结合 LEAVE 使用，LEAVE 作用类似于 BREAK[begin_label:] LOOPstatement_listEND LOOP [end_label]&#x2F;&#x2F; ITERATE：跳过当前循环的剩下的语句，直接进入下一轮循环，类似 CONTINUE&#x2F;&#x2F; REPEAT[begin_label:] REPEATstatement_listUNTIL conditionEND REPEAT [end_label]&#x2F;&#x2F; WHILE[begin_label:] WHILE condition DOstatement_listEND WHILE [end_label] 事件调度器：可以在某个时间点触发操作，或者每隔一段时间执行固定代码： 12345678910&#x2F;&#x2F; 时间点CREATE EVENT myeventON SCHEDULE AT CURRENT_TIMESTAMP + INTERVAL 1 HOURDOUPDATE myschema.mytable SET mycol &#x3D; mycol + 1;&#x2F;&#x2F; 时间间隔CREATE EVENT myeventON SCHEDULE EVERY 5 SECONDDOUPDATE myschema.mytable SET mycol &#x3D; mycol + 1; 第十三章 触发器触发器操作： 1234567&#x2F;&#x2F; 创建CREATE TRIGGER trigger_name [BEFORE | AFTER] [INSERT | DELETE | UPDATE]ON table_name FOR EACH ROW trigger_stmt&#x2F;&#x2F; 删除DROP TRIGGER [schema_name.]trigger_name&#x2F;&#x2F; 查看show triggers 触发器使用：在触发器中，使用别名 OLD 和 NEW 来引用发生变化的记录内容。另外，触发器存在如下限制： 触发程序不能调用将数据返回客户端的存储程序 不能在触发器中使用以显式或隐式方式开始或结束事务的语句 第十四章 事务控制和锁定语句锁定级别：MySQL 支持对 MyISAM 和 MEMORY 存储引擎的表进行表级锁定，对 BDB 存储引擎的表进行页级锁定，对 InnoDB 存储引擎的表进行行级锁定。 表锁定：当所需要的锁已经被其他线程获取，此时该线程会进行等待，其操作如下： 12345LOCK TABLEStbl_name &#123;READ [LOCAL] | [LOW_PRIORITY] WRITE&#125;[, tbl_name &#123;READ [LOCAL] | [LOW_PRIORITY] WRITE&#125;] ...UNLOCK TABLES 事务控制：默认情况，MySQL 是自动提交的，CHAIN 会立即启动一个新事物，并且和刚才的事务具有相同的隔离级别，RELEASE 则会断开和客户端的连接。另外，所有的 DDL 语句都是不能回滚的。在事务中可以通过定义 SAVEPOINT，指定回滚事务的一个部分，但是不能指定提交事务的一个部分。 1234START TRANSACTION | BEGIN [WORK]COMMIT [WORK] [AND [NO] CHAIN] [[NO] RELEASE]ROLLBACK [WORK] [AND [NO] CHAIN] [[NO] RELEASE]SET AUTOCOMMIT &#x3D; &#123;0 | 1&#125; 分布式事务：分布式事务通常涉及到一个事务管理器和多个资源管理器，采用两阶段提交。 第十五章 SQL中的安全问题SQL 注入：利用数据库的外部接口将用户数据插入到实际的 SQL 语言中，从而达到入侵的目的。常见的语句：SELECT * FROM user WHERE username=&#39;$username&#39; AND password= &#39;$password&#39;;，此时对 username 赋值为angel&#39; or &#39;1=1，angel&#39;/*或angel&#39;#都会导致注入成功，前者使用逻辑，后者使用注释。 应对方式： PrepareStatement + Bind-Variable：通过转义用户输入的参数防护 使用应用程序提供的转换函数：如mysql_real_escape_string() 自定义：正则校验，特殊字符转义等 第十六章 SQL Mode及其相关问题SQL Mode 简介：SQL Mode 通常用来解决以下问题： 设置不同的 SQL Mode，可以设置不同程度的数据校验，保证准确性 通过设置为 ANSI 模式，便于迁移 在数据迁移之前，改变 SQL Mode，可以便于数据迁移 SQL Mode 功能： 校验日期数据合法性 MOD(X, 0) 在 TRADITIONAL 模式下会直接产生错误 启用 NO_BACKSLASH_ESCAPE 使得反斜线成为普通字符 启用 PIPES_AS_CONCAT 模式，将|视作字符串的链接操作符 常见的 SQL Mode： image-20210305190614244 SQL Mode 在迁移中使用方式：可以通过组合不同的 sql_mode 来构成适合于其他数据库的数据格式，这样就可以使得导出的数据更容易导入到对应的数据库。 第十七章 MySQL分区概述：分区有利于管理非常大的表，采用分而治之的思想，将表分成一系列的分区，分区对于应用来说是完全透明的，具体而言，使用分区的好处有： 和单个磁盘相比，能存储更多的数据 优化查询 通过删除分区直接删除相关的数据 跨多个磁盘，能获得更大的吞吐量 分区类型：无论那种分区，都只能使用主键或者唯一键 RANGE 分区：给予一个给定的连续区间范围，将数据分配到不同分区。使用VALUES LESS THAN划定范围。 LIST 分区：类似 RANGE 分区，不过 LIST 对应的是枚举值。使用VALUES IN划定分区。 COLUMNS 分区：支持分区的键的数据类型更广，并且支持多列分区（多列排序）。又分为RANGE COLUMNS和LIST COLUMNS。 HASH 分区：给予给定的分区个数，将数据分区。主要用于分散热点读，确保负载均衡。使用PARTITION BY HASH(expr) PARTITIONS num进行分区，底层采用的是 MOD 算法。常规的 HASH 算法挺不错，但是需要增加分区或者合并分区的时候，问题就出现了，即需要重新 MOD 计算。为此，可以使用线性 HASH 分区，其优点在于存在分区维护的时候，MySQL 能够处理得更加迅速。 KEY 分区：类似于 HASH 分区，只不过 HASH 分区允许使用用户自定义的表达式，而 Key 分区不允许使用用户自定义的表达式，需要使用 MySQL 服务器提供的 HASH 函数。 子分区：指对每个分区的再次分割，使用SUBPARTITION BY语句实现。 分区管理： RANGE &amp; LIST 分区管理： 123456&#x2F;&#x2F; 删除ALTER TABLE tbl_name DROP PARTITION p_name;&#x2F;&#x2F; 添加ALTER TABLE tbl_name ADD PARTITION (patition_stmt);&#x2F;&#x2F; 重新组织ALTER TABLE tbl_name REORGANIZE PARTITION p_name into (patition_stmt); HASH &amp; KEY 分区管理： 1234&#x2F;&#x2F; 合并ALTER TABLE tbl_name COALESCE PARTITION p_num;&#x2F;&#x2F; 增加ALTER TABLE tbl_name ADD PARTITIONS p_num; 第十八章 SQL优化优化 SQL 语句的一般步骤： 通过使用SHOW [SESSION | GLOBAL] STATUS命令了解各种 SQL 的执行频率 定位执行效率较低的 SQL 语句：使用--log-slow-queries=[file_name]启动，或者使用show processlist查看进程列表 通过 EXPLANIN 分析低效 SQL 的执行计划 通过 show profile分析 SQL 通过trace分析优化器如何选择执行计划 确定问题并且采取相应的优化措施 索引问题： 索引的存储分类：索引是在存储引擎层中实现的，MySQL 暂时提供以下四种类型的索引： image-20210306193145571 使用索引的场景： 匹配全值 匹配值的范围查询 匹配最左前缀：比如在 col1 + col2 + col3 字段上的联合索引能够被包含 col1、(col1 + col2)、(col1 + col2 + col3)的等值查询利用到 仅仅对索引进行查询 匹配列前缀：仅仅使用索引中的第一列,并且只包含索引第一列的开头一部分进行查找 能够实现索引匹配部分精确而其他部分进行范围匹配 如果列名是索引,那么使用 column_name is null 就会使用索引 .存在索引但不能使用索引的典型场景： 以%开头的 LIKE 查询不能够利用 B-Tree 索引 数据类型出现隐式转换的时候也不会使用索引 复合索引的情况下，不满足最左原则 用 or 分割开的条件，如果 or 前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到 查看索引使用情况：show status like ‘Handler_read%’; 简单的优化方法： 定期分析表和检查表：ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name 和 CHECK TABLE tbl_name [, tbl_name] … [option] 定期优化表：OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name。 常用的 SQL 优化： 大批量数据插入：使用 load 命令，通过简单设置可提高导入速度： 12345678&#x2F;&#x2F; MyISAM 存储引擎的表ALTER TABLE tbl_name DISABLE KEYS;loading the dataALTER TABLE tbl_name ENABLE KEYS;&#x2F;&#x2F; InnoDB 存储引擎的表SET UNIQUE_CHECKS&#x3D;0loading the dataSET UNIQUE_CHECKS&#x3D;1 优化 INSERT 语句：同一个客户插入很多行，尽量使用多个值表的 INSERT 语句 优化 ORDER BY 语句：MySQL 排序方式有使用有序索引，第二种是对返回数据排序（FileSort）。对于 FIleSort 算法，MySQL 有两种方式：两次扫描算法和一次扫描算法，通过增加 max_length_for_sort_data 的大小使得尽可能使用一次扫描算法。 优化 GROUP BY 语句：默认情况下，MySQL 对 GROUP BY 字段进行排序，如果想要不排序，可以追加使用 ORDER BY NULL 禁止排序。 优化嵌套查询：有些情况下，子查询可以被更有效率的 JOIN 替代 优化 OR 条件：对于含有 OR 的查询子句，如果要利用索引，则 OR 之间的每个条件列都必须用到索引；如果没有索引，则应该考虑增加索引。 优化分页查询：考虑分页场景limit 1000,20，此时 MySQL 排序出前 1020 条记录后仅仅需要返回第 1001-1020 条记录，而前 1000 条记录则会被抛弃。优化方案有： 在索引上完成排序分页的操作，最后根据主键关联回原表查询所需要的其他列内容。 把 LIMIIT 查询转换成某个位置的查询，如根据上次的查询的最大 id 计算下一次的最大的值。 常用 SQL 技巧： 正则表达式的使用 使用 RAND() 提取随机行：select * from category order by rand() 使用 BIT GROUP FUNCTION 做统计 使用外键需要注意的问题：在 MySQL 中，InnoDB 存储系统支持对外键约束条件的检查，而对于其他类型存储引擎的表则没有这种检查 第十九章 优化数据库对象优化表的数据类型：在设计表的时候需要考虑字段的长度留有一定的冗余，但是不推荐让很多字段留有大量的冗余，这会造成磁盘空间的浪费，可以使用PROCEDURE ANALYSE()对表进行分析。 通过拆分提高表的访问效率： 垂直拆分：一个表中的某些列常用，某些列不常用的情况，缺点是查询所有数据的时候需要联合数据 水平拆分：表很大，分割后可以降低查询时需要读的数据和索引的页数；表中的数据本来就具有某些独立性，如时间段，缺点在于查询所有数据的时候需要联合数据 逆规范化：数据的规范化程度并不是越高越好，规范化程度越高，产生的关系就越多，从而导致表之间的连接操作越频繁，而导致性能下降。常用的方法有增加冗余列、增加派生列、重新组表和分割表。 使用中间表提高统计查询速度：如查询最近一周的消费情况，就可以在原来的消费表上建立起来，优点在于中间表复制源表部分数据，并且与源表相“隔离”，另外，中间表上可以灵活地添加索引或增加临时用的新字段。 第二十章 锁问题MySQL 锁概述：MySQL 提供以下三类级别的锁： 表级锁：如 MyISAM 和 MEMORY 引擎，开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低 行级锁：如 InnoDB 引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高 页面锁：如 BDB 引擎，开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 MyISAM 表锁： 查询表级锁争用情况：检查 table_locks_waited 和 table_locks_immediate 状态变量 表级锁模式：表共享读锁和表独占写锁 如何加表锁： 隐式：执行查询语句时，自动加读锁；执行更新操作时，自动加写锁 显式：LOCK TABLES tbl_name [READ | WRITE] [LOCAL]，LOCAL 参数允许用户在表尾并发插入记录，另外需要注意别名问题 并发插入：存在系统变量 concurrent_insert，用于控制并发插入行为： 设置为 0 时，不允许并发插入。 设置为 1 时，如果 MyISAM 表中没有空洞(即表的中间没有被删除的行)，MyISAM允许在一个进程读表的同时，另一个进程从表尾插入记录。默认项 设置为 2 时，无论 MyISAM 表中有没有空洞，都允许在表尾并发插入记录。 锁调度：如果同时存在一个写进程和一个读进程获取相同项的锁，MySQL 会优先将锁给写进程。可以通过调整low-priority-updates变量改变调度行为；还有一个动态调度，设置max_write_lock_count，当一个表的写锁达到这个值后，MySQL 就暂时将写请求的优先级降低 InnoDB 锁问题：InnoDB 相较于 MyISAM 最大的不同点是支持事务和采用了行级锁。 背景：事务和 ACID 属性，并发事务处理带来的问题，事务隔离级别 查询行锁使用情况：检查 InnoDB_row_lock 状态变量 锁模式：行锁有共享锁和排他锁，表锁（意向锁）有 IS 和 IX 锁，意向锁是自动加的，对于更新操作，会加上 X 锁，对于普通的 SELECT 语句，则不会加任何锁，但是可以通过SELECT ... LOCK IN SHARE MODE获取 S 锁，通过SELECT ... FOR UPDATE获取 X 锁 行锁实现方式：一般分为以下三种： Record Lock：对索引项加锁。 Gap lock：对索引项之间的“间隙”、第一条记录前的“间隙”或最后一条记录后的“间隙”加锁。 Next-key lock：前两种的组合，对记录及其前面的间隙加锁。 其实现方式会导致： 在不通过索引条件查询时，InnoDB 会锁定表中的所有记录 虽然是访问不同行的记录，但是如果是使用相同的索引键，是会出现锁冲突的 当表有多个索引的时候，不同的事务可以使用不同的索引锁定不同的行，不论是使用主键索引、唯一索引或普通索引，InnoDB 都会使用行锁来对数据加锁 Next-Key 锁：对于键值在条件范围内但并不存在的记录，叫做“间隙(GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的 Next-Key 锁。 什么时候使用表锁：事务需要更新大部分或全部数据，表又比较大；事务涉及多个表，比较复杂，很可能引起死锁，造成大量事务回滚。 死锁：MyISAM 是 deadlock free 的，这是因为 MyISAM 一次获取需要的所有锁，而在InnoDB中，锁是逐步获得的。可以使用顺序加锁，申请大粒度锁等情况规避死锁。 第二十一章 优化MySQL ServerMySQL 体系结构概览：体系结构图如下： image-20210309145331858 主要包含有以下几类线程： master thread：主要负责将脏缓存页刷新到数据文件，执行 purge操作，触发检查点，合并插入缓冲区等 insert buffer thread：主要负责插入缓冲区的合并操作 read thread write thread log thread purge thread lock thread MySQL 内存管理及优化： MyISAM 内存优化：使用 key buffer 缓存索引块，对于数据块，则依赖于 IO 缓存 key_buffer_size 使用多个索引缓存：MySQL 通过 session 之间共享 key buffer 提高使用效率，但是不能消除其竞争 调整中点插入策略：对 LRU 的改进 调整 read_buffer_size 和 read_rnd_buffer_size InnoDB 内存优化：用一块内存区做 IO 缓存池，该缓存池不仅用来缓存 InnoDB 的索引块，而且也用来缓存InnoDB的数据块，缓存池逻辑上由 free list、flush list 和 LRU list 组成。InnoDB 使用的 LRU 算法是类似两级队列的方法 innodb_buffer_pool_size 的设置 调整 old sublist 大小 调整 innodb_old_blocks_time 的设置：确定从 old sublist 到 young sublist 的时间 调整缓存池数量 innodb_buffer_pool_instances 控制 innodb buffer 刷新，延长数据缓存时间 InnoDB doublewrite：原因是 MySQL 的数据页大小（一般是 16KB）与操作系统的 IO 数据页大小（一般是 4KB）不一致，无法保证 InnoDB 缓存页被完整、一致地刷新到磁盘。原理是用系统表空间中的一块连续磁盘空间（100个连续数据页，大小为 2MB）作为 doublewrite buffer，当进行脏页刷新时，首先将脏页的副本写到系统表空间的 doublewrite buffer 中，然后调用 fsync 刷新操作系统 IO 缓存，确保副本被真正写入磁盘。 调整排序缓存大小 sort_buffer_size 和连接缓存大小 join_buffer_size InnoDB log 机制及优化：采用 redo 日志，优化方法如下 innodb_flush_log_at_trx_commit 的设置 设置 log file size，控制检查点 调整 innodb_log_buffer_size 调整 MySQL 并发相关的参数： 调整 max connections，提高并发连接 调整 back_log：积压请求栈大小 调整 table_open_cache：控制所有 SQL 执行线程可打开表缓存的数量 调整 thread cache size innod block wait timeout 的设置 第二十二章 磁盘IO问题使用磁盘阵列：RAID 将数据分布到若干物理磁盘上，确保了数据存储的可靠性，同时提供并发读写的能力，常见的级别如下： image-20210311145856797 虚拟文件卷或软 RAID：相较于单个磁盘，性能有所改善 使用 Symbolic Links 分布 IO：默认情况下，数据库名和表名对应的就是文件系统的目录名和文件名，但是这样不利于多磁盘并发读写的能力，可以使用符号链接将不同的数据库指向不同的物理磁盘，达到分布磁盘 IO 的目的。 禁止操作系统更新文件的 atime 属性：LINUX 系统下，每次读取一个文件，操作系统就会将读操作的时间写回到磁盘上，这可能会影响 IO 性能。 使用裸设备存放 InnoDB 的共享表空间：对于 MyISAM，数据文件的读写完全依赖于操作系统，但是对于 InnoDB 来说，其自己实现了数据缓存机制，操作系统的缓存系统可能对其有反作用，可将数据放倒 Raw Device 上。 调整 IO 调度算法：传统硬盘读取数据分为将磁头移动到磁盘表面正确位置，将磁盘旋转，使得正确的数据移动到磁头下面，继续旋转，直到所有数据读取完。Linux 实现了四种 IO 调度算法： NOOP：不对 I/O请求排序，除了合并请求也不会进行其他任何优化 最后期限（Deadline）算法：维护了一个拥有合并和排序功能的请求队列之外，额外维护了两个队列，分别是读请求队列和写请求队列，它们都是带有超时的FIFO队列。超时的请求会被先处理。 预期算法（Anticipatory）：和 Deadline 算法类似，不过当其处理完一个I/O请求之后并不会直接返回处理下一个请求，而是等待片刻（默认 6ms），等待期间如果有新来的相邻扇区的请求，会直接处理新来的请求，当等待时间结束后，调度才返回处理下一个队列请求。 完全公平队列：把 I/O请求按照进程分别放入进程对应的队列中，公平是针对进程而言的。 第二十三章 应用优化使用连接池：建立连接的代价较大，使用连接池可以减去建立新连接的开销。 减少对 MySQL 的访问： 避免对同一数据做重复索引 使用查询缓存：查询缓存（Query Cache）会保存 SELECT 查询的文本和相应的结果，每次更新就会清空缓存，适用于更新不频繁的表 增加 CACHE 层：如在用户端上建立一个二级数据库，将访问频率高的放在这个库上面 负载均衡： 利用 MySQL 复制分流查询操作：一个主服务器承担更新操作，而多台从服务器承担查询操作，主从之间通过复制实现数据的同步 采用分布式数据库架构 第二十五章 MySQL中的常用工具mysql（客户端连接工具）： 连接选项：-u，-p，-h，-P（端口） 客户端字符集选项：-default-character-set myisampack（MyISAM 表压缩工具）：可以使用很高的压缩率来对 MyISAM 存储引擎的表进行压缩，使得压缩后的表占用比压缩前小得多的磁盘空间。但是压缩后的表也将成为一个只读表，不能进行DML操作。 mysqladmin（MySQL 管理工具）：可以用它来检查服务器的配置和当前的状态、创建并删除数据库等。它的功能和mysql客户端非常类似，主要区别在于它更侧重于一些管理方面的功能，比如关闭数据库。 mysqlbinlog（日志管理工具）：由于服务器生成的二进制日志文件以二进制格式保存，所以如果想要检查这些文件的文本格式，就会用到 mysqlbinlog 日志管理工具。 mysqlcheck（MyISAM 表维护工具）：可以检查和修复 MyISAM 表，还可以优化和分析表。其集成了mysql工具中check、repair、analyze、optimize的功能。 mysqldump（数据导出工具）：用来备份数据库或在不同数据库之间进行数据迁移。 mysqlimport（数据导入工具）：客户端数据导入工具，用来导入 mysqldump 加 -T 选项后导出的文本文件。 mysqlshow（数据库对象查看工具）：用来很快地查找存在哪些数据库、数据库中的表、表中的列或索引。 第二十六章 MySQL日志错误日志：记录了当mysqld启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。 二进制日志：记录了所有的 DDL（数据定义语言）语句和 DML（数据操纵语言）语句，但是不包括数据查询语句。此日志对于灾难时的数据恢复起着极其重要的作用。格式有： STATEMENT：每一条对数据造成修改的SQL语句都会记录在日志中，这种格式的优点是日志记录清晰易读、日志量少，对I/O影响较小。缺点是在某些情况下slave的日志复制会出错。 ROW：将每一行的变更记录到日志中，而不是记录SQL语句，优点是会记录每一行数据的变化细节，不会出现某些情况下无法复制的情况。缺点是日志量大，对I/O影响较大。 MIXED：能尽量利用两种模式的优点，而避开它们的缺点。 查询日志：查询日志记录了客户端的所有语句，而二进制日志不包含只查询数据的语句。 慢查询日志：记录了所有执行时间超过参数 long_query_time 设置值并且扫描记录数不小于min_examined_row_limit 的所有 SQL 语句的日志。 日志查询分析工具：mysqlsla，myprofi 和 mysql-explain-slow-log 等。 第二十七章 备份与恢复备份恢复策略： 备份的表的存储引擎是事务型还是非事务型 全备份和增量备份 定期备份 使用复制方法来异地备份，但是复制对于数据库的误操作无能为力 逻辑备份和恢复：逻辑备份的最大优点是对于各种存储引擎都可以用同样的方法来备份；而物理备份则不同，不同的存储引擎有着不同的备份方法。 备份：将数据库中的数据备份为一个文本文件，可以被查看和编辑，可使用 mysqldump 工具实现 完全恢复：mysql –uroot –p dbname &lt; bakfile，还需要执行日志重做：mysqlbinlog binlog-file | mysql -u root –p*** 基于时间点恢复：某个时间点发生了误操作，我们只需要不完全恢复即可： 12shell&gt;mysqlbinlog --stop-date&#x3D;&quot;2005-04-20 9:59:59&quot; &#x2F;var&#x2F;log&#x2F;mysql&#x2F;bin.123456 | mysql -u root –pmypwdshell&gt;mysqlbinlog --start-date&#x3D;&quot;2005-04-20 10:01:00&quot; &#x2F;var&#x2F;log&#x2F;mysql&#x2F;bin.123456| mysql-u root -pmypwd \\ 基于位置恢复： 12shell&gt;mysqlbinlog --stop-position&#x3D;&quot;368312&quot; &#x2F;var&#x2F;log&#x2F;mysql&#x2F;bin.123456 | mysql -u root -pmypwdshell&gt;mysqlbinlog --start-position&#x3D;&quot;368315&quot; &#x2F;var&#x2F;log&#x2F;mysql&#x2F;bin.123456 | mysql -u root -pmypwd 物理备份和恢复： 冷备份：就是停掉数据库服务，cp 数据文件的方法。 热备份： MyISAM：本质是将要备份的表加读锁，然后再复制数据文件到备份目录 InnoDB：使用 ibbackup 或者 Xtrabackup 表的导入和导出： 导出： 使用 SELECT …INTO OUTFILE … 命令来导出数据 使用 mysqldump 导入： 使用 LOAD DATA INFILE… 命令，该命令加载数据最快 使用 mysqlimport 第二十八章 MySQL权限与安全MySQL 权限管理： 工作原理：首先对连接的用户进行身份认证，然后对通过的用户赋予对应权限 权限表：系统会用到 mysql 数据库下面的 user，host 和 db 这三个权限表 image-20210314234141680 帐号管理： 12345678910&#x2F;&#x2F; 创建帐号，赋予权限GRANT pri_type ON &#123;tbl_name | db_name.*&#125; TO user [IDENTIFIED BY [PASSWORD] &#39;password&#39;]]&#x2F;&#x2F; 查看帐号权限show grants for user@host;&#x2F;&#x2F; 撤销权限REVOKE pri_type ON &#123;tbl_name | db_name.*&#125; FROM user.&#x2F;&#x2F; 修改账户密码SET PASSWORD FOR &#39;jeffrey&#39;@&#39;%&#39; &#x3D; PASSWORD(&#39;biscuit&#39;);&#x2F;&#x2F; 删除帐号DROP USER user MySQL 安全问题： 操作系统相关： 严格控制操作系统账号和权限 尽量避免以 root 权限运行 MySQL 防止 DNS 欺骗，最好使用 IP 地址而不是域名 数据库相关： 删除匿名帐号 给 root 帐号设置口令 只授予帐号必需的权限 除 root 外，任何用户不应有 mysql 库 user 表的存取权限 不要把 FILE、PROCESS 或 SUPER 权限授予管理员以外的账号 DROP TABLE 命令并不收回以前的相关访问授权 使用 SSL 其他安全设置选项： old-passwords：PASSWORD生成的密码是 16 位，在 4.1 之后，生成的函数值变为了 41 位 skip-grant-tables：不使用权限表 skip-network：适用于应用和数据库在一台机器上的情况 第三十章 MySQL常见问题和应用技巧忘记 MySQL 的 root 密码：首先手动 kill 掉 MySQL 进程，接着使用--skip-grant-tables选项重启登陆到 MySQL 服务，之后就可以更新密码，并且刷新权限表。 处理MyISAM存储引擎的表损坏： 使用 myisamchk 工具：myisamchk -r tablename 使用 SQL 命令：CHECK TABLE 和 REPAIR TABLE MyISAM 表超过 4GB 无法访问的问题：对数据文件的最大 size 进行扩充 磁盘目录空间不足问题：更改数据文件和索引文件的默认位置 DNS 反向解析问题：--skip-name-resolve mysql.sock 丢失后连接数据库：使用其他协议如--protocol=TCP|PIPE|SOCKET 第三十一章 MySQL复制MySQL 复制的优点： 主库出现问题，可以切换到从库提供服务 可以在从库上执行查询操作，降低主库的访问压力 可以在从库上执行备份，以避免备份期间影响主库的服务 复制概述：首先，MySQL 主库会在数据变更的时候将其记录在 Binlog 中，主库推送 binlog 中的事件到从库的中继日志 Relay Log，之后从库根据 Relay Log 重做数据变更操作，通过逻辑复制以此达到数据一致。复制流程如下： image-20210325183741481 复制中的各类文件：binlog 会把所有的数据修改操作以二进制的形式记录到文件中，binlog 支持三种格式： Statement：基于 SQL 语句级别的 Binlog，每条修改数据的 SQL 都会保存到 Binlog 里 Row：基于行级别，记录每一行数据的变化，数据量大 Mixed：混合 Statement 和 Row 模式 复制的三种常见架构： 一主多从复制架构：对实时性要求不是特别高的读请求通过负载均衡分布到多个从库上，降低主库的读取压力 image-20210325185811765 多级复制架构：解决了一主多从场景下，主库的 I/O 负载和网络压力，当然也有缺点：MySQL 的复制是异步复制，多级复制场景下主库的数据是经历两次复制才到达从库 image-20210325185912264 双主复制：主库 Master1 和 Master2 互为主从，所有 Web Client 客户端的写请求都访问主库 Master1，而读请求可以选择访问主库 Master1 或 Master2 image-20210325190023002 复制搭建过程： 异步复制： image-20210325190234096 半同步复制：为了保证主库上的每一个 Binlog 事务都能够被可靠的复制到从库上，主库在每次事务成功提交时，并不及时反馈给前端应用用户，而是等待其中一个从库也接收到 Binlog 事务并成功写入中继日志后，主库才返回Commit操作成功给客户端 image-20210325190248324 第三十二章 MySQL ClusterMySQL Cluster 架构：节点类型可以分为三类：管理节点，SQL 节点和数据节点。前台应用一定的负载均衡算法将对数据库的访问分散到不同的 SQL 节点上，然后 SQL 节点对数据节点进行数据访问并从数据节点返回结果，最后 SQL 节点将收到的结果返给前台应用。 image-20210325191905612 第三十三章 高可用架构MMM 架构：MMM（Master-Master replication manager for MySQL）是一套支持双主故障切换和双主日常管理的脚本程序。实现了故障切换的功能，另一方面其内部附加的工具脚本也可以实现多个 slaves 的 read 负载均衡。由于 MMM 无法完全地保证数据一致性，所以 MMM 适用于对数据的一致性要求不是很高，但是又想最大程度的的保证业务可用性的场景对于那些对数据的一致性要求很高的业务，非常不建议采用 MMM 这种高可用性架构。 image-20210325193804536 MHA 架构：MHA（Master High Availability）目前在 MySQL 高可用方面是一个相对成熟的解决方案，MHA能在最大程度上保证数据的一致性，以达到真正意义上的高可用。工作原理如下： 从宕机崩溃的 master 保存二进制日志事件 识别含有最新更新的 slave 应用差异的中继日志（relay log）到其他 slave 应用从 master 保存的二进制日志事件 提升一个 slave 为新 master 使其他的 slave 连接新的 master 进行复制 image-20210325194616649","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.zsstrike.top/tags/MySQL/"}]},{"title":"《Java并发编程实战》笔记","slug":"《Java并发编程实战》笔记","date":"2020-12-17T10:43:50.000Z","updated":"2020-12-19T11:50:06.606Z","comments":true,"path":"2020/12/17/《Java并发编程实战》笔记/","link":"","permalink":"http://blog.zsstrike.top/2020/12/17/%E3%80%8AJava%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/","excerpt":"本文总结了《Java并发编程实战》中的关键点，可以用于查阅其中的知识点。","text":"本文总结了《Java并发编程实战》中的关键点，可以用于查阅其中的知识点。 第二章 线程安全性线程安全性：当多个类访问同一个类的时候，这个类始终都能表现出正确的行为，就称该线程是线程安全的。 原子性： 竞态条件（Race Condition）：当某个计算的正确性取决于多个线程的交替执行时序时，那么就会发生竞态条件。比如懒汉式单例模式中的 getInstance 方法，基于先检查后执行，由于需要检查 instance 是否为 null，再判断是否需要实例化，此时就存在竞态条件。 复合操作：指的是将一系列的操作合并成一个，使其满足原子性，比如 AtomicLong 里面的 incrementAndGet 方法。 加锁机制： 内置锁：使用关键字 synchronized 实现同步锁，修饰方法的时候锁就是方法调用所在的对象，静态的 synchronized 方法以 Class 对象为锁。 123synchronized (lock) &#123; // 访问或者修改由锁保护的共享对象&#125; 重入：当某个线程请求一个由其他线程持有的锁的时候，发出请求的线程会阻塞。但是如果一个线程试图获得一个已经由它自己持有的锁，那么这个请求就会成功。 第三章 对象的共享可见性： 失效数据：一个线程修改某个数据后，如果没有进行同步操作，另外一个线程再去读的话，可能就会读到之前的数据。 非原子的 64 位操作：Java 内存模型要求，变量的读取和写入都是原子操作，但是对于非 volatile 类型的 long 和 double 变量，JVM 允许将其分解为两个 32 位的操作。此时就可能发生失效数据的读取。 加锁与可见性：加锁的含义不仅在于互斥行为，还在于内存可见性，为了确保所有的线程能看到共享变量的最新值，所有执行读操作或者写操作的线程必须在同一个锁上同步。 volatile 变量：对变量的更新操作将会通知到其他线程。不建议过度使用 volatile 变量，因为volatile 变量只能保证可见性，不能确保原子性。 发布与逸出：发布指对象能够在当前作用域之外的代码中使用；逸出指的是当某个不应该被发布的对象被发布。不要在构造过程中使得 this 逸出，常见错误是在构造函数中启动一个线程。 线程封闭：避免同步的方式就是不共享数据，如果仅在单线程内访问数据，就不需要同步，这就是线程封闭。 Ad-hoc 线程封闭：维护线程封闭的职责完全由程序实现来承担。 栈封闭：是线程封闭的一种特例，在栈封闭中，只能通过局部变量才能访问到对象。 ThreadLocal 类：能使线程中的某个值与保存值的对象关联起来。ThreadLocal 提供 get 和 set 方法，这些方法为每个使用该变量的线程都存有一份独立副本，因此是线程独立的。通常用于防止对可变的单例变量或全局变量进行共享。 不变性：不可变对象一定是线程安全的。 final 域：用于构造不可变性对象，使用 final 修饰的域是不可更改的。另外，final 域可以确保初始化过程的安全性。 第四章 对象的组合设计线程安全的类：收集同步需求，以来状态的操作，状态的所有权。 实例封闭：封装简化了线程安全类的实现过程，当一个对象封装到另外一个对象中的时候，能够访问被封装对象的代码路径都是已知的，这样更适合对代码进行分析和加锁。 Java 监视器模式：使用私有锁对象而不是对象的内置锁的优点有，私有的锁对象可以将锁封装起来，但是客户端还是可以获取到共有方法来访问锁。 12345678910public class ProvateLock &#123; private final Object myLock = new Object(); Widget widget; void someMethod() &#123; synchronized (myLock) &#123; // 访问修改Widget的状态 &#125; &#125;&#125; 在现有的线程安全类中添加功能： 客户端加锁机制：对于使用某个对象 X 的客户端代码，使用 X 本身用于保护其状态的锁来保护这段客户端代码。 组合：使用组合方法构建对象，同时在上层再次加锁，实现同步。 第五章 基础构建模块同步容器类： 问题：同步容器类都是线程安全的，但是在某些情况需要额外的客户端加锁实现复合操作。 迭代器与 ConcurrentModificationException：如果在迭代期间对迭代对象进行了修改，可能就会抛出该异常。可以使用加锁来解决该问题，但是可能会带来验证 的性能问题。如果不想在迭代期间对对象进行加锁操作，可以先克隆容器，并在副本上迭代。 隐藏迭代器：比如打印一个 set 的时候就隐式用到了迭代器。 并发容器： ConcurrentHashMap：同步容器类在执行期间都持有一个锁，而并发容器类则使用了一种不同的加锁策略：使用粒度更细的加锁机制实现最大程度的共享，称为分段锁。该策略能够在并发编程的环境中实现更大的吞吐量。另外，并发容器类提供的迭代器不会抛出 ConcurrentModificationException，因此不需要在迭代过程中对容器加锁。由于他们返回的迭代器具有弱一致性，也即可以容忍并发的修改，当创建迭代器会遍历已有的元素，并可以（不保证）在迭代器构造后将修改反映给容器。 额外的原子 Map 操作：由于并发容器不支持加锁，因此我们不能基于加锁来实现复合操作。但是，一些复合原子操作已经内置提供：putifAbsent，remove和replace。 CopyOnWriteArrayList：用于替代 List，提供更好的并发性能，并且迭代器件不需要对容器加锁或者复制。每次修改容器的时候都会复制底层数组，需要一定开销。 阻塞队列和生产者-消费者模式： 阻塞队列：提供了可阻塞的 put 和 take 方法，以及支持定时的 offer 和 poll 方法。如果队列满了，那么 put 方法阻塞直到有空间可用。阻塞队列提供了 offer 方法，如果数据项不能添加到队列中，将返回失败状态，客户端可以根据此来调整生产者的数量。类库有 LinkedBlockingQueue 和 ArrayBlockingQueue。 串行线程封闭：对于可变对象，生产者-消费者和阻塞队列一起，促进了穿行线程封闭，从而将对象所有权从生产者交付给消费者。 双端队列：ArrayDeque 和 LinkedBlockingDeque。双端队列可用于另外一种工作模式，工作密取。在该模式下，每个消费者有自己的双端队列，如果一个消费者完成了自己双端队列中的全部工作，那么它可以从其他消费者双端队列末尾秘密获取工作。 阻塞方法与中断方法：线程可能会被阻塞，阻塞原因有：等待 IO 操作，等待一个锁，等待从 sleep 中醒来。阻塞的线程只有得到外部某个事件发生的时候，才能脱离阻塞，回到Runnable 状态。而中断是一种协作机制，一个线程不能强制要求其他线程停止正在执行的操作而去执行其他的操作。 同步工具类： 闭锁（Latch）：闭锁的作用相当于一扇门：在闭锁到达结束状态之前，这扇门一直关闭，并且没有任何线程能通过，当到达结束状态时，这扇门会打开并且允许所有线程通过。一旦到达结束状态后，就再也不会改变状态。在 Java 中可以使用 CountDownLatch。 FutureTask：也可用作闭锁。其状态有三种：等待执行，正在运行，运行完成。Future.get 的行为取决于任务的状态，如果任务完成，立即返回结果，否则将阻塞直到任务完成。 信号量（Semaphore）：计数信号量用来控制访问某个特定资源的操作数量，或者同时执行某个指定操作的数量。可以通过 acquire 和 release 来进行获取和释放信号量的操作。 栅栏（Barrier）：栅栏类似闭锁，它能阻塞一组线程直到某个时间发生。栅栏和闭锁的关键区别在于，所有线程必须同事到达栅栏位置，才能继续执行。闭锁用于等待事件，栅栏则用于等待其他线程。CyclicBarrier 可以使一定数量的参与方反复在栅栏处汇集。当线程达到栅栏的时候，调用 await 方法，这个方法将阻塞直到所有线程都到达栅栏位置。如果所有线程都到达，那么栅栏将会打开，此时所有线程将被释放，而栅栏被重置以便下次使用。如果对 await 的调用超时，那么栅栏就被认为是打破了，所有阻塞的 await 调用将终止并且抛出 BrokenBarrierException。 第六章 任务执行在线程中执行任务： 串行地执行任务：每次只会执行一个任务，但是执行的性能低下。 显式地为任务创建线程：通过为每个请求提供一个新的线程提供服务，实现更高的响应性。 无限制创建线程的不足：线程生命周期开销高，资源消耗，稳定性。 Executor 框架：Java 中，任务执行的主要抽象是 Executor，而不是 Thread。Executor 基于生产者-消费者模式。其接口定义： 123public interface Executor &#123; void execute(Runnable command);&#125; 线程池：指的是管理一组同构工作线程的资源池。通过重用现有的线程而不是创建新的线程来处理新的请求，可以减少新的线程的创建和销毁的开销。通常需要配置一个合适大小的线程池，使得提高处理器的效率和防止过多线程竞争资源使得内存耗尽。在 Java 中可以通过调用静态工厂方法来创建一个线程池：newFixedThreadPool，newCachedThreadPool，newSingleThreadPool，newScheduledThreadPool。 Executor 生命周期：添加了 ExecutorService 接口，其中包含了3种状态：运行，关闭和已终止。shutdown 方法将会执行平缓的关闭过程：不再接受新的任务，同时等待已经提交的任务执行完成。而 shutdownNow 方法则执行粗暴的关闭过程：它将尝试取消所有运行中的任务，同时不再启动队列中尚未开始的任务。 延迟任务和周期管理：Timer 类负责管理延迟任务以及周期任务。Timer 在执行所有的定时任务的时候只会创建一个线程。如果某个任务的执行时间过长，那么将会破坏其他 TimerTask 的定时精确性。基于以上原因，建议使用 ScheduledThreadPoolExecutor。 找出可利用的并行性： 携带结果的任务 Callable 和 Future：Runnable 是一种有很大局限的抽象，虽然 run 能写入到日志文件或者某个共享的数据结构，但是它不能返回一个值或者抛出一个异常。Callable 则是一种更好的抽象，他认为主入口点将返回一个值，并可能抛出一个异常。而 Future 则表示一个任务的生命周期，并且提供相应的方法来判断是否完成，以及获取任务的结果等。 12345678910public interface Callable&lt;V&gt; &#123; V call() throws Exception;&#125;public interface Future&lt;V&gt; &#123; boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException, CancellationException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, CancellationException, TimeoutException;&#125; CompletionService：将 Executor 与 BlockingQueue 的功能结合在一起，通过将一组 Callable 任务提交给它来执行，然后使用 take 和 poll 等方法来获得已经完成的结果，这些结果会被封装成 Future。ExecutorCompletionService 实现了 CompletionService。 为任务设置时限：可以通过 Future.get 来支持该需求。 第七章 取消与关闭任务取消： 中断：线程中断是一种协作机制，每个线程都有一个 boolean 类型的中断状态。当这个线程被被中断的时候，其状态设置为 true。对于中断操作的正确理解：它不会真正地中断一个正在运行的线程，而只是发出中断请求，然后由线程在下一个合适的时刻中断自己。 中断策略：合理的中断策略应该是某种形式的线程级取消操作或者是服务级取消操作。如果需要恢复中断状态： 1Thread.currentThread().interrupt(); 响应中断：当调用可中断的阻塞函数时，如 Thread.sleep，有两种策略用于处理 InterruptException。其一是传递异常，其二是恢复中断状态。 通过 Future 实现取消：ExecutorService.submit 将返回一个 Future 来描述任务。Future 拥有一个 cancel 方法，该方法带有一个 boolean 类型的参数 mayInterruptIfRunning。 采用 newTaskFor 来封装非标准的取消：newTaskFor 是一个工厂方法，它将创建 Future 代表任务，通过定制表示任务的 Future 可以改表 Future.cancel 的行为。 停止基于线程的服务： 关闭 ExecutorService：使用 shutdown 或者 shutdownNow。 毒丸对象：另外一种关闭生产者-消费者的方法就是使用毒丸对象：毒丸是指一个放在队列上的对象，当消费者得到这个对象的时候，立刻停止执行。 shutdownNow 的局限性：使用该方法的时候，它将会取消所有正在执行的任务，并且返回所有已经提交但尚未开始的任务。然而，我们并不知道那些任务已经开始但是尚未正常结束。 处理非正常的线程中止：导致线程提前死亡的原因主要就是 RuntimeException。如果没有捕获该异常，程序就会在控制台打印栈信息，然后退出执行。在 Thread 中提供了 UncaughtExceptionHandler，它能检测出某个线程由于未捕获的异常而终结的情况。 JVM 关闭： 关闭钩子：在正常的关闭中，JVM 首先调用所有已注册过的关闭钩子（Shutdown Hook）。JVM 不保证关闭钩子的调用顺序。 守护线程：守护线程不会阻碍 JVM 的关闭。应该尽量少使用守护线程。 终结期：在回收器释放对象之前，会调用它们的 finalize 方法，从而保证一些持久化的资源被释放。最好不要使用 finalize 方法进行资源回收。 第八章 线程池的使用在任务与执行策略之间的隐性耦合： 线程饥饿死锁：如果两个线程相互依赖对方的执行结果，那么就会发生饥饿死锁。 运行时间较长的任务：如果任务的执行时间较长，那么即使不出现死锁，线程池的响应性也会变得很糟糕。可以限定任务等待资源的事件，而不是无限制的等待。如果等待超时，那么需要中止任务或者将任务重新放回队列中。 设置线程池的大小：配置合适的线程池的大小既不会造成资源的浪费，也不会产生线程频繁切换的代价。 配置 ThreadPoolExecutor：可以使用它的通用构造函数来自定义： 1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; ... &#125; 线程的创建和销毁：线程池的基本大小，最大大小以及存活时间等因素共同负责线程的创建与销毁。 管理任务队列：ThreadPoolExecutor 允许提供一个 BlockingQueue 来保存等待执行的任务。基本的任务排队方式有三种：无界队列，有界队列和同步移交。 饱和策略：当有界队列被填满后，饱和策略开始发挥作用。如中止，抛弃，抛弃最旧的。调用者运行策略既不会抛弃任务，也不会抛出异常，而是将某些任务会退到调用者，从而降低新任务的流量。 线程工厂：每当线程池需要创建一个线程的时候，都是通过线程工厂方法来完成的。通过实现 ThreadFactory 接口，可以进行定制化的工作。 扩展 ThreadPoolExecutor：可以在子类中改写 beforeExecute，afterExecute 和 terminated 方法来实现定制。 第十章 避免活跃性危险死锁： 锁顺序死锁：两个线程试图以不同的顺序获得相同的锁。通过固定顺序获得锁，可以消除该问题。 动态的锁顺序死锁：考虑 transfer(from, to, amount)，如果存在transfer(A, B, 10) 和 transfer(B, A, 10) ，那么就可能发生死锁。 开放调用：如果在调用某个方法的时候不需要持有锁，那么称其为开放调用。 资源死锁：当多个线程相互持有彼此正在等待的锁而又不释放自己已经持有的锁的时候，就会发生死锁。 死锁的避免和检测： 支持定时的锁：可以使用 Lock 类的定时 tryLock 功能来代替内置锁机制。当使用内置锁的时候，只要没有获得锁就会一直等待下去，而显式锁则可以指明一个超时时限。 通过线程转储信息来分析死锁：线程转储信息中包含了加锁信息，例如每个线程持有了哪些锁，在那些栈帧中获得了这些锁，以及被阻塞的线程正在等待哪一个锁。 其他活跃性危险： 饥饿：当线程无法访问它所需要的资源而不能继续执行的时候，就会发生饥饿。引发饥饿的最常见资源就是 CPU 时钟周期。 糟糕的响应性：不良的锁管理也会导致糟糕的响应性。如果某个线程长时间占用锁（容器的迭代），其他想访问该容器的线程就必须等待很长时间。 活锁：该问题尽管不会阻塞线程，但也不能继续执行，因为线程将不断重复执行相同的操作，而且总会失败。可以通过引入随机性来解决该问题。 第十一章 性能与可伸缩性对性能的思考： 性能与可伸缩性：性能可以通过服务时间，延迟时间，吞吐率，效率等指标衡量。可伸缩性指的是当增加计算资源时，程序的吞吐量或者处理能力相应增加。 Amdahl 定律 线程引入的开销： 上下文切换 内存同步：同步可能使用特殊指令，即内存栅栏，该指令可以刷新缓存，使得缓存无效，从而使得各个线程都能看到最新的值。内存栅栏会抑制一些编译器的优化操作。 阻塞：当在锁上发生竞争的时候，竞争失败的线程就会阻塞。JVM 实现阻塞的行为有自旋等待，或者通过操作系统挂起。 减少锁的竞争： 缩小锁的范围 减少锁的粒度 锁分段：如 ConcurrentHashMap 一些替代独占锁的方法：使用并发容器，ReadWriteLock，不可变对象以及原子变量 第十二章 并发程序的测试正确性测试： 基本的单元测试 对阻塞操作的测试 安全性的测试 资源管理的测试 性能测试： 增加计时功能 多种算法比较 响应性衡量 避免性能测试的陷阱： 垃圾回收：垃圾回收的执行时序是无法预测的 动态编译 对代码路径的不真实采样 不真实的竞争程度 无用代码消除 其他的测试方法： 代码审查 静态分析工具 分析与检测工具 第十三章 显式锁Lock 与 ReentrantLock：Lock 接口提供了一种无条件的，可轮询的，定时的以及可中断的锁获取操作。ReentrantLock 则实现了 Lock 接口。 12345678public interface Lock &#123; void lock(); void lockInterruptibly() throws InterruptedException; boolean tryLock(); boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException; void unlock(); Condition newCondition();&#125; 轮询锁与定时锁：由 tryLock 方法实现，同时具有完善的错误恢复机制，使用这两种锁可以避免死锁的发生。 可中断的锁获取操作：lockInterruptibly 方法能够在获得锁的同时保持对中断的响应。 非块结构的加锁：内置锁中，锁的获取和释放操作都是基于代码块的。而分段锁技术则不是块结构的锁。 性能考虑因素：在 Java5 中，当线程数增大的时候，内置锁的性能急剧下降，而 ReentrantLock 的性能下降更加平缓。在 Java6 中，两者的可伸缩性基本相同。 公平性：ReentrantLock 的构造函数中提供了两种公平性的选择：创建一个非公平的锁（默认）或者一个公平的锁。在公平的锁上，线程将按照他们发出请求的顺序来获得锁，但是在非公平的锁上，则允许插队。大多数的情况下，非公平锁的性能高于公平锁的性能。 在 synchronized 和 ReentrantLock 之间进行选择：ReentrantLock 在加锁和内存上提供的语义与内置锁相同，另外，它还实现了其他功能，如定时的锁等待，公平性以及非块结构的加锁。内置锁则更加简洁，同时能在线程转储中给出哪些栈帧获得了哪些锁。 读写锁：ReentrantLock 实现了一种标准的互斥锁，互斥通常是一种过硬的加锁规则，因此限制了并发性。可以使用读写锁来改善： 1234public interface ReadWriteLock &#123; Lock readLock(); Lock writeLock();&#125; 在读写锁的加锁策略中，允许多个操作同时执行，但每次最多只允许一个写操作。ReentrantReadWriteLock 实现了上述接口，提供可重入的语义，同时构造的时候可以选择是否公平锁。 第十四章 构建自定义的同步工具状态依赖性的管理：如 BlockingQueue 中的 put 和 take 操作的前提分别时队列不为空或者满的状态，当前提没有满足的时候，可以抛出异常，或者保持阻塞直到条件被满足。 条件队列：使得一组线程（等待线程集合）能够通过某种方式来等待特定的条件变成真。Object 中的 wait，notify 和 notifyAll 方法就构成了内部条件队列的 API。Object.wait 会自动释放锁，并且请求操作系统挂起当前线程。当被挂起的线程醒来的时候，它将在返回之前重新获取锁。 使用条件队列： 条件谓词：指的是操作正常执行的前提条件，如队列不为空 过早唤醒：wait 方法的返回并不意味着线程正在等待的条件谓词已经变成真的了。因为可能被其他线程通过 notifyAll 唤醒，但是它的条件为此可能并未变为真的，此时就需要再次进行条件判断。 丢失的信号：也是一种活跃性故障。指的是线程必须等待一个已经为真的条件，但在开始等待之前没有检查条件谓词。 通知：在 put 方法成功执行后，将会调用 notifyAll，向任何等待“不为空”条件的线程发出通知。只使用 notify 可能会造成信号丢失的情况。 显式的 Condition 对象：正如 Lock 是一种广义的内置锁，Condition 也是一种广义的内置条件队列。 123456789public interface Condition &#123; void await() throws InterruptedException; boolean await(long time, TimeUnit unit) throws InterruptedException; long awaitNanos(long nanosTimeout) throws InterruptedException; void awaitUninterruptibly(); boolean awaitUntil(Date deadline) throws InterruptedException; void signal(); void signalAll();&#125; 内置锁的缺陷在于每个内置锁都只能有一个相关联的条件队列。而 Condition 和 Lock 一起使用就可以消除该问题。和内置条件队列不同的是，对于每个 Lock，可以有任意数量的 Condition 对象。在 Condition 中相应的方法是 await，signal 和 signalAll。 Synchronizer 剖析：在 ReentrantLock 和 Semaphore 两个接口存在许多共同点，如都可以用作阀门，即每次只允许一定数量的线程通过；都支持可中断；都支持公平和非公平的队列操作。事实上，它们的实现都使用了一个共同的基类，AbstractQueuedSynchronizer（AQS）。AQS 是一个用于构建锁和同步器的框架，CountDownLatch，ReentrantReadWriteLock 和 FutureTask 都是基于 AQS 实现。 同步容器中的 AQS： ReentrantLock：支持独占，实现了 tryAcquire，tryRelease 和 isHeldExclusively。将同步状态用于保存锁获取操作的次数，还维护一些 owner 的变量保存当前所有者线程的标识符。 Semaphore 和 CountDownLatch：前者将同步状态用于保存当前可用许可的数量，后者保存当前的计数值。 FutureTask：同步状态用来保存任务的状态。 第十五章 原子变量与非阻塞同步机制锁的劣势：重量级的同步方式，使用 volatile 变量可以同步，但是不支持原子操作，另外，当一个线程正在等待锁的时候，不能做任何有用的事情。 原子变量类： 原子变量是一种更好的 volatile：原子变量不但支持同步操作，还提供部分原子操作支持。 锁与原子变量的性能比较：在高度竞争的情况下，锁的性能超过原子变量的性能；而在适度竞争情况下，原子变量的性能超过锁的性能。 非阻塞算法：如果在某个算法中，一个线程的失败挥着挂起不会造成其他线程的失败或挂起，俺么该算法就是非阻塞算法。 非阻塞的栈 非阻塞的链表 原子的域更新器：compareAndSet 保证了操作的原子性 第十六章 Java 内存模型内存模型： 在共享内存的多处理器体系结构中，每个处理器有自己的缓存，并且定期的与主内存进行协调。在需要进行内存同步的时候，就可以执行内存栅栏指令，来保证数据的一致性。JVM 通过在合适的位置上插入内存栅栏来屏蔽 JMM 与底层平台内存模型的差异。 重排序 Java 内存模型：如果两个操作之间缺乏 Happens-Before 关系，那么 JVM 就可以对他们进行任意重排序。 Happens-Before 的规则包括： 程序顺序规则 监视器锁规则 volatile 变量规则 线程启动规则 线程结束规则 中断规则 终结期规则 传递性","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.zsstrike.top/tags/Java/"}]},{"title":"《Effective Java》笔记","slug":"《Effective-Java》笔记","date":"2020-12-06T10:04:57.000Z","updated":"2020-12-17T10:23:43.559Z","comments":true,"path":"2020/12/06/《Effective-Java》笔记/","link":"","permalink":"http://blog.zsstrike.top/2020/12/06/%E3%80%8AEffective-Java%E3%80%8B%E7%AC%94%E8%AE%B0/","excerpt":"本文是《Effective Java》第三版的读书笔记。","text":"本文是《Effective Java》第三版的读书笔记。 第二章 创建和销毁对象 考虑使用静态工厂方法代替构造函数：获取一个类的实例的传统方式是使用类提供的公开构造函数，另外一种方法是类提供公开静态工厂方法，用于返回实例。使用静态工厂方法优点： 静态工厂方法有确切名称，便于阅读 静态工厂方法不需要在每次调用时创建新对象 可以通过静态工厂方法获取返回类型的任何子类的对象，提供灵活性 返回对象的类可以随调用的不同而变化，作为输入参数的函数，声明的返回类型的任何子类型都是允许的 当编写包含方法的类时，返回对象的类不需要存在，如JDBC 静态工厂方法缺点： 没有公共或受保护构造函数的类不能被子类化 程序员很难找到它们，下面是一些静态工厂方法的常用名称： from：一种类型转换方法，接收单个参数并且返回相应实例 of：一个聚合方法，接受多个参数返回一个实例 valueOf：替代from和of但是更加冗长的方法 instance或getInstance：返回一个实例，该实例由参数描述，但具有不同的值（可能会缓存） create或newInstance：该方法保证每个调用都返回一个新实例 getType：类似于 getInstance，但如果工厂方法位于不同的类中，则使用此方法 newType：与 newInstance 类似，但是如果工厂方法在不同的类中使用 type：一个用来替代 getType 和 newType 的比较简单的方式 当构造函数有多个参数的时候，考虑使用构造器：静态工厂和构造函数都有一个局限，就是不能对大量可选参数做很好的扩展。当我们的可选参数个数大于4个时，往往需要重载很多个构造函数，会降低代码的可维护性。另外一种选择是JavaBean模式，但是JavaBean可能在构建的过程中处于不一致状态。此时我们可以使用构造器来生成所需对象。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// Builder Patternpublic class NutritionFacts &#123; private final int servingSize; private final int servings; private final int calories; private final int fat; private final int sodium; private final int carbohydrate; public static class Builder &#123; // Required parameters private final int servingSize; private final int servings; // Optional parameters - initialized to default values private int calories = 0; private int fat = 0; private int sodium = 0; private int carbohydrate = 0; public Builder(int servingSize, int servings) &#123; this.servingSize = servingSize; this.servings = servings; &#125; public Builder calories(int val) &#123; calories = val; return this; &#125; public Builder fat(int val) &#123; fat = val; return this; &#125; public Builder sodium(int val) &#123; sodium = val; return this; &#125; public Builder carbohydrate(int val) &#123; carbohydrate = val; return this; &#125; public NutritionFacts build() &#123; return new NutritionFacts(this); &#125; &#125; private NutritionFacts(Builder builder) &#123; servingSize = builder.servingSize; servings = builder.servings; calories = builder.calories; fat = builder.fat; sodium = builder.sodium; carbohydrate = builder.carbohydrate; &#125;&#125; 这样我们在生成代码的时候就可以通过链式调用来生成我们的对象实例。构造器模式很灵活，一个构造器可以构造多个对象。但是构造器的缺点就是为了创建一个对象，必须首先创建它的构造器。 使用私有构造函数或枚举类型实施单例模式：实现单例模式的第一种方法： 123456// Singleton with public final fieldpublic class Elvis &#123; public static final Elvis INSTANCE = new Elvis(); private Elvis() &#123; ... &#125; public void leaveTheBuilding() &#123; ... &#125;&#125; 上述代码可以防止用户来自己创建Elvis实例。但是拥有特殊权限的客户端可以借助AccessibleObject.setAccessible 方法利用反射调用私有构造函数，如果需要防范这种问题，需要修改构造器，使其在请求创建第二个实例的时候抛出异常即可。另外一种方法： 1234567// Singleton with static factorypublic class Elvis &#123; private static final Elvis INSTANCE = new Elvis(); private Elvis() &#123; ... &#125; public static Elvis getInstance() &#123; return INSTANCE; &#125; public void leaveTheBuilding() &#123; ... &#125;&#125; 静态工厂方法的一个优点是，它可以在不更改 API 的情况下决定类是否是单例，如为每个线程返回一个单例；第二个优点是，如果应用程序需要的话，可以编写泛型的单例工厂。实现单例的第三种方法： 12345// Enum singleton - the preferred approachpublic enum Elvis &#123; INSTANCE; public void leaveTheBuilding() &#123; ... &#125;&#125; 这种方法类似于 public 字段方法，但是它更简洁，默认提供了序列化机制，提供了对多个实例化的严格保证，即使面对复杂的序列化或反射攻击也是如此。 用私有构造函数实现不可实例化：对于一个工具类库，如Arrays，实例化这些类是没有意义的。试图通过使类抽象来实施不可实例化是行不通的。因为可以对类进行子类化，并实例化子类。有一个简单的习惯用法来确保不可实例化。只有当类不包含显式构造函数时，才会生成默认构造函数，因此可以通过包含私有构造函数使类不可实例化： 1234567// Noninstantiable utility classpublic class UtilityClass &#123; // Suppress default constructor for noninstantiability private UtilityClass() &#123; throw new AssertionError(); &#125; ... // Remainder omitted&#125; 因为显式构造函数是私有的，所以在类之外是不可访问的。AssertionError 不是严格要求的，但是它提供了保障，以防构造函数意外地被调用。 依赖注入优于硬连接资源：尽量将类依赖的资源在创建新实例时将资源传递给构造函数，从而实现依赖注入。 123456789// Dependency injection provides flexibility and testabilitypublic class SpellChecker &#123; private final Lexicon dictionary; public SpellChecker(Lexicon dictionary) &#123; this.dictionary = Objects.requireNonNull(dictionary); &#125; public boolean isValid(String word) &#123; ... &#125; public List&lt;String&gt; suggestions(String typo) &#123; ... &#125;&#125; 另外，这种模式的一个有用变体是将资源工厂传递给构造函数。Java 8 中引入的 Supplier&lt;T&gt; 非常适合表示工厂。尽管依赖注入极大地提高了灵活性和可测试性，但它可能会使大型项目变得混乱，这些项目通常包含数千个依赖项。 避免创建不必要的对象：作为一个不该做的极端例子，请考虑下面的语句： 1String s = new String(&quot;bikini&quot;); // DON&#x27;T DO THIS! 该语句每次执行时都会创建一个新的 String 实例，而这些对象创建都不是必需的。String 构造函数的参数 (&quot;bikini&quot;) 本身就是一个 String 实例，在功能上与构造函数创建的所有对象相同。另外，有些对象的创建代价很高，如果你需要重复地使用这样一个「昂贵的对象」，那么最好将其缓存以供复用： 1234567// Reusing expensive object for improved performancepublic class RomanNumerals &#123; private static final Pattern ROMAN = Pattern.compile(&quot;^(?=.)M*(C[MD]|D?C&#123;0,3&#125;)&quot; + &quot;(X[CL]|L?X&#123;0,3&#125;)(I[XV]|V?I&#123;0,3&#125;)$&quot;); static boolean isRomanNumeral(String s) &#123; return ROMAN.matcher(s).matches(); &#125;&#125; 另外，还需要注意基本类型优于包装类，需要提防意外的自动自动装箱。 排除过时的对象引用：考虑一个栈的pop操作： 12345public Object pop() &#123; if (size == 0) throw new EmptyStackException(); return elements[--size];&#125; 上述代码没有明显的问题，但是存在内存泄露的隐患：如果堆栈增长，然后收缩，那么从堆栈中弹出的对象将不会被垃圾收集，即使使用堆栈的程序不再引用它们。改进方式： 1234567public Object pop() &#123; if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result;&#125; 一般来说，一个类管理它自己的内存时，程序员应该警惕内存泄漏。当释放一个元素时，该元素中包含的任何对象引用都应该被置为 null。 另一个常见的内存泄漏源是缓存。一旦将对象引用放入缓存中，就很容易忘记它就在那里，并且在它变得无关紧要之后很久仍将它留在缓存中。如果你非常幸运地实现了一个缓存，只要缓存外有对其键的引用，那么就将缓存表示为 WeakHashMap，当条目过时后，条目将被自动删除。 内存泄漏的第三个常见来源是侦听器和其他回调。 如果你实现了一个 API，其中客户端注册回调，但不显式取消它们，除非你采取一些行动，否则它们将累积。 避免使用终结器和清除器：终结器是不可预测的，通常是危险的，也是不必要的。清除器的危险比终结器小，但仍然不可预测、缓慢，而且通常是不必要的。终结器和清除器的一个缺点是不能保证它们会被立即执行，另外一个缺点是它们可能会使的即将要被清理的对象死而复生。终结器和清除器可以充当一个安全网，以防资源的所有者忽略调用它的 close 方法。 使用try-with-resources优于try-finally：从历史上看，try-finally 语句是确保正确关闭资源的最佳方法，即使在出现异常或返回时也是如此。但是当存在两个资源的时候，可能就需要嵌套的调用了，这会导致代码不易阅读。最好的方法就是使用try-with-resources： 123456789// try-with-resources on multiple resources - short and sweetstatic void copy(String src, String dst) throws IOException &#123; try (InputStream in = new FileInputStream(src);OutputStream out = new FileOutputStream(dst)) &#123; byte[] buf = new byte[BUFFER_SIZE]; int n; while ((n = in.read(buf)) &gt;= 0) out.write(buf, 0, n); &#125;&#125; 第三章 对象的通用方法 覆盖 equals 方法时应该遵守的约定：当满足下面的条件的时候，不应该覆盖equals方法： 类的每个实例本质上是唯一的 该类不需要提供逻辑相等测试 超类已经覆盖了equals，超类行为适合于这个类 类是私有的或者包私有的，并且你确信它的 equals 方法永远不会被调用 equals方法实现了等价关系：反身性，对称性，传递性，一致性，最后还需要满足非无效性：即o.equals(null)返回false。为了搞笑实现equals方法，需要： 使用 == 运算符检查参数是否是对该对象的引用 使用 instanceof 运算符检查参数是否具有正确的类型 将参数转换为正确的类型 对于类中的每个「重要」字段，检查参数的字段是否与该对象的相应字段匹配 是否满足等价关系 当覆盖 equals 方法的时候，总要覆盖 hashCode 方法：由于相等的对象必须具有相等的散列码，如果PhoneNumber没有实现hashCode方法的话： 123Map&lt;PhoneNumber, String&gt; m = new HashMap&lt;&gt;();m.put(new PhoneNumber(707, 867, 5309), &quot;Jenny&quot;);// m.get(new PhoneNumber(707, 867,5309)) == null 第三行的结果将是null，而不是&quot;Jenny&quot;。实现hashCode方法的一个简单方法步骤： 声明一个名为 result 的 int 变量，并将其初始化为对象中第一个重要字段的散列码 c 对象中剩余的重要字段 f，执行以下操作： 为字段计算一个整数散列码 c：如果字段是基本数据类型，计算 Type.hashCode(f)，其中 type 是与 f 类型对应的包装类。如果字段是对象引用，并且该类的 equals 方法通过递归调用 equals 方法来比较字段，则递归调用字段上的 hashCode 方法。如果字段是一个数组，则将其每个重要元素都视为一个单独的字段。也就是说，通过递归地应用这些规则计算每个重要元素的散列码，并将每个步骤 2.b 的值组合起来。如果数组中没有重要元素，则使用常量，最好不是 0。如果所有元素都很重要，那么使用 Arrays.hashCode。 将步骤 2.a 中计算的散列码 c 合并到 result 变量 返回result 一个简单的demo： 12345678// Typical hashCode method@Overridepublic int hashCode() &#123; int result = Short.hashCode(areaCode); result = 31 * result + Short.hashCode(prefix); result = 31 * result + Short.hashCode(lineNum); return result;&#125; 始终覆盖 toString 方法：虽然Object提供了默认的toString方法，但是它返回的字符串通常不是用户希望看到的。提供一个好的 toString 实现（能）使类更易于使用，使用该类的系统（也）更易于调试。当实际使用时，toString 方法应该返回对象中包含的所有有用信息。 明智地覆盖 clone 方法：Cloneable 接口的目的是作为 mixin 接口，用于让类来宣称它们允许克隆。不幸的是，它没有达到这个目的。它的主要缺点是缺少 clone 方法，并且 Object 类的 clone 方法是受保护的。它决定了 Object 类受保护的 clone 实现的行为：如果一个类实现了 Cloneable 接口，Object 类的 clone 方法则返回该类实例的逐字段拷贝；否则它会抛出 CloneNotSupportedException。默认提供的clone方法执行的是浅拷贝，如果需要深拷贝，就需要自己覆盖clone方法，实现该功能。 考虑实现 Comparable 接口：与本章讨论的其他方法不同，compareTo 方法不是在 Object 中声明的。相反，它是 Comparable 接口中的唯一方法。通过让类实现 Comparable，就可与依赖于此接口的所有通用算法和集合实现进行互操作。如果一个类有多个重要的字段，此时就需要用户来指定对应的比较顺序。在 Java 8 中，Comparator 接口配备了一组比较器构造方法，可以流畅地构造比较器。然后可以使用这些比较器来实现 Comparator 接口所要求的 compareTo 方法。 12345678// Comparable with comparator construction methodsprivate static final Comparator&lt;PhoneNumber&gt; COMPARATOR = comparingInt((PhoneNumber pn) -&gt; pn.areaCode) .thenComparingInt(pn -&gt; pn.prefix) .thenComparingInt(pn -&gt; pn.lineNum);public int compareTo(PhoneNumber pn) &#123; return COMPARATOR.compare(this, pn);&#125; 第四章 类和接口 尽量减少类和成员的可访问性：隐藏内部数据和其他实现细节用于实现信息封装，可以解耦组成系统的组件。通用方法是让每个类或者成员尽可能不可访问。对于顶级（非嵌套）类和接口，只有两个可能的访问级别：包私有和公共。如果一个方法覆盖了超类方法，那么它在子类中的访问级别就不能比超类更严格。公共类的实例字段很少采用 public 修饰，因为带有公共可变字段的类通常不是线程安全的。请注意，非零长度的数组总是可变的，因此对于类来说，拥有一个公共静态 final 数组字段或返回该字段的访问器是错误的。如果一个类具有这样的字段或访问器，客户端将能够修改数组的内容。对于 Java 9，作为模块系统的一部分，还引入了另外两个隐式访问级别。模块是包的分组单位，就像包是类的分组单位一样。模块可以通过模块声明中的导出声明显式地导出它的一些包。 在公共类中，使用访问器方法，而不是公共字段：如果类可以在包之外访问，那么提供访问器方法来保持更改类内部表示的灵活性。但是，如果一个类是包级私有的或者是私有嵌套类，那么公开它的数据字段并没有什么本质上的错误。无论是在类定义还是在使用它的客户端代码中，这种方法产生的视觉混乱都比访问方法少。虽然公共类直接公开字段从来都不是一个好主意，但是如果字段是不可变的，那么危害就会小一些。 减少可变性：不可变类就是一个实例不能被修改的类。要使类不可变，请遵循以下 5 条规则： 不要提供修改对象状态的方法 确保类不能被扩展 所有字段用 final 修饰 所有字段设为私有 确保对任何可变组件的独占访问 不可变对象提供的好处： 不可变对象本质上是线程安全的 不可变对象可以很好的作为其他对象的构建模块 不可变对象自带提供故障原子性。他们的状态从未改变，所以不可能出现暂时的不一致。 不可变类的主要缺点是每个不同的值都需要一个单独的对象。 优先选择复合而不是继承：在包中使用继承是安全的，其中子类和超类实现由相同的程序员控制。在对专为扩展而设计和文档化的类时使用继承也是安全的。然而，对普通的具体类进行跨包边界的继承是危险的。与方法调用不同，继承破坏了封装。换句话说，子类的功能正确与否依赖于它的超类的实现细节。子类脆弱的一个原因是他们的超类可以在后续版本中获得新的方法。有一种方法可以避免上述所有问题。与其扩展现有类，不如为新类提供一个引用现有类实例的私有字段。这种设计称为复合，因为现有的类是新类的一个组件。只有在子类确实是超类的子类型的情况下，继承才合适。换句话说，只有当两个类之间存在「is-a」关系时，类 B 才应该扩展类 A。 继承要设计良好并且具有文档，否则禁止使用：首先，类必须精确地在文档中记录覆盖任何方法的效果。换句话说，类必须在文档中记录它对可覆盖方法的自用性。对于每个公共或受保护的方法，文档必须指出方法调用的可覆盖方法、调用顺序以及每次调用的结果如何影响后续处理过程。但是，这是否违背了一个格言：好的 API 文档应该描述一个给定的方法做什么，而不是如何做？是的，它确实违背了！这是继承违反封装这一事实的不幸结果。要为一个类编制文档，使其能够安全地子类化，你必须描述实现细节，否则这些细节应该是未指定的。为了允许继承，类必须遵守更多的限制。构造函数不能直接或间接调用可重写的方法。 如果你违反了这个规则，程序就会失败。超类构造函数在子类构造函数之前运行，因此在子类构造函数运行之前将调用子类中的覆盖方法。如果重写方法依赖于子类构造函数执行的任何初始化，则该方法的行为将不像预期的那样。 接口优于抽象类：Java 有两种机制来定义允许多种实现的类型：接口和抽象类。由于 Java 8 中引入了接口的默认方法，这两种机制都允许你为一些实例方法提供实现。一个主要区别是，一个类要实现抽象类定义的类型，该类必须是抽象类的子类。因为 Java 只允许单一继承，这种限制对抽象类而言严重制约了它们作为类型定义的使用。使用接口的优点： 可以很容易地对现有类进行改造，以实现新的接口 接口是定义 mixin（混合类型）的理想工具 接口允许构造非层次化类型框架 为后代设计接口：在 Java 8 之前，在不破坏现有实现的情况下向接口添加方法是不可能的。如果在接口中添加新方法，通常导致现有的实现出现编译时错误，提示缺少该方法。在 Java 8 中，添加了默认的方法构造，目的是允许向现有接口添加方法。除非必要，否则应该避免使用默认方法向现有接口添加新方法，在这种情况下，你应该仔细考虑现有接口实现是否可能被默认方法破坏。尽管默认方法现在已经是 Java 平台的一部分，但是谨慎地设计接口仍然是非常重要的。虽然默认方法使向现有接口添加方法成为可能，但这样做存在很大风险。 如果一个接口包含一个小缺陷，它可能会永远影响它的使用者；如果接口有严重缺陷，它可能会毁掉包含它的 API。 接口只用于定义类型：当一个类实现了一个接口时，这个接口作为一种类型，可以用来引用类的实例。不满足上述条件的一种接口是所谓的常量接口。如果你想导出常量，有几个合理的选择。如果这些常量与现有的类或接口紧密绑定，则应该将它们添加到类或接口。例如，所有数值包装类，比如 Integer 和 Double，都导出 MIN_VALUE 和 MAX_VALUE 常量。如果将这些常量看作枚举类型的成员，那么应该使用 enum 类型导出它们。否则，你应该使用不可实例化的工具类导出常量。 类层次结构优于带标签的类：有时候，你可能会遇到这样一个类，它的实例有两种或两种以上的样式，并且包含一个标签字段来表示实例的样式。这样的标签类有许多缺点。它们充斥着样板代码，包括 enum 声明、标签字段和 switch 语句。标签类冗长、容易出错和低效。面向对象的语言提供了一个更好的选择来定义能够表示多种类型对象的单一数据类型：子类型。标签类只是类层次结构的简易模仿。 静态成员类优于非静态成员类：有四种嵌套类：静态成员类、非静态成员类、匿名类和局部类。除了第一种，所有的类都被称为内部类。静态成员类是最简单的嵌套类。最好把它看做是一个普通的类，只是碰巧在另一个类中声明而已，并且可以访问外部类的所有成员，甚至那些声明为 private 的成员。静态成员类的一个常见用法是作为公有的辅助类。从语法上讲，静态成员类和非静态成员类之间的唯一区别是静态成员类在其声明中具有修饰符 static。如果声明的成员类不需要访问外部的实例，那么应始终在声明中添加 static 修饰符，使其成为静态的而不是非静态的成员类。匿名类的适用性有很多限制。你不能实例化它们，除非在声明它们的时候。在 lambda 表达式被添加到 Java 之前，匿名类是动态创建小型函数对象和进程对象的首选方法，但 lambda 表达式现在是首选方法。局部类是四种嵌套类中最不常用的。局部类几乎可以在任何能够声明局部变量的地方使用，并且遵守相同的作用域规则。局部类具有与其他嵌套类相同的属性。 源文件仅限有单个顶层类：虽然 Java 编译器允许你在单个源文件中定义多个顶层类，但这样做没有任何好处，而且存在重大风险。这种风险源于这样一个事实：在源文件中定义多个顶层类使得为一个类提供多个定义成为可能。 第五章 泛型 不要使用原始类型：声明中具有一个或多个类型参数的类或接口就是泛型类或泛型接口，每个泛型都定义了一个原始类型，它是没有任何相关类型参数的泛型的名称。例如，List&lt;E&gt; 对应的原始类型是 List。原始类型的行为就好像所有泛型信息都从类型声明中删除了一样。它们的存在主要是为了与之前的泛型代码兼容。当从集合中检索元素时，编译器会为你执行不可见的强制类型转换，并确保它们不会失败。使用原始类型（没有类型参数的泛型）是合法的，但是你永远不应该这样做。如果使用原始类型，就会失去泛型的安全性和表现力。考虑如下程序： 1234567891011// Fails at runtime - unsafeAdd method uses a raw type (List)!public static void main(String[] args) &#123; List&lt;String&gt; strings = new ArrayList&lt;&gt;(); unsafeAdd(strings, Integer.valueOf(42)); String s = strings.get(0); // Has compiler-generated cast&#125;private static void unsafeAdd(List list, Object o) &#123; list.add(o);&#125; 该程序可以编译，但因为它使用原始类型 List，所以你会得到一个警告： 1234Test.java:10: warning: [unchecked] unchecked call to add(E) as amember of the raw type Listlist.add(o);^ 实际上，如果你运行程序，当程序试图将调用 strings.get(0) 的结果强制转换为字符串时，你会得到一个 ClassCastException。这是一个由编译器生成的强制类型转换，它通常都能成功，但在本例中，我们忽略了编译器的警告，并为此付出了代价。 如果将 unsafeAdd 声明中的原始类型 List 替换为参数化类型 List，并尝试重新编译程序，你会发现它不再编译，而是发出错误消息： 1234Test.java:5: error: incompatible types: List&lt;String&gt; cannot beconverted to List&lt;Object&gt;unsafeAdd(strings, Integer.valueOf(42));^ 对于元素类型未知且无关紧要的集合，你可能会尝试使用原始类型。这种方法是可行的，但是它使用的是原始类型，这是很危险的。安全的替代方法是使用无界通配符类型。如果你想使用泛型，但不知道或不关心实际的类型参数是什么，那么可以使用问号代替。 12&#x2F;&#x2F; Uses unbounded wildcard type - typesafe and flexiblestatic int numElementsInCommon(Set&lt;?&gt; s1, Set&lt;?&gt; s2) &#123; ... &#125; 对于不应该使用原始类型的规则，有一些小的例外。必须在类字面量中使用原始类型。换句话说，List.class，String[].class 和 int.class 都是合法的，但是 List.class 和 List.class 不是。第二个例外是 instanceof 运算符。由于泛型信息在运行时被删除，因此在不是无界通配符类型之外的参数化类型上使用 instanceof 操作符是非法的。使用无界通配符类型代替原始类型不会以任何方式影响 instanceof 运算符的行为。在这种情况下，尖括号和问号只是多余的。下面的例子是使用通用类型 instanceof 运算符的首选方法： 12345// Legitimate use of raw type - instanceof operatorif (o instanceof Set) &#123; // Raw type Set&lt;?&gt; s = (Set&lt;?&gt;) o; // Wildcard type ...&#125; 总之，使用原始类型可能会在运行时导致异常，所以不要轻易使用它们。它们仅用于与引入泛型之前的遗留代码进行兼容和互操作。快速回顾一下，Set 是一个参数化类型，表示可以包含任何类型的对象的集合，Set 是一个通配符类型，表示只能包含某种未知类型的对象的集合，Set 是一个原始类型，它选择了泛型系统。前两个是安全的，后一个就不安全了。 消除 unchecked 警告：使用泛型获得的经验越多，得到的警告就越少，但是不要期望新编写的代码能够完全正确地编译。力求消除所有 unchecked 警告。 如果你消除了所有警告，你就可以确信你的代码是类型安全的，这是一件非常好的事情。如果不能消除警告，但是可以证明引发警告的代码是类型安全的，那么（并且只有在那时）使用 SuppressWarnings(“unchecked”) 注解来抑制警告。SuppressWarnings 注解可以用于任何声明中，从单个局部变量声明到整个类。总是在尽可能小的范围上使用 SuppressWarnings 注解。 每次使用SuppressWarnings(“unchecked”) 注解时，要添加一条注释，说明这样做是安全的。 list 优于数组：数组与泛型有两个重要区别。首先，数组是协变的。这个听起来很吓人的单词的意思很简单，如果 Sub 是 Super 的一个子类型，那么数组类型 Sub[] 就是数组类型 Super[] 的一个子类型。数组和泛型之间的第二个主要区别：数组是具体化的。这意味着数组在运行时知道并强制执行他们的元素类型。相比之下，泛型是通过擦除来实现的。 由于这些基本差异，数组和泛型不能很好地混合。例如，创建泛型、参数化类型或类型参数的数组是非法的。因此，这些数组创建表达式都不是合法的：new List[]、new List[]、new E[]。所有这些都会在编译时导致泛型数组创建错误。为了更具体，请考虑以下代码片段： 123456// Why generic array creation is illegal - won&#x27;t compile!List&lt;String&gt;[] stringLists = new List&lt;String&gt;[1]; // (1)List&lt;Integer&gt; intList = List.of(42); // (2)Object[] objects = stringLists; // (3)objects[0] = intList; // (4)String s = stringLists[0].get(0); // (5) 假设创建泛型数组的第 1 行是合法的。第 2 行创建并初始化一个包含单个元素的 List。第 3 行将 List 数组存储到 Object 类型的数组变量中，这是合法的，因为数组是协变的。第 4 行将 List 存储到 Object 类型的数组的唯一元素中，这是成功的，因为泛型是由擦除实现的：List 实例的运行时类型是 List，List[] 实例的运行时类型是 List[]，因此这个赋值不会生成 ArrayStoreException。现在我们有麻烦了。我们将一个 List 实例存储到一个数组中，该数组声明只保存 List 实例。在第 5 行，我们从这个数组的唯一列表中检索唯一元素。编译器自动将检索到的元素转换为 String 类型，但它是一个 Integer 类型的元素，因此我们在运行时得到一个 ClassCastException。为了防止这种情况发生，第 1 行（创建泛型数组）必须生成编译时错误。 当你在转换为数组类型时遇到泛型数组创建错误或 unchecked 强制转换警告时，通常最好的解决方案是使用集合类型 List，而不是数组类型 E[]。 总之，数组和泛型有非常不同的类型规则。数组是协变的、具体化的；泛型是不变的和可被擦除的。因此，数组提供了运行时类型安全，而不是编译时类型安全，对于泛型反之亦然。一般来说，数组和泛型不能很好地混合。如果你发现将它们混合在一起并得到编译时错误或警告，那么你的第一个反应该是将数组替换为 list。 优先使用泛型：考虑一个泛型栈结构： 123public Stack() &#123; elements = new E[DEFAULT_INITIAL_CAPACITY];&#125; 通常至少会得到一个错误或警告，这个类也不例外。幸运的是，这个类只生成一个错误： 123Stack.java:8: generic array creationelements &#x3D; new E[DEFAULT_INITIAL_CAPACITY];^ 每当你编写由数组支持的泛型时，就会出现这个问题。有两种合理的方法来解决它。第一个解决方案直接绕过了创建泛型数组的禁令：创建对象数组并将其强制转换为泛型数组类型。现在，编译器将发出一个警告来代替错误。这种用法是合法的，但（一般而言）它不是类型安全的： 1234Stack.java:8: warning: [unchecked] unchecked castfound: Object[], required: E[]elements &#x3D; (E[]) new Object[DEFAULT_INITIAL_CAPACITY];^ 消除 Stack 中泛型数组创建错误的第二种方法是将字段元素的类型从 E[] 更改为 Object[]。如果你这样做，你会得到一个不同的错误： 1234Stack.java:19: incompatible typesfound: Object, required: EE result &#x3D; elements[--size];^ 通过将从数组中检索到的元素转换为 E，可以将此错误转换为警告，但你将得到警告： 1234Stack.java:19: warning: [unchecked] unchecked castfound: Object, required: EE result &#x3D; (E) elements[--size];^ 消除泛型数组创建的两种技术都有其追随者。第一个更容易读：数组声明为 E[] 类型，这清楚地表明它只包含 E 的实例。它也更简洁：在一个典型的泛型类中，从数组中读取代码中的许多点；第一种技术只需要一次转换（在创建数组的地方），而第二种技术在每次读取数组元素时都需要单独的转换。因此，第一种技术是可取的，在实践中更常用。 泛型比需要在客户端代码中转换的类型更安全、更容易使用。 优先使用泛型方法：允许类型参数被包含该类型参数本身的表达式限制，尽管这种情况比较少见。这就是所谓的递归类型限定。递归类型边界的一个常见用法是与 Comparable 接口相关联，后者定义了类型的自然顺序： 123public interface Comparable&lt;T&gt; &#123; int compareTo(T o);&#125; 许多方法采用实现 Comparable 的元素集合，在其中进行搜索，计算其最小值或最大值，等等。要做到这些，需要集合中的每个元素与集合中的每个其他元素相比较，换句话说，就是列表中的元素相互比较。 12// Using a recursive type bound to express mutual comparabilitypublic static &lt;E extends Comparable&lt;E&gt;&gt; E max(Collection&lt;E&gt; c); 类型限定 &lt;E extends Comparable&lt;E&gt;&gt; 可以被理解为「可以与自身进行比较的任何类型 E」，这或多或少与相互可比性的概念相对应。 使用有界通配符增加 API 的灵活性：假设我们想添加一个方法，该方法接受一系列元素并将它们全部推入堆栈。这是第一次尝试： 12345// pushAll method without wildcard type - deficient!public void pushAll(Iterable&lt;E&gt; src) &#123; for (E e : src) push(e);&#125; 该方法能够正确编译，但并不完全令人满意。下面的代码将会产生错误： 123Stack&lt;Number&gt; numberStack = new Stack&lt;&gt;();Iterable&lt;Integer&gt; integers = ... ;numberStack.pushAll(integers); 错误信息： 1234StackTest.java:7: error: incompatible types: Iterable&lt;Integer&gt;cannot be converted to Iterable&lt;Number&gt; numberStack.pushAll(integers); ^ Java 提供了一种特殊的参数化类型，有界通配符类型来处理这种情况。pushAll 的输入参数的类型不应该是「E 的 Iterable 接口」，而应该是「E 的某个子类型的 Iterable 接口」，并且有一个通配符类型，它的确切含义是：Iterable&lt;? extends E&gt;： 12345// Wildcard type for a parameter that serves as an E producerpublic void pushAll(Iterable&lt;? extends E&gt; src) &#123; for (E e : src) push(e);&#125; popAll方法的代码如下： 12345// popAll method without wildcard type - deficient!public void popAll(Collection&lt;E&gt; dst) &#123; while (!isEmpty()) dst.add(pop());&#125; 同样，如果目标集合的元素类型与堆栈的元素类型完全匹配，那么这种方法可以很好地编译。但这也不是完全令人满意： 123Stack&lt;Number&gt; numberStack = new Stack&lt;Number&gt;();Collection&lt;Object&gt; objects = ... ;numberStack.popAll(objects); 同样，有一个通配符类型，它的确切含义是：Collection&lt;? super E&gt;。 12345// Wildcard type for parameter that serves as an E consumerpublic void popAll(Collection&lt;? super E&gt; dst) &#123; while (!isEmpty()) dst.add(pop());&#125; 总而言之，PECS 表示生产者应使用 extends，消费者应使用 super。 明智地合用泛型和可变参数：泛型和可变参数不能很好的结合起来。考虑如下代码： 12345678// Mixing generics and varargs can violate type safety!// 泛型和可变参数混合使用可能违反类型安全原则！static void dangerous(List&lt;String&gt;... stringLists) &#123; List&lt;Integer&gt; intList = List.of(42); Object[] objects = stringLists; objects[0] = intList; // Heap pollution String s = stringLists[0].get(0); // ClassCastException&#125; 此方法没有显式的强制类型转换，但在使用一个或多个参数调用时抛出 ClassCastException。它的最后一行有一个由编译器生成的隐式强制转换。此转换失败，表明类型安全性受到了影响，并且在泛型可变参数数组中存储值是不安全的。为什么使用泛型可变参数声明方法是合法的，而显式创建泛型数组是非法的？答案是，带有泛型或参数化类型的可变参数的方法在实际开发中非常有用，因此语言设计人员选择忍受这种不一致性。事实上，Java 库导出了几个这样的方法，包括 Arrays.asList(T… a)、Collections.addAll(Collection&lt;? super T&gt; c, T… elements) 以及 EnumSet.of(E first, E… rest)。 在 Java 7 中添加了 SafeVarargs 注释，以允许使用泛型可变参数的方法的作者自动抑制客户端警告。本质上，SafeVarargs 注释构成了方法作者的一个承诺，即该方法是类型安全的。 可变参数方法和泛型不能很好地交互，因为可变参数工具是构建在数组之上的漏洞抽象，并且数组具有与泛型不同的类型规则。虽然泛型可变参数不是类型安全的，但它们是合法的。如果选择使用泛型（或参数化）可变参数编写方法，首先要确保该方法是类型安全的，然后使用 @SafeVarargs 对其进行注释。 考虑类型安全的异构容器： 123456789101112// Typesafe heterogeneous container pattern - implementationpublic class Favorites &#123; private Map&lt;Class&lt;?&gt;, Object&gt; favorites = new HashMap&lt;&gt;(); public &lt;T&gt; void putFavorite(Class&lt;T&gt; type, T instance) &#123; favorites.put(Objects.requireNonNull(type), instance); &#125; public &lt;T&gt; T getFavorite(Class&lt;T&gt; type) &#123; return type.cast(favorites.get(type)); &#125;&#125; 这里发生了一些微妙的事情。每个 Favorites 实例都由一个名为 favorites 的私有 Map&lt;Class&lt;?&gt;, Object&gt; 支持。你可能认为由于通配符类型是无界的，所以无法将任何内容放入此映射中，但事实恰恰相反。需要注意的是，通配符类型是嵌套的：通配符类型不是 Map 的类型，而是键的类型。这意味着每个键都可以有不同的参数化类型：一个可以是 Class&lt;String&gt;，下一个是 Class&lt;Integer&gt;，等等。这就是异构的原理。 接下来要注意的是 favorites 的值类型仅仅是 Object。换句话说，Map 不保证键和值之间的类型关系，即每个值都是其键所表示的类型。实际上，Java 的类型系统还没有强大到足以表达这一点。但是我们知道这是事实，当需要检索一个 favorite 时，我们会利用它。 putFavorite 的实现很简单：它只是将从给定 Class 对象到给定 Favorites 实例的放入 favorites 中。如前所述，这将丢弃键和值之间的「类型关联」；将无法确定值是键的实例。但这没关系，因为 getFavorites 方法可以重新建立这个关联。 getFavorite 的实现比 putFavorite 的实现更复杂。首先，它从 favorites 中获取与给定 Class 对象对应的值。这是正确的对象引用返回，但它有错误的编译时类型：它是 Object（favorites 的值类型），我们需要返回一个 T。因此，getFavorite 的实现通过使用 Class 的 cast 方法，将对象引用类型动态转化为所代表的 Class 对象。 总之，以集合的 API 为例的泛型在正常使用时将每个容器的类型参数限制为固定数量。你可以通过将类型参数放置在键上而不是容器上来绕过这个限制。你可以使用 Class 对象作为此类类型安全异构容器的键。以这种方式使用的 Class 对象称为类型标记。还可以使用自定义键类型。例如，可以使用 DatabaseRow 类型表示数据库行（容器），并使用泛型类型 Column&lt;T&gt; 作为它的键。 第六章 枚举和注解 用枚举类型代替 int 常量：在枚举类型被添加到 JAVA 之前，表示枚举类型的一种常见模式是声明一组 int 的常量，这种技术称为 int 枚举模式，它有许多缺点。它没有提供任何类型安全性，并且几乎不具备表现力。如果你传递一个苹果给方法，希望得到一个橘子，使用 == 操作符比较苹果和橘子时编译器并不会提示错误，或更糟的情况： 12// Tasty citrus flavored applesauce!int i = (APPLE_FUJI - ORANGE_TEMPLE) / APPLE_PIPPIN; 使用 String 常量代替 int 常量。这种称为 String 枚举模式的变体甚至更不可取。虽然它确实为常量提供了可打印的字符串，但是它可能会导致不知情的用户将字符串常量硬编码到客户端代码中，而不是使用字段名。使用枚举可以解决上述问题。 从表面上看，Java 枚举类型可能与其他语言（如 C、c++ 和 c#）的枚举类型类似，但不能只看表象。Java 的枚举类型是成熟的类，比其他语言中的枚举类型功能强大得多，在其他语言中的枚举本质上是 int 值。除了纠正 int 枚举的不足之外，枚举类型还允许添加任意方法和字段并实现任意接口。 编写一个富枚举类型很容易，如上述的 Planet。要将数据与枚举常量关联，可声明实例字段并编写一个构造函数，该构造函数接受数据并将其存储在字段中。 枚举本质上是不可变的，因此所有字段都应该是 final。字段可以是公共的，但是最好将它们设置为私有并提供公共访问器。 有一种更好的方法可以将不同的行为与每个枚举常量关联起来，这些方法称为特定常量方法实现： 12345678// Enum type with constant-specific method implementationspublic enum Operation &#123; PLUS &#123;public double apply(double x, double y)&#123;return x + y;&#125;&#125;, MINUS &#123;public double apply(double x, double y)&#123;return x - y;&#125;&#125;, TIMES &#123;public double apply(double x, double y)&#123;return x * y;&#125;&#125;, DIVIDE&#123;public double apply(double x, double y)&#123;return x / y;&#125;&#125;; public abstract double apply(double x, double y);&#125; 使用实例字段替代序数：所有枚举都有一个 ordinal 方法，该方法返回枚举类型中每个枚举常数的数值位置。 123456// Abuse of ordinal to derive an associated value - DON&#x27;T DO THISpublic enum Ensemble &#123; SOLO, DUET, TRIO, QUARTET, QUINTET,SEXTET, SEPTET, OCTET, NONET, DECTET; public int numberOfMusicians() &#123; return ordinal() + 1; &#125;&#125; 虽然这个枚举可以工作，但维护却是噩梦。如果常量被重新排序，numberOfMusicians 方法将被破坏。有一个简单的解决方案：不要从枚举的序数派生与枚举关联的值；而是将其存储在实例字段中： 123456789public enum Ensemble &#123; SOLO(1), DUET(2), TRIO(3), QUARTET(4), QUINTET(5),SEXTET(6), SEPTET(7), OCTET(8), DOUBLE_QUARTET(8),NONET(9), DECTET(10),TRIPLE_QUARTET(12); private final int numberOfMusicians; Ensemble(int size) &#123; this.numberOfMusicians = size; &#125; public int numberOfMusicians() &#123; return numberOfMusicians; &#125;&#125; 枚举规范对 ordinal 方法的评价是这样的：「大多数程序员都不会去使用这个方法。它是为基于枚举的通用数据结构（如 EnumSet 和 EnumMap）而设计的」。除非你使用这个数据结构编写代码，否则最好完全避免使用这个方法。 用 EnumSet 替代位字段：位字段模式如下： 123456789// Bit field enumeration constants - OBSOLETE!public class Text &#123; public static final int STYLE_BOLD = 1 &lt;&lt; 0; // 1 public static final int STYLE_ITALIC = 1 &lt;&lt; 1; // 2 public static final int STYLE_UNDERLINE = 1 &lt;&lt; 2; // 4 public static final int STYLE_STRIKETHROUGH = 1 &lt;&lt; 3; // 8 // Parameter is bitwise OR of zero or more STYLE_ constants public void applyStyles(int styles) &#123; ... &#125;&#125; 允许你使用位运算的 OR 操作将几个常量组合成一个 Set： 1text.applyStyles(STYLE_BOLD | STYLE_ITALIC); 位字段表示方式允许使用位运算高效地执行 Set 操作，如并集和交集。但是位字段具有 int 枚举常量所有缺点，甚至更多。当位字段被打印为数字时，它比简单的 int 枚举常量更难理解。没有一种简单的方法可以遍历由位字段表示的所有元素。 当之前的示例修改为使用枚举和 EnumSet 而不是位字段时。它更短，更清晰，更安全： 123456// EnumSet - a modern replacement for bit fieldspublic class Text &#123; public enum Style &#123; BOLD, ITALIC, UNDERLINE, STRIKETHROUGH &#125; // Any Set could be passed in, but EnumSet is clearly best public void applyStyles(Set&lt;Style&gt; styles) &#123; ... &#125;&#125; 下面是将 EnumSet 实例传递给 applyStyles 方法的客户端代码： 1text.applyStyles(EnumSet.of(Style.BOLD, Style.ITALIC)); EnumSet 类结合了位字段的简洁性和性能。EnumSet 的一个真正的缺点是，从 Java 9 开始，它不能创建不可变的 EnumSet。但是，可以用 Collections.unmodifiableSet 包装 EnumSet，实现不可变性，但简洁性和性能将受到影响。 使用 EnumMap 替换序数索引：如果想要使用 Enum 里面的美居元素来对一组对象进行分组的话，请不要使用序数索引： 12// Using ordinal() to index into an array - DON&#x27;T DO THIS!Set&lt;Plant&gt;[] plantsByLifeCycle =(Set&lt;Plant&gt;[]) new Set[Plant.LifeCycle.values().length]; 这样带来的问题是不便于维护。Java 提供了一种简单的方式实现该目的，EnumMap： 12// Using an EnumMap to associate data with an enumMap&lt;Plant.LifeCycle, Set&lt;Plant&gt;&gt; plantsByLifeCycle =new EnumMap&lt;&gt;(Plant.LifeCycle.class); 这个程序比原来的版本更短，更清晰，更安全，速度也差不多。没有不安全的转换；不需要手动标记输出，因为 Map 的键是能转换为可打印字符串的枚举；在计算数组索引时不可能出错。 使用接口模拟可扩展枚举：利用枚举类型可以实现任意接口这一事实，为 opcode 类型定义一个接口，并为接口的标准实现定义一个枚举： 123456789101112131415161718192021222324252627282930// Emulated extensible enum using an interfacepublic interface Operation &#123; double apply(double x, double y);&#125;public enum BasicOperation implements Operation &#123; PLUS(&quot;+&quot;) &#123; public double apply(double x, double y) &#123; return x + y; &#125; &#125;, MINUS(&quot;-&quot;) &#123; public double apply(double x, double y) &#123; return x - y; &#125; &#125;, TIMES(&quot;*&quot;) &#123; public double apply(double x, double y) &#123; return x * y; &#125; &#125;, DIVIDE(&quot;/&quot;) &#123; public double apply(double x, double y) &#123; return x / y; &#125; &#125;; private final String symbol; BasicOperation(String symbol) &#123; this.symbol = symbol; &#125; @Override public String toString() &#123; return symbol; &#125;&#125; 注解优于命名模式：使用命名模式来标明某些程序元素需要工具或框架特殊处理的方式是很常见的，例如，在版本 4 之前，JUnit 测试框架要求其用户通过以字符 test 开头的名称来指定测试方法。命名模式有几个问题： 首先，排版错误会导致没有提示的失败 无法确保只在相应的程序元素上使用它们 它们没有提供将参数值与程序元素关联的好方法 假设我们声明了一个 Test 注解，那么我们需要相应的工具来解析这些注解： 12345678910111213141516171819202122232425// Program to process marker annotationsimport java.lang.reflect.*;public class RunTests &#123; public static void main(String[] args) throws Exception &#123; int tests = 0; int passed = 0; Class&lt;?&gt; testClass = Class.forName(args[0]); for (Method m : testClass.getDeclaredMethods()) &#123; if (m.isAnnotationPresent(Test.class)) &#123; tests++; try &#123; m.invoke(null); passed++; &#125; catch (InvocationTargetException wrappedExc) &#123; Throwable exc = wrappedExc.getCause(); System.out.println(m + &quot; failed: &quot; + exc); &#125; catch (Exception exc) &#123; System.out.println(&quot;Invalid @Test: &quot; + m); &#125; &#125; &#125; System.out.printf(&quot;Passed: %d, Failed: %d%n&quot;,passed, tests - passed); &#125;&#125; 使用注解很简洁，同时也防止了使用命名模式所带来的一系列的问题。 坚持使用 @Override 注解：在每个方法声明上都使用 @Override 注解来覆盖超类型声明，那么编译器可以帮助你减少受到有害错误的影响，如错误将重写实现为重载。在具体类中，可以不对覆盖抽象方法声明的方法使用该注解。 使用标记接口定义类型：标记接口是一种不包含任何方法声明的接口，它只是指定一个类，该类实现了具有某些属性的接口。例如 Serializable 接口。与标记注解相比，标记接口有两个优点。首先，标记接口定义的类型由标记类的实例实现；标记注解不会。标记接口相对于标记注解的另一个优点是可以更精确地定位它们。相对于标记接口，标记注解的主要优势是它们可以是其他注解功能的一部分。 第七章 Lambda表达式和流 lambda 表达式优于匿名类：在历史上，带有单个抽象方法的接口被用作函数类型。它们的实例（称为函数对象）表示函数或操作。自从 JDK 1.1 在 1997 年发布以来，创建函数对象的主要方法就是匿名类： 123456// Anonymous class instance as a function object - obsolete!Collections.sort(words, new Comparator&lt;String&gt;() &#123; public int compare(String s1, String s2) &#123; return Integer.compare(s1.length(), s2.length()); &#125;&#125;); 在 Java 8 中官方化了一个概念，即具有单个抽象方法的接口是特殊的，应该得到特殊处理。这些接口现在被称为函数式接口，允许使用 lambda 表达式创建这些接口的实例： 12// Lambda expression as function object (replaces anonymous class)Collections.sort(words,(s1, s2) -&gt; Integer.compare(s1.length(), s2.length())); 一般来说，省略lambda中参数的类型，除非编译器不能自动推断出来。另外，在lambda表达式中this关键字指向的是外部的类的实例，但是匿名类指的是匿名类自己。 方法引用优于 lambda 表达式：lambda 表达式与匿名类相比，主要优势是更简洁。Java 提供了一种方法来生成比 lambda 表达式更简洁的函数对象：方法引用。 1map.merge(key, 1, Integer::sum); 而是用lambda代码如下： 1map.merge(key, 1, (count, incr) -&gt; count + incr); 方法引用通常为 lambda 表达式提供了一种更简洁的选择。如果方法引用更短、更清晰，则使用它们；如果没有，仍然使用 lambda 表达式。 优先使用标准函数式接口：现在 Java 已经有了 lambda 表达式，编写 API 的最佳实践已经发生了很大的变化。java.util.function 包提供了大量的标准函数接口供你使用。如果一个标准的函数式接口可以完成这项工作，那么你通常应该优先使用它，而不是使用专门构建的函数式接口。六个基本的函数式接口总结如下： Interface Function Signature Example UnaryOperator T apply(T t) String::toLowerCase BinaryOperator T apply(T t1, T t2) BigInteger::add Predicate boolean test(T t) Collection::isEmpty Function R apply(T t) Arrays::asList Supplier T get() Instant::now Consumer void accept(T t) System.out::println 还有 6 个基本接口的 3 个变体，用于操作基本类型 int、long 和 double。Function 接口还有 9 个额外的变体，在结果类型为基本数据类型时使用。 明智地使用流：在 Java 8 中添加了流 API，以简化序列或并行执行批量操作的任务。这个 API 提供了两个关键的抽象：流（表示有限或无限的数据元素序列）和流管道（表示对这些元素的多阶段计算）。流管道的计算是惰性的：直到调用 Terminal 操作时才开始计算，并且对完成 Terminal 操作不需要的数据元素永远不会计算。这种惰性的求值机制使得处理无限流成为可能。流 API 非常通用，实际上任何计算都可以使用流来执行，但这并不意味着你就应该这样做。如果使用得当，流可以使程序更短、更清晰；如果使用不当，它们会使程序难以读取和维护。由于 Java 不支持基本字符流： 1&quot;Hello world!&quot;.chars().forEach(System.out::print); 你可能希望它打印 Hello world!，但如果运行它，你会发现它打印 721011081081113211911111410810033。这是因为 “Hello world!”.chars() 返回的流元素不是 char 值，而是 int 值，因此调用了 print 的 int 重载。强制转换可以解决这个问题： 1&quot;Hello world!&quot;.chars().forEach(x -&gt; System.out.print((char) x)); 有些事情你可以对代码块做，而你不能对函数对象（通常是 lambda 表达式或方法引用）做 从代码块中，可以读取或修改作用域中的任何局部变量；在 lambda 表达式中，只能读取 final 或有效的 final 变量，不能修改任何局部变量。 从代码块中，可以从封闭方法返回、中断或继续封闭循环，或抛出声明要抛出的任何已检查异常；在 lambda 表达式中，你不能做这些事情。 相反，流使做一些事情变得非常容易： 元素序列的一致变换 过滤元素序列 使用单个操作组合元素序列 将元素序列累积到一个集合中，可能是按某个公共属性对它们进行分组 在元素序列中搜索满足某些条件的元素 在流中使用无副作用的函数：你可能会看到如下使用流的代码片段，它用于构建文本文件中单词的频率表： 1234567// Uses the streams API but not the paradigm--Don&#x27;t do this!Map&lt;String, Long&gt; freq = new HashMap&lt;&gt;();try (Stream&lt;String&gt; words = new Scanner(file).tokens()) &#123; words.forEach(word -&gt; &#123; freq.merge(word.toLowerCase(), 1L, Long::sum); &#125;);&#125; 简单地说，它根本不是流代码，而是伪装成流代码的迭代代码。它没有从流 API 中获得任何好处，而且它（稍微）比相应的迭代代码更长、更难于阅读和更难以维护。这个问题源于这样一个事实：这段代码在一个 Terminal 操作中（forEach）执行它的所有工作，使用一个会改变外部状态的 lambda 表达式（频率表）。改进后的代码： 12345// Proper use of streams to initialize a frequency tableMap&lt;String, Long&gt; freq;try (Stream&lt;String&gt; words = new Scanner(file).tokens()) &#123; freq = words.collect(groupingBy(String::toLowerCase, counting()));&#125; 这个代码片段与前面的代码片段做了相同的事情，但是正确地使用了流 API。它更短更清晰。 将流的元素收集到一个真正的 Collection 中的 collector 非常简单。这样的 collector 有三种：toList()、toSet() 和 toCollection(collectionFactory)。它们分别返回 List、Set 和程序员指定的集合类型。 12345// Pipeline to get a top-ten list of words from a frequency tableList&lt;String&gt; topTen = freq.keySet().stream() .sorted(comparing(freq::get).reversed()) .limit(10) .collect(toList()); 另外还有 groupingBy 和 join。 优先选择 Collection 而不是流作为返回类型：在编写返回元素序列的方法时，有些用户可能希望将它们作为流处理，而有些用户可能希望对它们进行迭代。如果可以返回集合，那么就这样做。如果你已经在一个集合中拥有了元素，或者序列中的元素数量足够小，可以创建一个新的元素，那么返回一个标准集合，例如 ArrayList 。否则，请考虑像对 power 集那样实现自定义集合。如果返回集合不可行，则返回流或 iterable，以看起来更自然的方式返回。 谨慎使用并行流：在主流语言中，Java 一直走在提供简化并发编程任务工具的前列。当 Java 在 1996 年发布时，它内置了对线程的支持，支持同步和 wait/notify。Java 5 引入了 java.util.concurrent。具有并发集合和执行器框架的并发库。Java 7 引入了 fork-join 包，这是一个用于并行分解的高性能框架。Java 8 引入了流，它可以通过对 parallel 方法的一次调用来并行化。 并行性带来的性能提升在 ArrayList、HashMap、HashSet 和 ConcurrentHashMap 实例上的流效果最好；int 数组和 long 数组也在其中。 这些数据结构的共同之处在于，它们都可以被精确且廉价地分割成任意大小的子程序，这使得在并行线程之间划分工作变得很容易。 并行化流不仅会导致糟糕的性能，包括活动失败；它会导致不正确的结果和不可预知的行为（安全故障）。 如果管道使用映射器、过滤器和其他程序员提供的函数对象，而这些对象没有遵守其规范，则并行化管道可能导致安全故障。流规范对这些功能对象提出了严格的要求。例如，传递给流的 reduce 操作的累加器和组合器函数必须是关联的、不干扰的和无状态的。 第八章 方法 检查参数的有效性：大多数方法和构造函数都对传递给它们的参数值有一些限制。例如，索引值必须是非负的，对象引用必须是非空的。如果一个无效的参数值被传递给一个方法，如果该方法在执行之前会检查它的参数，那么这个过程将迅速失败，并引发适当的异常。如果方法未能检查其参数，可能会发生以下几件事。该方法可能会在处理过程中出现令人困惑的异常而失败。更糟的是，该方法可以正常返回，但会静默计算错误的结果。在 Java 7 中添加的 Objects.requireNonNull 方法非常灵活和方便，因此不再需要手动执行空检查。在 Java 9 中，范围检查功能被添加到 java.util.Objects 中。这个功能由三个方法组成：checkFromIndexSize、checkFromToIndex 和 checkIndex。非公共方法可以使用断言检查它们的参数。 在执行方法的计算任务之前，应该显式地检查方法的参数，这条规则也有例外。一个重要的例外是有效性检查成本较高或不切实际，或者检查是在计算过程中隐式执行了。例如，考虑一个为对象 List 排序的方法，比如 Collections.sort(List)。List 中的所有对象必须相互比较。在对 List 排序的过程中，List 中的每个对象都会与列表中的其他对象进行比较。如果对象不能相互比较，将抛出 ClassCastException，这正是 sort 方法应该做的。因此，没有必要预先检查列表中的元素是否具有可比性。 在需要时制作防御性副本：即使使用一种安全的语言，如果你不付出一些努力，也无法与其他类隔离。你必须进行防御性的设计，并假定你的类的客户端会尽最大努力破坏它的不变量。 随着人们越来越多地尝试破坏系统的安全性，这个观点越来越正确。考虑这样的一个类： 123456789101112131415161718192021// Broken &quot;immutable&quot; time period classpublic final class Period &#123; private final Date start; private final Date end; public Period(Date start, Date end) &#123; if (start.compareTo(end) &gt; 0) throw new IllegalArgumentException(start + &quot; after &quot; + end); this.start = start; this.end = end; &#125; public Date start() &#123; return start; &#125; public Date end() &#123; return end; &#125; ... // Remainder omitted&#125; 乍一看，这个类似乎是不可变的，并且要求一个时间段的开始时间不能在结束时间之后。然而，利用 Date 是可变的这一事实很容易绕过这个约束： 12345// Attack the internals of a Period instanceDate start = new Date();Date end = new Date();Period p = new Period(start, end);end.setYear(78); // Modifies internals of p! 为了防止这样的攻击，可以选择制作防御性副本： 12345678910111213141516// Repaired constructor - makes defensive copies of parameterspublic Period(Date start, Date end) &#123; this.start = new Date(start.getTime()); this.end = new Date(end.getTime()); if (this.start.compareTo(this.end) &gt; 0) throw new IllegalArgumentException(this.start + &quot; after &quot; + this.end);&#125;// Repaired accessors - make defensive copies of internal fieldspublic Date start() &#123; return new Date(start.getTime());&#125;public Date end() &#123; return new Date(end.getTime());&#125; 防御性复制可能会带来性能损失，最好的方法是使用不可变类，比如Instant（或 Local-DateTime 或 ZonedDateTime）来代替 Date，Date 已过时，不应在新代码中使用。 仔细设计方法签名： 仔细选择方法名称。 不要提供过于便利的方法 避免长参数列表 对于参数类型，优先选择接口而不是类 双元素枚举类型优于 boolean 参数 明智地使用重载：考虑下面的代码： 123456789101112131415161718192021// Broken! - What does this program print?public class CollectionClassifier &#123; public static String classify(Set&lt;?&gt; s) &#123; return &quot;Set&quot;; &#125; public static String classify(List&lt;?&gt; lst) &#123; return &quot;List&quot;; &#125; public static String classify(Collection&lt;?&gt; c) &#123; return &quot;Unknown Collection&quot;; &#125; public static void main(String[] args) &#123; Collection&lt;?&gt;[] collections = &#123; new HashSet&lt;String&gt;(),new ArrayList&lt;BigInteger&gt;(),new HashMap&lt;String, String&gt;().values() &#125;; for (Collection&lt;?&gt; c : collections) System.out.println(classify(c)); &#125; 你可能期望这个程序打印 Set，然后是 List 和 Unknown Collection，但是它没有这样做。它打印 Unknown Collection 三次。为什么会这样？因为 classify 方法被重载，并且 在编译时就决定了要调用哪个重载。 这个程序的行为违反常规，因为重载方法的选择是静态的，而覆盖方法的选择是动态的。 在运行时根据调用方法的对象的运行时类型选择覆盖方法的正确版本。 123456789101112131415161718192021class Wine &#123; String name() &#123; return &quot;wine&quot;; &#125;&#125;class SparklingWine extends Wine &#123; @Override String name() &#123; return &quot;sparkling wine&quot;; &#125;&#125;class Champagne extends SparklingWine &#123; @Override String name() &#123; return &quot;champagne&quot;; &#125;&#125;public class Overriding &#123; public static void main(String[] args) &#123; List&lt;Wine&gt; wineList = List.of(new Wine(), new SparklingWine(), new Champagne()); for (Wine wine : wineList) System.out.println(wine.name()); &#125;&#125; 正如你所期望的，这个程序打印出 wine、sparkling 和 champagne，即使实例的编译时类型是循环每次迭代中的 wine。 应该避免混淆重载的用法。安全、保守的策略是永远不导出具有相同数量参数的两个重载。这些限制并不十分繁琐，因为你总是可以为方法提供不同的名称，而不是重载它们。 明智地使用可变参数：可变参数方法接受指定类型的零个或多个参数。可变参数方法首先创建一个数组，其大小是在调用点上传递的参数数量，然后将参数值放入数组，最后将数组传递给方法。在性能关键的情况下使用可变参数时要小心。每次调用可变参数方法都会导致数组分配和初始化。如果你已经从经验上确定你负担不起这个成本，但是你仍需要可变参数的灵活性，那么有一种模式可以让你鱼与熊掌兼得。假设你已经确定对方法 95% 的调用只需要三个或更少的参数。可以声明该方法的 5 个重载，每个重载 0 到 3 个普通参数，当参数数量超过 3 个时引入可变参数： 12345public void foo() &#123; &#125;public void foo(int a1) &#123; &#125;public void foo(int a1, int a2) &#123; &#125;public void foo(int a1, int a2, int a3) &#123; &#125;public void foo(int a1, int a2, int a3, int... rest) &#123; &#125; 返回空集合或数组，而不是 null：在几乎每次使用返回 null 来代替空集合或数组的方法时，都需要使用这种权宜之计。它很容易出错，因为编写客户端的程序员可能忘记编写特殊情况的代码来处理 null 返回。这样的错误可能会被忽略多年，因为这样的方法通常返回一个或多个对象。此外，在空容器中返回 null 会使返回容器的方法的实现复杂化。数组的情况与集合的情况相同。永远不要返回 null，而应该返回零长度的数组。永远不要用 null 来代替空数组或集合。它使你的 API 更难以使用，更容易出错，并且没有性能优势。 明智地返回 Optional：在 Java 8 之前，在编写在某些情况下无法返回值的方法时，可以采用两种方法。要么抛出异常，要么返回 null（假设返回类型是对象引用类型）。这两种方法都不完美。应该为异常条件保留异常，并且抛出异常代价高昂，因为在创建异常时捕获整个堆栈跟踪。返回 null 没有这些缺点，但是它有自己的缺点。如果方法返回 null，客户端必须包含特殊情况代码来处理 null 返回的可能性，除非程序员能够证明 null 返回是不可能的。如果客户端忽略检查 null 返回并将 null 返回值存储在某个数据结构中，那么 NullPointerException 可能会在将来的某个时间，在代码中的某个与该问题无关的位置产生。 在 Java 8 中，还有第三种方法来编写可能无法返回值的方法。Optional&lt;T&gt; 类表示一个不可变的容器，它可以包含一个非空的 T 引用，也可以什么都不包含。不包含任何内容的 Optional 被称为空。一个值被认为存在于一个非空的 Optional 中。Optional 的本质上是一个不可变的集合，它最多可以容纳一个元素。具备 Optional 返回值的方法比抛出异常的方法更灵活、更容易使用，并且比返回 null 的方法更不容易出错。 如果你发现自己编写的方法不能总是返回确定值，并且你认为该方法的用户在每次调用时应该考虑这种可能性，那么你可能应该让方法返回一个 Optional。但是，你应该意识到，返回 Optional 会带来实际的性能后果；对于性能关键的方法，最好返回 null 或抛出异常。最后，除了作为返回值之外，你几乎不应该以任何其他方式使用 Optional。 为所有公开的 API 元素编写文档注释：如果 API 要可用，就必须对其进行文档化。传统上，API 文档是手工生成的，保持与代码的同步是一件苦差事。Java 编程环境使用 Javadoc 实用程序简化了这一任务。Javadoc 使用特殊格式的文档注释（通常称为文档注释）从源代码自动生成 API 文档。 要正确地编写 API 文档，必须在每个公开的类、接口、构造函数、方法和字段声明之前加上文档注释。 如果一个类是可序列化的，还应该记录它的序列化形式。方法的文档注释应该简洁地描述方法与其客户端之间的约定。 第九章 通用程序设计 将局部变量的作用域最小化：通过最小化局部变量的范围，可以提高代码的可读性和可维护性，并降低出错的可能性。在做循环操作的时候，尽量使用for而不是while。下面是使用局部变量的方法技巧： 将局部变量的作用域最小化，最具说服力的方式就是在第一次使用它的地方声明。 每个局部变量声明都应该包含一个初始化表达式。 最小化局部变量范围的最后一种技术是保持方法小而集中。 for-each 循环优于传统的 for 循环：通常我们使用迭代器来遍历集合，使用数组索引遍历数组，但是他们的缺点就是需要自己维护迭代器和索引变量，最好的方法是使用for-each循环。但是，下列情况不适合for-each循环： 破坏性过滤，如果需要遍历一个集合并删除选定元素，则需要使用显式的迭代器，以便调用其 remove 方法。 转换，如果需要遍历一个 List 或数组并替换其中部分或全部元素的值，那么需要 List 迭代器或数组索引来替换元素的值。 并行迭代，如果需要并行遍历多个集合，那么需要显式地控制迭代器或索引变量，以便所有迭代器或索引变量都可以同步执行。 了解并使用库：假设你想要生成 0 到某个上界之间的随机整数： 12345// Common but deeply flawed!static Random rnd = new Random();static int random(int n) &#123; return Math.abs(rnd.nextInt()) % n;&#125; 有三个缺点：首先，如果 n 是小的平方数，随机数序列会在相当短的时间内重复。第二个缺陷是，如果 n 不是 2 的幂，那么平均而言，一些数字将比其他数字更频繁地返回。第三个缺陷是，在极少数情况下会返回超出指定范围的数字，这是灾难性的结果。 从 Java 7 开始，就不应该再使用 Random。在大多数情况下，选择的随机数生成器现在是 ThreadLocalRandom。使用这些库能生成更高的随机数，同时，你不必浪费时间为那些与你的工作无关的问题编写专门的解决方案。 若需要精确答案就应避免使用 float 和 double 类型：float 和 double 类型特别不适合进行货币计算，因为不可能将 0.1（或 10 的任意负次幂）精确地表示为 float 或 double。为了得到准确值，应该使用BigDecimal或者是long，int类型进行计算操作。 基本数据类型优于包装类：将 == 操作符应用于包装类型几乎都是错误的，而在使用基本类型的时候则是正确的。在操作中混合使用基本类型和包装类型时，包装类型就会自动拆箱，如果包装类是null，可能就会导致NullPointerException。 什么时候该使用包装类型？第一个是作为集合中的元素、键和值。在参数化类型和方法中，必须使用包装类型作为类型参数，因为 Java 不允许使用基本类型。最后，在进行反射方法调用时，必须使用包装类型。 总之，只要有选择，就应该优先使用基本类型，而不是包装类型。基本类型更简单、更快。当你的程序使用 == 操作符比较两个包装类型时，它会执行标识比较，这几乎肯定不是你想要的。 其他类型更合适时应避免使用字符串：字符串被设计用来表示文本，它们在这方面做得很好。下面是一些字符串不推荐使用的方式： 字符串是枚举类型的糟糕替代品 字符串是聚合类型的糟糕替代品 总之，当存在或可以编写更好的数据类型时，应避免将字符串用来表示对象。如果使用不当，字符串比其他类型更麻烦、灵活性更差、速度更慢、更容易出错。字符串经常被误用的类型包括基本类型、枚举和聚合类型。 当心字符串连接引起的性能问题：使用字符串串联运算符重复串联 n 个字符串需要 n 的平方级时间。这是字符串不可变这一事实导致的结果。当连接两个字符串时，将复制这两个字符串的内容。要获得能接受的性能，请使用 StringBuilder 代替 String。 通过接口引用对象：应该优先使用接口而不是类来引用对象。如果存在合适的接口类型，那么应该使用接口类型声明参数、返回值、变量和字段。惟一真正需要引用对象的类的时候是使用构造函数创建它的时候。如果你养成了使用接口作为类型的习惯，那么你的程序将更加灵活。如果没有合适的接口存在，那么用类引用对象是完全合适的。 接口优于反射：核心反射机制 java.lang.reflect 提供对任意类的编程访问。给定一个 Class 对象，你可以获得 Constructor、Method 和 Field 实例，分别代表了该 Class 实例所表示的类的构造器、方法和字段。使用反射是有代价的： 你失去了编译时类型检查的所有好处，包括异常检查 执行反射访问所需的代码既笨拙又冗长 性能降低，反射方法调用比普通方法调用慢得多 反射是一种功能强大的工具，对于某些复杂的系统编程任务是必需的，但是它有很多缺点。如果编写的程序必须在编译时处理未知的类，则应该尽可能只使用反射实例化对象，并使用在编译时已知的接口或超类访问对象。 明智地使用本地方法：Java 本地接口（JNI）允许 Java 程序调用本地方法，这些方法是用 C 或 C++ 等本地编程语言编写的。为了提高性能，很少建议使用本地方法。使用本地方法有严重的缺点。由于本地语言不安全，使用本地方法的应用程序不再能免受内存毁坏错误的影响同时很难进行调试。 明智地进行优化：不要过早地进行优化。为了获得良好的性能而改变 API 是一个非常糟糕的想法。同时，在每次尝试优化之前和之后测量性能。 遵守被广泛认可的命名约定： Identifier Type Example Package or module org.junit.jupiter.api, com.google.common.collect Class or Interface Stream, FutureTask, LinkedHashMap, HttpClient Method or Field remove, groupingBy, getCrc Constant Field MIN_VALUE, NEGATIVE_INFINITY Local Variable i, denom, houseNum Type Parameter T, E, K, V, X, R, U, V, T1, T2 第十章 异常 仅在确有异常条件下使用异常：异常只适用于确有异常的情况；它们不应该用于一般的控制流程。下列代码不应该使用： 12345678// Horrible abuse of exceptions. Don&#x27;t ever do this!try &#123; int i = 0; while(true) range[i++].climb(); &#125; catch (ArrayIndexOutOfBoundsException e) &#123;&#125; 我们完全可以使用for-each循环实现。 对可恢复情况使用 checked 异常，对编程错误使用运行时异常：Java 提供了三种可抛出项：checked 异常、运行时异常和错误。决定是使用 checked 异常还是 unchecked 异常的基本规则是：使用 checked 异常的情况是为了合理地期望调用者能够从中恢复。有两种 unchecked 的可抛出项：运行时异常和错误。它们在行为上是一样的：都是可抛出的，通常不需要也不应该被捕获。使用运行时异常来指示编程错误。 绝大多数运行时异常都表示操作违反了先决条件。 避免不必要地使用 checked 异常：在 API 中过度使用 checked 异常会变得不那么令人愉快。如果一个方法抛出 checked 异常，调用它的代码必须在一个或多个 catch 块中处理它们；或者通过声明抛出，让它们向外传播。无论哪种方式，它都给 API 的用户带来了负担。消除 checked 异常的最简单方法是返回所需结果类型的 Optional 对象（Item-55）。该方法只返回一个空的 Optional 对象，而不是抛出一个 checked 异常。 总之，如果谨慎使用，checked 异常可以提高程序的可靠性；当过度使用时，它们会使 API 难以使用。如果调用者不应从失败中恢复，则抛出 unchecked 异常。如果恢复是可能的，并且你希望强制调用者处理异常条件，那么首先考虑返回一个 Optional 对象。只有当在失败的情况下，提供的信息不充分时，你才应该抛出一个 checked 异常。 鼓励复用标准异常：使你的 API 更容易学习和使用，因为它符合程序员已经熟悉的既定约定。最常见的可复用异常： Exception Occasion for Use IllegalArgumentException Non-null parameter value is inappropriate（非空参数值不合适） IllegalStateException Object state is inappropriate for method invocation（对象状态不适用于方法调用） NullPointerException Parameter value is null where prohibited（禁止参数为空时仍传入 null） IndexOutOfBoundsException Index parameter value is out of range（索引参数值超出范围） ConcurrentModificationException Concurrent modification of an object has been detected where it is prohibited（在禁止并发修改对象的地方检测到该动作） UnsupportedOperationException Object does not support method（对象不支持该方法调用） 另外，不要直接复用 Exception、RuntimeException、Throwable 或 Error。应当将这些类视为抽象类。你不能对这些异常进行可靠的测试，因为它们是方法可能抛出的异常的超类。 抛出能用抽象解释的异常：当一个方法抛出一个与它所执行的任务没有明显关联的异常时，这是令人不安的。这种情况经常发生在由方法传播自低层抽象抛出的异常。为了避免这个问题，高层应该捕获低层异常，并确保抛出的异常可以用高层抽象解释。 这个习惯用法称为异常转换： 123456// Exception Translationtry &#123; ... // Use lower-level abstraction to do our bidding&#125; catch (LowerLevelException e) &#123; throw new HigherLevelException(...);&#125; 虽然异常转换优于底层异常的盲目传播，但它不应该被过度使用。在可能的情况下，处理低层异常的最佳方法是确保低层方法避免异常。 为每个方法记录会抛出的所有异常：始终单独声明 checked 异常，并使用 Javadoc 的 @throw 标记精确记录每次抛出异常的条件。使用 Javadoc 的 @throw 标记记录方法会抛出的每个异常，但是不要对 unchecked 异常使用 throws 关键字。如果一个类中的许多方法都因为相同的原因抛出异常，你可以在类的文档注释中记录异常， 而不是为每个方法单独记录异常。 异常详细消息中应包含捕获失败的信息：当程序由于未捕获异常而失败时，系统可以自动打印出异常的堆栈跟踪。堆栈跟踪包含异常的字符串表示，这是调用其 toString 方法的结果。这通常包括异常的类名及其详细信息。要捕获失败，异常的详细消息应该包含导致异常的所有参数和字段的值。因为许多人在诊断和修复软件问题的过程中可能会看到堆栈跟踪，所以不应包含密码、加密密钥等详细信息。 尽力保证故障原子性：在对象抛出异常之后，通常希望对象仍然处于定义良好的可用状态，即使在执行操作时发生了故障。对于 checked 异常尤其如此，调用者希望从异常中恢复。一般来说，失败的方法调用应该使对象处于调用之前的状态。 具有此属性的方法称为具备故障原子性。有几种方式可以达到这种效果： 最简单的方法是设计不可变对象 对计算进行排序，以便可能发生故障的部分都先于修改对象的部分发生 以对象的临时副本执行操作，并在操作完成后用临时副本替换对象的内容 编写恢复代码，拦截在操作过程中发生的故障，并使对象回滚到操作开始之前的状态 不要忽略异常：如果在方法调用的周围加上一条 try 语句，其 catch 块为空，可以很容易忽略异常，空 catch 块违背了异常的目的，它的存在是为了强制你处理异常情况。如果你选择忽略异常，catch 块应该包含一条注释，解释为什么这样做是合适的，并且应该将变量命名为 ignored。 第十一章 并发 对共享可变数据的同步访问：synchronized 关键字确保一次只有一个线程可以执行一个方法或块。没有同步，一个线程所做的的更改可能对其他线程不可见。同步不仅阻止线程察觉到处于不一致状态的对象，而且确保每个进入同步方法或块的线程都能察觉由同一把锁保护的所有已修改的效果。 12345678910111213141516// Broken! - How long would you expect this program to run?public class StopThread &#123; private static boolean stopRequested; public static void main(String[] args) throws InterruptedException &#123; Thread backgroundThread = new Thread(() -&gt; &#123; int i = 0; while (!stopRequested) i++; &#125;); backgroundThread.start(); TimeUnit.SECONDS.sleep(1); stopRequested = true; &#125;&#125; 在缺乏同步的情况下，无法保证后台线程何时（如果有的话）看到主线程所做的 stopRequested 值的更改。在缺乏同步的情况下，虚拟机可以很好地转换这段代码： 123456while (!stopRequested) i++;into this code:if (!stopRequested) while (true) i++; 这种优化称为提升，这正是 OpenJDK 服务器 VM 所做的。结果是活性失败：程序无法取得进展。 注意，写方法（requestStop）和读方法（stopRequested）都是同步的。仅同步写方法是不够的！除非读和写操作都同步，否则不能保证同步工作。虽然 volatile 修饰符不执行互斥，但它保证任何读取字段的线程都会看到最近写入的值。 123456// Broken - requires synchronization!private static volatile int nextSerialNumber = 0;public static int generateSerialNumber() &#123; return nextSerialNumber++;&#125; 问题在于增量运算符 (++) 不是原子性的。它对 nextSerialNumber 字段执行两个操作：首先读取值，然后返回一个新值，旧值再加 1。如果第二个线程在读取旧值和写入新值之间读取字段，则第二个线程将看到与第一个线程相同的值，并返回相同的序列号。可以将 synchronized 添加到方法声明中，另外，也使用 AtomicLong 类，它是 java.util.concurrent.atomic 的一部分。 总之，当多个线程共享可变数据时，每个读取或写入数据的线程都必须执行同步。 在缺乏同步的情况下，不能保证一个线程的更改对另一个线程可见。 避免过度同步：过度的同步可能导致性能下降、死锁甚至不确定行为。作为规则，你应该在同步区域内做尽可能少的工作。获取锁，检查共享数据，根据需要进行转换，然后删除锁。如果你必须执行一些耗时的活动，请设法将其移出同步区域。 Executor、task、流优于直接使用线程：java.util.concurrent 已经添加到 Java 中。这个包有一个 Executor 框架，它是一个灵活的基于接口的任务执行工具。对于小程序或负载较轻的服务器，Executors.newCachedThreadPool 通常是一个不错的选择，因为它不需要配置，而且通常「做正确的事情」。但是对于负载沉重的生产服务器来说，缓存的线程池不是一个好的选择！在缓存的线程池中，提交的任务不会排队，而是立即传递给线程执行。如果没有可用的线程，则创建一个新的线程。如果服务器负载过重，所有 CPU 都被充分利用，并且有更多的任务到达，就会创建更多的线程，这只会使情况变得更糟。因此，在负载沉重的生产服务器中，最好使用 Executors.newFixedThreadPool，它为你提供一个线程数量固定的池，或者直接使用 ThreadPoolExecutor 类来实现最大限度的控制。 并发实用工具优于 wait 和 notify：考虑到正确使用 wait 和 notify 的困难，你应该使用更高级别的并发实用工具。java.util.concurrent 中级别较高的实用工具可分为三类：Executor 框架，Item-80 简要介绍了该框架；并发集合；同步器。本条目简要介绍并发集合和同步器。 并发集合是标准集合接口，如 List、Queue 和 Map 的高性能并发实现。为了提供高并发性，这些实现在内部管理它们自己的同步。一些集合接口使用阻塞操作进行了扩展，这些操作将等待（或阻塞）成功执行。例如，BlockingQueue 扩展了 Queue 并添加了几个方法，包括 take，它从队列中删除并返回首个元素，如果队列为空，则等待。 同步器是允许线程彼此等待的对象，允许它们协调各自的活动。最常用的同步器是 CountDownLatch 和 Semaphore。较不常用的是 CyclicBarrier 和 Exchanger。最强大的同步器是 Phaser。 始终使用 wait 习惯用法，即循环来调用 wait 方法；永远不要在循环之外调用它。 循环用于在等待之前和之后测试条件。 文档应包含线程安全属性：类的线程安全的描述通常属于该类的文档注释，但是具有特殊线程安全属性的方法应该在它们自己的文档注释中描述这些属性。没有必要记录枚举类型的不变性。 明智地使用延迟初始化：延迟初始化是延迟字段的初始化，直到需要它的值。与大多数优化一样，延迟初始化的最佳建议是「除非需要，否则不要这样做」。在大多数情况下，常规初始化优于延迟初始化。 不要依赖线程调度器：任何依赖线程调度器来保证正确性或性能的程序都可能是不可移植的。如果线程没有做有用的工作，它们就不应该运行。线程优先级可以少量地用于提高已经工作的程序的服务质量，但绝不应该用于「修复」几乎不能工作的程序。 第十二章 序列化 Java 序列化的替代方案：序列化的一个根本问题是它的可攻击范围太大，且难以保护，而且问题还在不断增多：通过调用 ObjectInputStream 上的 readObject 方法反序列化对象图。这个方法本质上是一个神奇的构造函数，可以用来实例化类路径上几乎任何类型的对象，只要该类型实现 Serializable 接口。避免序列化利用的最好方法是永远不要反序列化任何东西。永远不要反序列化不可信的数据。 序列化是危险的，应该避免。如果你从头开始设计一个系统，可以使用跨平台的结构化数据，如 JSON 或 protobuf。不要反序列化不可信的数据。如果必须这样做，请使用对象反序列化过滤，但要注意，它不能保证阻止所有攻击。避免编写可序列化的类。 非常谨慎地实现 Serializable：使类的实例可序列化非常简单，只需实现 Serializable 接口即可。因为这很容易做到，所以有一个普遍的误解，认为序列化只需要程序员付出很少的努力。而事实上要复杂得多。虽然使类可序列化的即时代价可以忽略不计，但长期代价通常是巨大的： 一旦类的实现被发布，它就会降低更改该类实现的灵活性 增加了出现 bug 和安全漏洞的可能性 增加了与发布类的新版本相关的测试负担 为继承而设计的类（Item-19）很少情况适合实现 Serializable 接口，接口也很少情况适合扩展它。另外，内部类不应该实现 Serializable。 考虑使用自定义序列化形式：当对象的物理表示与其逻辑数据内容有很大差异时，使用默认的序列化形式有四个缺点： 它将导出的 API 永久地绑定到当前的内部实现 它会占用过多的空间 它会消耗过多的时间 它可能导致堆栈溢出 无论你是否使用默认的序列化形式，必须对对象序列化强制执行任何同步操作，就像对读取对象的整个状态的任何其他方法强制执行的那样。无论选择哪种序列化形式，都要在编写的每个可序列化类中声明显式的序列版本 UID。 这消除了序列版本 UID 成为不兼容性的潜在来源。这么做还能获得一个小的性能优势。如果没有提供序列版本 UID，则需要执行高开销的计算在运行时生成一个 UID。不要更改序列版本 UID，除非你想破坏与现有序列化所有实例的兼容性。 防御性地编写 readObject 方法：当对象被反序列化时，对任何客户端不能拥有的对象引用的字段进行防御性地复制至关重要。 因此，对于每个可序列化的不可变类，如果它包含了私有的可变组件，那么在它的 readObjec 方法中，必须要对这些组件进行防御性地复制： 12345678910// readObject method with defensive copying and validity checkingprivate void readObject(ObjectInputStream s) throws IOException, ClassNotFoundException &#123; s.defaultReadObject(); // Defensively copy our mutable components start = new Date(start.getTime()); end = new Date(end.getTime()); // Check that our invariants are satisfied if (start.compareTo(end) &gt; 0) throw new InvalidObjectException(start +&quot; after &quot;+ end);&#125; 下面是编写 readObject 方法的指导原则： 对象引用字段必须保持私有的的类，应防御性地复制该字段中的每个对象 检查任何不变量，如果检查失败，则抛出 InvalidObjectException 如果必须在反序列化后验证整个对象图，那么使用 ObjectInputValidation 接口 不要直接或间接地调用类中任何可被覆盖的方法 对于实例控制，枚举类型优于 readResolve：在可能的情况下，使用枚举类型强制实例控制不变量。如果这是不可能的，并且你需要一个既可序列化又实例控制的类，那么你必须提供一个 readResolve 方法，并确保该类的所有实例字段都是基本类型，或使用 transient 修饰。 考虑以序列化代理代替序列化实例：实现 Serializable 接口的决定增加了出现 bug 和安全问题的可能性，因为它允许使用一种超语言机制来创建实例，而不是使用普通的构造函数。然而，有一种技术可以大大降低这些风险。这种技术称为序列化代理模式。序列化代理模式相当简单。首先，设计一个私有静态嵌套类，它简洁地表示外围类实例的逻辑状态。这个嵌套类称为外围类的序列化代理。它应该有一个构造函数，其参数类型是外围类。这个构造函数只是从它的参数复制数据：它不需要做任何一致性检查或防御性复制。 序列化代理模式有两个限制。它与客户端可扩展的类不兼容；序列化代理模式所增强的功能和安全性并不是没有代价的。 当你发现必须在客户端不可扩展的类上编写 readObject 或 writeObject 方法时，请考虑序列化代理模式。要想稳健地将带有重要约束条件的对象序列化时，这种模式可能是最容易的方法。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.zsstrike.top/tags/Java/"}]},{"title":"一致性算法","slug":"一致性算法","date":"2020-11-25T08:02:10.000Z","updated":"2020-12-17T10:23:43.559Z","comments":true,"path":"2020/11/25/一致性算法/","link":"","permalink":"http://blog.zsstrike.top/2020/11/25/%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/","excerpt":"本文主要介绍分布式系统中的一致性算法，包括 Panxos，Raft 和 ZAB 算法。","text":"本文主要介绍分布式系统中的一致性算法，包括 Panxos，Raft 和 ZAB 算法。 一致性概念CAP 理论：对于一个分布式系统，不能同时满足以下三点： 一致性（Consistency）：在分布式系统中的所有数据备份，在同一时刻是否同样的值。 可用性（Availability）：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。 分区容错性（Partition Tolerance）：一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。容忍性就提高了。 image-20201125163500285 一致性模型： 弱一致性：如果能容忍后续的部分或者全部访问不到，则是弱一致性。 最终一致性：如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。如 DNS，Gossip（Cassandra 通信协议）。 强一致性：对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。如 Raft，ZAB，Paxos。 问题：数据不能存在单点上，分布式系统对 fault tolerence 的一般解决方案是 state machine replication。其实我们今天讨论的准确的说，应该是 state machine replication的共识( consensus)算法。paxos其实是一个共识算法。系统的最终一致性，不仅需要达成共识，还会取决于 clientl的行为。 强一致性算法： 主从同步复制： Master 接受写请求 Master 复制日志到 slave Master 等待，直到所有 slave 返回成功信息 问题：一个节点失败，Master阻塞，导致整个集群不可用，保证了一致性，可用性却大大降低。 多数派：每次写都保证写入大于N/2个节点，每次读保证从大于N/2个节点中读。 问题：并发环境下，无法保证系统正确性，顺序很重要 image-20201125165428310 Paxos：分为 Basic Paxos，Multi Paxos 和 Fast Paxos。 Basic Paxos： 角色分配： Client：请求发起者。像是民众 Proposer：接受 Client 请求，向集群提出提议，像是议员 Acceptor（Voter）：提议投票和接受者，只有形成法定人数（Quorum，一般为多数派）时，提议才会最终被接受。像是国会。 Learner：提议接受者，backup。像是记录员 阶段： Phase 1a：Prepare：proposer 提出一个提案，编号为 N，这个 N 大于之前提出提案的编号。 Phase 1b：Promise：如果 N 大于此 acceptor 之前接受的提案编号，则接受，否则拒绝。 Phase 2a：Accept：如果达到多数派，此时 proposer 发出accept 请求，请求包含编号N，以及提案内容。 Phase 2b：Accepted：如果 N 大于此 acceptor 之前接受的提案编号，则接受，否则拒绝。 image-20201125182135831 问题：部分节点失败，但是达到了 Quoroms image-20201125182308674 问题：Proposer 失败 image-20201125182517810 问题：活锁，指的是两个 Proposer 互相间隔发出提案，要求进行投票 image-20201125182646023 上图中的提案编号应该依次是 1,2,3,4… 使用 Random Timeout 来解决上述问题。 其他问题：难以实现，效率低（2 轮 RPC） Multi Paxos： 新概念： Leader：唯一的 Proposer，所有请求都需要经过此 Leader 流程：首先执行 Leader 竞选（Basic Paxos 阶段1），选择之后直接执行 Basic Paxos 阶段2即可 image-20201125183931856 简化：减少角色，Server 之一同时充当 Proposer 和 Acceptor image-20201125184130384 第一阶段也是进行 Leader 选举，然后再 Propose 提案。 强一致性算法Raft 算法： 三个问题： Leader Election：通过 Election Timeout 来转变为 Candidate，进行选举，选举成功后，Leader 会发送 heartbeat timeout，来表示自己存在与网络当中 image-20201125185427865 问题：Leader 宕机了，剩余节点继续执行 Leader Election 问题：如果两个节点同时成为 Candidate，则通过 Random Timeout 来恢复 Log Replication：首先将 log entry 发送给 follower，之后如果获取到大多数投票后，就进行数据持久化，同时发送消息给客户端，最后发送信号给 follower，进行持久化 image-20201125190201928 Safety：发生故障时或者网络发生分区后，如何进行数据恢复。 image-20201125190443066 在下面的网络分区由于没有达到多数派，数据不会被持久化，但是上面的分区却可以进行数据持久化，这也是为什么一般集群中节点的数量是奇数的原因。当网络被修复之后，由于上面一部分的 Term 大于下面一部分，下面一部分就会更改 Leader，同事新的 Leader 会发送数据包进行数据持久化操作。 image-20201125190848884 重新定义角色： Leader：整个集群只有一个 Leader Follower：只会接受来自 Leader 的请求 Candidate：准备竞选 Leader 的节点 原理动画解释：http://thesecretlivesofdata.com/raft/ 场景测试：https://raft.github.io ZAB：基本上和 Raft 相同，在一些名词的法上有些区别：如 ZAB 将某一个 leader 的周期称为 epoch,而 raft 则称之为term。实现上也有些许不同：如 raft 保证日志连续性，心跳方向为 leader 至 follower，而 ZAB 则相反。","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://blog.zsstrike.top/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"深入理解JAVA虚拟机笔记","slug":"深入理解JAVA虚拟机笔记","date":"2020-11-12T10:59:12.000Z","updated":"2020-12-17T10:23:43.629Z","comments":true,"path":"2020/11/12/深入理解JAVA虚拟机笔记/","link":"","permalink":"http://blog.zsstrike.top/2020/11/12/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JAVA%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%AC%94%E8%AE%B0/","excerpt":"本文主要整理由周志明编写的《深入理解Java虚拟机》第三版书籍的整理笔记。","text":"本文主要整理由周志明编写的《深入理解Java虚拟机》第三版书籍的整理笔记。 第二章 Java内存区域与内存溢出异常运行时数据区域： image-20201112190800951 程序计数器：通过改变其值来获取下一条需要执行的字节码指令。 虚拟机栈：每个方法执行的时候会创建一个栈帧，用于存储局部变量，方法出口等信息。 本地方法栈：同虚拟机栈，只不过本地方法栈是为本地方法服务的。 堆：几乎所有的对象实例都会在这里面分配。 方法区：用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。 运行时常量池：是方法区的一部分，常量池表，Class文件中描述信息会放在此处。 直接内存：在JDK 1.4中新加入了NIO（New Input/Output）类，引入了一种基于通道（Channel）与缓冲区（Buffer）的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆里面的DirectByteBuffer对象作为这块内存的引用进行操作。 对象的创建：当Java虚拟机遇到一条字节码new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。在类加载检查通过后，接下来虚拟机将为新生对象分配内存。分配方式有指针碰撞和空闲列表两种方式。接下来，就需要执行构造函数了，也就是Class文件中的&lt;init&gt;()方法。 对象的内存布局：对象在堆里面的内存布局分为三部分：对象头，实例数据，对齐填充 对象头：第一类用于存储对象自身的运行时数据，如哈希码，GC分代年龄等，第二部分是类型指针，用于确定该对象是那个类的实例。 实例数据：从父类继承和该类中定义的数据。 对齐填充：用于保证对象是8字节对齐的。 对象的访问定位：主流的方式有两种，使用句柄或者使用直接指针，HotSpot虚拟机使用直接指针方式。 句柄：好处就是reference中存储的是稳定句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而referrence不用修改 image-20201112195734391 直接指针：好处就是速度更快，它节省了一次指针定位的时间开销 image-20201112195807566 第三章 垃圾收集器与内存分配策略引用计数算法：在对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加一；当引用失效时，计数器值就减一；任何时刻计数器为零的对象就是不可能再被使用的。该方法不能检测循环引用。 可达性分析算法：基本思路就是通过一系列称为“GC Roots”的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链”（Reference Chain），如果某个对象到GC Roots间没有任何引用链相连，或者用图论的话来说就是从GC Roots到这个对象不可达时，则证明此对象是不可能再被使用的。在Java技术体系中，GC Roots对象有： 虚拟机栈中引用的对象 在方法区中类静态属性引用的对象，常量引用的对象 同步锁持有的对象 引用类型：在JDK 1.2版之后，Java对引用的概念进行了扩充，有以下几类 强引用：最传统的“引用”的定义，是指在程序代码之中普遍存在的引用赋值 软引用：来描述一些还有用，但非必须的对象。只被软引用关联着的对象，在系统将要发生内存溢出异常前，会把这些对象列进回收范围之中进行第二次回收 弱引用：也是用来描述那些非必须对象，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生为止 虚引用：最弱的一种引用关系，一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例，为一个对象设置虚引用关联的唯一目的只是为了能在这个对象被收集器回收时收到一个系统通知 对象自我拯救：即使在可达性分析算法中判定为不可达的对象，这时候它们暂时还处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记，随后进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法。假如对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，那么虚拟机将这两种情况都视为“没有必要执行”。如果这个对象被判定为确有必要执行finalize()方法，那么该对象将会被放置在一个名为F-Queue的队列之中，并在稍后由一条由虚拟机自动建立的、低调度优先级的Finalizer线程去执行它们的finalize()方法。finalize()方法是对象逃脱死亡命运的最后一次机会，稍后收集器将对F-Queue中的对象进行第二次小规模的标记，如果对象要在finalize()中成功拯救自己，即只要重新与引用链上的任何一个对象建立关联即可，譬如把自己（this关键字）赋值给某个类变量或者对象的成员变量，那在第二次标记时它将被移出“即将回收”的集合。 回收方法区：在Java堆中，尤其是在新生代中，对常规应用进行一次垃圾收集通常可以回收70%至99%的内存空间，相比之下，方法区回收囿于苛刻的判定条件，其区域垃圾收集的回收成果往往远低于此。方法区的垃圾收集主要回收两部分内容：废弃的常量和不再使用的类型。 分代收集理论：建立在三个假说之上： 弱分代假说：绝大多数对象都是朝生夕灭的。 强分代假说：熬过越多次垃圾收集过程的对象就越难以消亡。 跨代引用假说：跨代引用相对于同代引用来说仅占极少数。 前两个假说表明如果一个区域中大多数对象都是朝生夕灭，难以熬过垃圾收集过程的话，那么把它们集中放在一起，每次回收时只关注如何保留少量存活而不是去标记那些大量将要被回收的对象，就能以较低代价回收到大量的空间；如果剩下的都是难以消亡的对象，那把它们集中放在一块，虚拟机便可以使用较低的频率来回收这个区域，这就同时兼顾了垃圾收集的时间开销和内存的空间有效利用；第三点表明我们就不应再为了少量的跨代引用去扫描整个老年代，也不必浪费空间专门记录每一个对象是否存在及存在哪些跨代引用。 标记-清除算法：首先标记出所有需要回收的对象，在标记完成后，统一回收掉所有被标记的对象。缺点：第一个是执行效率不稳定，如果Java堆中包含大量对象，而且其中大部分是需要被回收的，这时必须进行大量标记和清除的动作，导致标记和清除两个过程的执行效率都随对象数量增长而降低；第二个是内存空间的碎片化问题，标记、清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 image-20201127191857885 标记-复制算法：将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。如果内存中多数对象都是存活的，这种算法将会产生大量的内存间复制的开销，但对于多数对象都是可回收的情况，算法需要复制的就是占少数的存活对象，而且每次都是针对整个半区进行内存回收，分配内存时也就不用考虑有空间碎片的复杂情况，只要移动堆顶指针，按顺序分配即可。这样实现简单，运行高效，不过其缺陷也显而易见，这种复制回收算法的代价是将可用内存缩小为了原来的一半，空间浪费未免太多了一点。 image-20201127191840578 新生代存在朝生夕灭现象，存活者大概只有 10% 左右，内存空间比可以分为 8 ：1 标记-整理算法：其中的标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存。标记-清除算法与标记-整理算法的本质差异在于前者是一种非移动式的回收算法，而后者是移动式的。是否移动回收后的存活对象是一项优缺点并存的风险决策：在老年代这种每次回收都有大量对象存活区域，移动存活对象并更新所有引用这些对象的地方将会是一种极为负重的操作，而且这种对象移动操作必须全程暂停用户应用程序才能进行；而不移动对象的时候又存在空间碎片化问题。 image-20201127192619303 经典垃圾收集器： Serial 收集器：是一个单线程工作的收集器，但它的“单线程”的意义并不仅仅是说明它只会使用一个处理器或一条收集线程去完成垃圾收集工作，更重要的是强调在它进行垃圾收集时，必须暂停其他所有工作线程，直到它收集结束。优点是简单，内存消耗低；缺点是需要暂停其他工作线程。 image-20201127193322965 ParNew 收集器：实质上是Serial收集器的多线程并行版本。 image-20201127193714941 Parallel Scavenge收集器：也是一款新生代收集器，它同样是基于标记-复制算法实现的收集器，也是能够并行收集的多线程收集器，它的特点在于它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量。 Serial Old收集器：Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法。 image-20201127194020758 Parallel Old收集器：是Parallel Scavenge收集器的老年代版本，支持多线程并发收集，基于标记-整理算法实现。吞吐量优先收集器。 image-20201127194122503 CMS（Concurrent Mark Sweep）收集器：是一种以获取最短回收停顿时间为目标的收集器。很大一部分Java应用基于 B/S 实现，这类应用通常都会较为关注服务的响应速度，希望系统停顿时间尽可能短，以给用户带来良好的交互体验。CMS收集器就非常符合这类应用的需求。收集过程如下： 初始标记：记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快；Stop the World 并发标记：是从GC Roots的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行； 重新标记：则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录；Stop the World 并发清除：清理删除掉标记阶段判断的已经死亡的对象，由于不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发的。 image-20201127194644675 优点：并发收集，低停顿；缺点：对处理器资源敏感（并发阶段会导致应用程序变慢，降低总吞吐量），无法处理“浮动垃圾”（并发清理阶段，用户线程是还在继续运行的，程序在运行自然就还会伴随有新的垃圾对象不断产生），基于标记清除，空间碎片化问题严重。 Garbage First收集器：简称 G1 收集器，在G1收集器出现之前的所有其他收集器，包括CMS在内，垃圾收集的目标范围要么是整个新生代（Minor GC），要么就是整个老年代（Major GC），再要么就是整个Java堆（Full GC）。而G1跳出了这个樊笼，它可以面向堆内存任何部分来组成回收集（Collection Set，一般简称CSet）进行回收，衡量标准不再是它属于哪个分代，而是哪块内存中存放的垃圾数量最多，回收收益最大，这就是G1收集器的Mixed GC模式。G1不再坚持固定大小以及固定数量的分代区域划分，而是把连续的Java堆划分为多个大小相等的独立区域（Region），每一个Region都可以根据需要，扮演新生代的Eden空间、Survivor空间，或者老年代空间。收集器能够对扮演不同角色的Region采用不同的策略去处理，从而获取更好的收集效果。G1收集器过程： 初始标记：仅仅只是标记一下GC Roots能直接关联到的对象，需要短暂停顿 并发标记：从GC Root开始对堆中对象进行可达性分析，递归扫描整个堆里的对象图，找出要回收的对象，这阶段耗时较长，但可与用户程序并发执行。 最终标记：对用户线程做另一个短暂的暂停，用于处理并发阶段结束后仍遗留下来的最后那少量的SATB记录。 筛选回收：负责更新Region的统计数据，对各个Region的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划，可以自由选择任意多个Region构成回收集，然后把决定回收的那一部分Region的存活对象复制到空的Region中，再清理掉整个旧Region的全部空间。这里的操作涉及存活对象的移动，是必须暂停用户线程，由多条收集器线程并行完成的。 image-20201127201915199 低延迟垃圾收集器：HotSpot的垃圾收集器从Serial发展到CMS再到G1，经历了逾二十年时间，经过了数百上千万台服务器上的应用实践，已经被淬炼得相当成熟了，不过它们距离“完美”还是很遥远。衡量垃圾收集器的三项最重要的指标是：内存占用（Footprint）、吞吐量（Throughput）和延迟（Latency），三者共同构成了一个“不可能三角”。图3-14中浅色阶段表示必须挂起用户线程，深色表示收集器线程与用户线程是并发工作的。 image-20201127202152302 Shenandoah收集器：Shenandoah作为第一款不由Oracle（包括以前的Sun）公司的虚拟机团队所领导开发的HotSpot垃圾收集器，不可避免地会受到一些来自“官方”的排挤。Shenandoah反而更像是G1的下一代继承者，它们两者有着相似的堆内存布局，在初始标记、并发标记等许多阶段的处理思路上都高度一致，甚至还直接共享了一部分实现代码。虽然Shenandoah也是使用基于Region的堆内存布局，同样有着用于存放大对象的Humongous Region，默认的回收策略也同样是优先处理回收价值最大的Region……但在管理堆内存方面，它与G1至少有三个明显的不同之处，最重要的当然是支持并发的整理算法，G1的回收阶段是可以多线程并行的，但却不能与用户线程并发；其次，Shenandoah（目前）是默认不使用分代收集的；Shenandoah摒弃了在G1中耗费大量内存和计算资源去维护的记忆集，改用名为“连接矩阵”（Connection Matrix）的全局数据结构来记录跨Region的引用关系，降低了处理跨代指针时的记忆集维护消耗。大致上可以分为九个阶段：初始标记，并发标记，最终标记，并发清理，并发回收，初始引用更新，最终引用更新，并发清理。 image-20201127203047510 最重要三个阶段是并发标记、并发回收、并发引用更新。 ZCG收集器：Z Garbage Collector，是由Oracle公司研发的。ZGC和Shenandoah的目标是高度相似的，都希望在尽可能对吞吐量影响不太大的前提下[2]，实现在任意堆内存大小下都可以把垃圾收集的停顿时间限制在十毫秒以内的低延迟。ZGC也采用基于Region的堆内存布局，但与它们不同的是，ZGC的Region（在一些官方资料中将它称为Page或者ZPage，本章为行文一致继续称为Region）具有动态性——动态创建和销毁，以及动态的区域容量大小。Shenandoah使用转发指针和读屏障来实现并发整理，ZGC虽然同样用到了读屏障，但用的却是一条与Shenandoah完全不同，更加复杂精巧的解题思路：染色指针技术。分为四个阶段：并发标记，并发预备重分配，并发重分配，并发重映射。 image-20201127203846223 Epsilon收集器：这是一款以不能够进行垃圾收集为“卖点”的垃圾收集器，要负责堆的管理与布局、对象的分配、与解释器的协作、与编译器的协作、与监控子系统协作等职责，其中至少堆的管理和对象的分配这部分功能是Java虚拟机能够正常运作的必要支持，是一个最小化功能的垃圾收集器也必须实现的内容。对弈比较小的应用有用武之地。 第四章 虚拟机性能监控、故障处理工具基础故障处理工具： jps：虚拟机进程状况工具，可以列出正在运行的虚拟机进程，并显示虚拟机执行主类（Main Class，main()函数所在的类）名称以及这些进程的本地虚拟机唯一ID（LVMID，Local Virtual Machine Identifier）。 jstat（JVM Statistics Monitoring Tool）：用于监视虚拟机各种运行状态信息的命令行工具，可以显示本地或者远程[1]虚拟机进程中的类加载、内存、垃圾收集、即时编译等运行时数据。 jinfo（Configuration Info for Java）：实时查看和调整虚拟机各项参数。 jmap（Memory Map for Java）：用于生成堆转储快照（一般称为heapdump或dump文件）。 jhat（JVM Heap Analysis Tool）：与jmap搭配使用，来分析jmap生成的堆转储快照。 jstack（Stack Trace for Java）：用于生成虚拟机当前时刻的线程快照（一般称为threaddump或者 javacore文件），线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的目的通常是定位线程出现长时间停顿的原因，如线程间死锁、死循环。 可视化故障处理工具： JHSDB：基于服务性代理的调试工具 JConsole：Java监视与管理控制台 VisualVM：多合-故障处理工具，是功能最强大的运行监视和故障处理程序之一 JMC（Java Mission Control）：可持续在线的监控工具 第六章 类文件结构平台无关性：字节码(Byte Code)文件是构成平台无关性的基石，Java 虚拟机只接受字节码文件，而不管这些文件是怎么的得到的，这就为其他语言可以运行在 Java 虚拟机上提供了基础。 image-20201128225527804 Class类文件的结构： Class文件是一组以8个字节为基础单位的二进制流，并且按照 Big-Endian 来排列位数较大的数。 Class文件采用一种类似C语言的言结构体的伪结构来存储数据，这种伪结构中只有两种数据类型：“无符号数”和“表”。 无符号数属于基本的数据类型，以u1、u2、u4、u8来分别代表1个字节、2个字节、4个字节和8个字节的无符号数，无符号数可以用来描述数字、索引引用、数量值或者按照UTF-8编码构成字符串值。 表是由多个无符号数或者其他表作为数据项构成的复合数据类型，为了便于区分，所有表的命名都习惯性地以“_info”结尾。整个Class文件本质上也可以视作是一张表： image-20201128230232765 无论是无符号数还是表，当需要描述同一类型但数量不定的多个数据时，经常会使用一个前置的容量计数器加若干个连续的数据项的形式，这时候称这一系列连续的某一类型的数据为某一类型的“集合”。 魔数：每个Class文件的头4个字节被称为魔数（Magic Number），它的唯一作用是确定这个文件是否为一个能被虚拟机接受的Class文件。文件格式的制定者可以自由地选择魔数值，只要这个魔数值还没有被广泛采用过而且不会引起混淆。Class文件的魔数取得很有“浪漫气息”，值为0xCAFEBABE（咖啡宝贝？）。 Class 文件的版本：第5和第6个字节是次版本号（Minor Version），第7和第8个字节是主版本号（Major Version）。 常量池：常量池可以比喻为Class文件里的资源仓库，它是Class文件结构中与其他项目关联最多的数据，通常也是占用Class文件空间最大的数据项目之一。由于常量池中常量的数量是不固定的，所以在常量池的入口需要放置一项u2类型的数据，代表常量池容量计数值（constant_pool_count）。与Java中语言习惯不同，这个容量计数是从1而不是0开始的。这样做的目的在于，如果后面某些指向常量池的索引值的数据在特定情况下需要表达“不引用任何一个常量池项目”的含义，可以把索引值设置为0来表示。常量池中主要存放两大类常量：字面量（Literal）和符号引用（Symbolic References）。常量池中每一项常量都是一个表，截至JDK13，常量表中分别有17种不同类型的常量。这17类表都有一个共同的特点，表结构起始的第一位是个u1类型的标志位（tag，取值见表6-3中标志列），代表着当前常量属于哪种常量类型。17种常量类型： image-20201128235404018 访问标志：用于识别一些类或者接口层次的访问信息，包括：这个Class是类还是接口；是否定义为public类型；是否定义为abstract类型；如果是类的话，是否被声明为final； image-20201129000121788 类索引、父类索引与接口索引集合：类索引（this_class）和父类索引（super_class）都是一个u2类型的数据，而接口索引集合（interfaces）是一组u2类型的数据的集合，Class文件中由这三项数据来确定该类型的继承关系。 image-20201129091647440 字段表集合：字段表（field_info）用于描述接口或者类中声明的变量。 image-20201129091933843 字段修饰符放在access_flags项目中，跟随access_flags标志的是两项索引值：name_index descriptor_index。它们都是对常量池项的引用，分别代表着字段的简单名称以及字段和方法的描述符。 方法表集合：Class文件存储格式中对方法的描述与对字段的描述采用了几乎完全一致的方式，方法表的结构如同字段表一样，依次包括访问标志（access_flags）、名称索引（name_index）、描述符索引（descriptor_index）、属性表集合（attributes）几项，如表6-11所示。 image-20201129092210757 属性表集合：属性表（attribute_info）在前面的讲解之中已经出现过数次，Class文件、字段表、方法表都可以携带自己的属性表集合，以描述某些场景专有的信息。部分属性表如下： image-20201129092437699 对于每一个属性，它的名称都要从常量池中引用一个CONSTANT_Utf8_info类型的常量来表示，而属性值的结构则是完全自定义的，只需要通过一个u4的长度属性去说明属性值所占用的位数即可。 image-20201129092513520 字节码指令简介：由于Java虚拟机采用面向操作数栈而不是面向寄存器的架构，所以大多数指令都不包含操作数，只有一个操作码（一个字节），指令参数都存放在操作数栈中。由于Class文件格式放弃了编译后代码的操作数长度对齐，这就意味着虚拟机在处理那些超过一个字节的数据时，不得不在运行时从字节中重建出具体数据的结构；放弃了操作数长度对齐，就意味着可以省略掉大量的填充和间隔符号；用一个字节来代表操作码，也是为了尽可能获得短小精干的编译代码。 字节码和数据类型：大多数指令都包含其操作所对应的数据类型信息，如iload，fload。大部分指令都没有支持整数类型byte、char和short，甚至没有任何指令支持boolean类型。编译器会在编译期或运行期将byte和short类型的数据带符号扩展（Sign-Extend）为相应的int类型数据，将boolean和char类型数据零位扩展（Zero-Extend）为相应的int类型数据。 加载和存储指令：iload、iload_&lt;n&gt;、，istore、istore_&lt;n&gt;等 运算指令：算术指令用于对两个操作数栈上的值进行某种特定运算，并把结果重新存入到操作栈顶。分为两种：对整型数据进行运算的指令与对浮点型数据进行运算的指令。换句话说是不存在直接支持byte、short、char和boolean类型的算术指令，对于上述几种数据的运算，应使用操作int类型的指令代替。指令有：iadd、ladd、fadd、dadd等。Java虚拟机在进行浮点数运算时，所有的运算结果都必须舍入到适当的精度，非精确的结果必须舍入为可被表示的最接近的精确值；如果有两种可表示的形式与该值一样接近，那将优先选择最低有效位为零的；而在把浮点数转换为整数时，Java虚拟机使用IEEE 754标准中的向零舍入模式，这种模式的舍入结果会导致数字被截断，所有小数部分的有效字节都会被丢弃掉。 类型转换指令：Java虚拟机直接支持（即转换时无须显式的转换指令）宽化类型转换（即小范围类型向大范围类型的安全转换），处理窄化类型转换（Narrowing Numeric Conversion）时，就必须显式地使用转换指令来完成，这些转换指令包括i2b、i2c、i2s、l2i、f2i、f2l、d2i、d2l和d2f。在将int或long类型窄化转换为整数类型T的时候，转换过程仅仅是简单丢弃除最低位N字节以外的内容，N是类型T的数据类型长度，这将可能导致转换结果与输入值有不同的正负号。 对象创建和访问指令：对象创建后，就可以通过对象访问指令获取对象实例或者数组实例中的字段或者数组元素，这些指令包括： 创建类实例的指令：new 创建数组的指令：newarray、anewarray、multianewarray 访问类字段（static字段，或者称为类变量）和实例字段（非static字段，或者称为实例变量）的指令：getfield、putfield、getstatic、putstatic 把一个数组元素加载到操作数栈的指令：baload、caload、saload、iaload、laload、faload、daload、aaload 将一个操作数栈的值储存到数组元素中的指令：bastore、castore、sastore、iastore、fastore、dastore、aastore 取数组长度的指令：arraylength 检查类实例类型的指令：instanceof、checkcast 操作数栈管理指令：pop，dup，swap等 控制转移指令：ifeq，ret，if_icmpeq等 方法调用和返回指令： invokevirtual指令：用于调用对象的实例方法，根据对象的实际类型进行分派（虚方法分派），这也是Java语言中最常见的方法分派方式。 invokeinterface指令：用于调用接口方法，它会在运行时搜索一个实现了这个接口方法的对象，找出适合的方法进行调用。 invokespecial指令：用于调用一些需要特殊处理的实例方法，包括实例初始化方法、私有方法和父类方法。 invokestatic指令：用于调用类静态方法（static方法）。 invokedynamic指令：用于在运行时动态解析出调用点限定符所引用的方法。并执行该方法。前面四条调用指令的分派逻辑都固化在Java虚拟机内部，用户无法改变，而invokedynamic指令的分派逻辑是由用户所设定的引导方法决定的。 异常处理指令：在Java程序中显式抛出异常的操作（throw语句）都由athrow指令来实现。 同步指令：Java虚拟机可以支持方法级的同步和方法内部一段指令序列的同步，这两种同步结构都是使用管程（Monitor，更常见的是直接将它称为“锁”）来实现的。当方法调用时，调用指令将会检查方法的ACC_SYNCHRONIZED访问标志是否被设置，如果设置了，执行线程就要求先成功持有管程，然后才能执行方法，最后当方法完成（无论是正常完成还是非正常完成）时释放管程。同步一段指令集序列通常是由Java语言中的synchronized语句块来表示的，Java虚拟机的指令集中有monitorenter和monitorexit两条指令来支持synchronized关键字的语义。 公有设计，私有实现：《Java虚拟机规范》描绘了Java虚拟机应有的共同程序存储格式：Class文件格式以及字节码指令集。但一个优秀的虚拟机实现，在满足《Java虚拟机规范》的约束下对具体实现做出修改和优化也是完全可行的。虚拟机实现的方式主要有以下两种： 将输入的Java虚拟机代码在加载时或执行时翻译成另一种虚拟机的指令集； 将输入的Java虚拟机代码在加载时或执行时翻译成宿主机处理程序的本地指令集（即即时编译器代码生成技术） Class文件结构的发展：相对于语言、API以及Java技术体系中其他方面的变化，Class文件结构一直处于一个相对比较稳定的状态，Class文件的主体结构、字节码指令的语义和数量几乎没有出现过变动，所有对Class文件格式的改进，都集中在访问标志、属性表这些设计上原本就是可扩展的数据结构中添加新内容。 第七章 虚拟机类加载机制概述：Java虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这个过程被称作虚拟机的类加载机制。与那些在编译时需要进行连接的语言不同，在Java语言里面，类型的加载、连接和初始化过程都是在程序运行期间完成的，这种策略让Java语言进行提前编译会面临额外的困难，也会让类加载时稍微增加一些性能开销，但是却为Java应用提供了极高的扩展性和灵活性。 类加载的时机：一个类的整个生命周期如下： image-20201129102422665 加载、验证、准备、初始化和卸载这五个阶段的顺序是确定的，而解析阶段则不一定：它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定特性（也称为动态绑定或晚期绑定）。但是对于初始化阶段，《Java虚拟机规范》则是严格规定了有且只有六种情况必须立即对类进行“初始化”（而加载、验证、准备自然需要在此之前开始）： 遇到new、getstatic、putstatic或invokestatic这四条字节码指令时，如果类型没有进行过初始化，则需要先触发其初始化阶段。如使用new关键字，读取或设置一个类的静态字段，调用一个类的静态方法。 使用java.lang.reflect包的方法对类型进行反射调用的时候，如果类型没有进行过初始化，则需要先触发其初始化。 当初始化类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类。 如果一个java.lang.invoke.MethodHandle实例最后的解析结果为REF_getStatic、REF_putStatic、REF_invokeStatic、REF_newInvokeSpecial四种类型的方法句柄，并且这个方法句柄对应的类没有进行过初始化，则需要先触发其初始化。 当一个接口中定义了JDK 8新加入的默认方法（被default关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化。 类加载过程：加载、验证、准备、解析和初始化这五个阶段所执行的具体动作。 加载： 通过一个类的全限定名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。 验证：这一阶段的目的是确保Class文件的字节流中包含的信息符合《Java虚拟机规范》的全部约束要求，保证这些信息被当作代码运行后不会危害虚拟机自身的安全。 文件格式验证 元数据验证：这个类的父类是否继承了不允许被继承的类；如果这个类不是抽象类，是否实现了其父类或接口之中要求实现的所有方法 字节码验证：这阶段就要对类的方法体（Class文件中的Code属性）进行校验分析，如保证任何跳转指令都不会跳转到方法体以外的字节码指令上，保证方法体中的类型转换总是有效的等。 符号引用验证：验证该类是否缺少或者被禁止访问它依赖的某些外部类、方法、字段等资源。如符号引用中的类、字段、方法的可访问性（private、protected、public、&lt;package&gt;）是否可被当前类访问等。 准备：是正式为类中定义的变量（即静态变量，被static修饰的变量）分配内存并设置类变量初始值的阶段。需要注意的是如果是static int value = 123，准备阶段的初始值是0而不是123，因为这时尚未开始执行任何Java方法，而把value赋值为123的putstatic指令是程序被编译后，存放于类构造器&lt;clinit&gt;()方法之中的；但是如果是public static final int value = 123，那么准备阶段的值就是123。 解析：是Java虚拟机将常量池内的符号引用替换为直接引用的过程。 类或接口的解析：如果C不是一个数组类型，那虚拟机将会把代表N的全限定名传递给D的类加载器去加载这个类C。在加载过程中，由于元数据验证、字节码验证的需要，又可能触发其他相关类的加载动作，例如加载这个类的父类或实现的接口。一旦这个加载过程出现了任何异常，解析过程就将宣告失败。成功的话，那么C在虚拟机中实际上已经成为一个有效的类或接口了，但在解析完成前还要进行符号引用验证，确认D是否具备对C的访问权限。 字段解析 方法解析 接口方法解析 初始化：直到初始化阶段，Java虚拟机才真正开始执行类中编写的Java程序代码，将主导权移交给应用程序。初始化阶段就是执行类构造器&lt;clinit&gt;()方法的过程。&lt;clinit&gt;()并不是程序员在Java代码中直接编写的方法，它是Javac编译器的自动生成物。&lt;clinit&gt;()方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块（static{}块）中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序决定的，静态语句块中只能访问到定义在静态语句块之前的变量。Java虚拟机会保证在子类的&lt;clinit&gt;()方法执行前，父类的&lt;clinit&gt;()方法已经执行完毕。 类加载器：Java虚拟机设计团队有意把类加载阶段中的“通过一个类的全限定名来获取描述该类的二进制字节流”这个动作放到Java虚拟机外部去实现，以便让应用程序自己决定如何去获取所需的类。实现这个动作的代码被称为“类加载器”（Class Loader）。 类与类加载器：比较两个类是否“相等”，只有在这两个类是由同一个类加载器加载的前提下才有意义，否则，即使这两个类来源于同一个Class文件，被同一个Java虚拟机加载，只要加载它们的类加载器不同，那这两个类就必定不相等。 双亲委派模型： 三层类加载器： 启动类加载器：这个类加载器负责加载存放在&lt;JAVA_HOME&gt;\\lib目录，是Java虚拟机能够识别的（按照文件名识别，如rt.jar、tools.jar，名字不符合的类库即使放在lib目录中也不会被加载）类库加载到虚拟机的内存中。 扩展类加载器：负责加载\\lib\\ext目录中，或者被java.ext.dirs系统变量所指定的路径中所有的类库。 应用程序类加载器：由于应用程序类加载器是ClassLoader类中的getSystemClassLoader()方法的返回值，所以有些场合中也称它为“系统类加载器”。它负责加载用户类路径（ClassPath）上所有的类库，开发者同样可以直接在代码中使用这个类加载器。 JDK 9之前的Java应用都是由这三种类加载器互相配合来完成加载的，如果用户认为有必要，还可以加入自定义的类加载器来进行拓展： image-20201129130856054 双亲委派模型：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到最顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去完成加载。好处是Java中的类随着它的类加载器一起具备了一种带有优先级的层次关系。 Java模块化系统： 模块的兼容性：JDK 9提出了与“类路径”（ClassPath）相对应的“模块路径”（ModulePath）的概念。简单来说，就是某个类库到底是模块还是传统的JAR包，只取决于它存放在哪种路径上。有如下访问规则： JAR文件在类路径的访问规则：所有类路径下的JAR文件及其他资源文件，都被视为自动打包在一个匿名模块（Unnamed Module）里，这个匿名模块几乎是没有任何隔离的，它可以看到和使用类路径上所有的包、JDK系统模块中所有的导出包，以及模块路径上所有模块中导出的包。 模块在模块路径的访问规则：模块路径下的具名模块（Named Module）只能访问到它依赖定义中列明依赖的模块和包，匿名模块里所有的内容对具名模块来说都是不可见的，即具名模块看不见传统JAR包的内容。 JAR文件在模块路径的访问规则：如果把一个传统的、不包含模块定义的JAR文件放置到模块路径中，它就会变成一个自动模块（Automatic Module）。尽管不包含module-info.class，但自动模块将默认依赖于整个模块路径中的所有模块，因此可以访问到所有模块导出的包，自动模块也默认导出自己所有的包。 模块下的类加载器：JDK 9并没有从根本上动摇从JDK 1.2以来运行了二十年之久的三层类加载器架构以及双亲委派模型。但是为了模块化系统的顺利施行，模块化下的类加载器仍然发生了一些应该被注意到变动，主要包括以下几个方面： 扩展类加载器被平台类加载器取代 平台类加载器和应用程序类加载器都不再派生自java.net.URLClassLoader，现在启动类加载器、平台类加载器、应用程序类加载器全都继承于jdk.internal.loader.BuiltinClassLoader，在BuiltinClassLoader中实现了新的模块化架构下类如从模块中加载的逻辑，以及模块中资源可访问性的处理 JDK 9中虽然仍然维持着三层类加载器和双亲委派的架构，但类加载的委派关系也发生了变动。当平台及应用程序类加载器收到类加载请求，在委派给父加载器加载前，要先判断该类是否能够归属到某一个系统模块中，如果可以找到这样的归属关系，就要优先委派给负责那个模块的加载器完成加载 image-20201129132525034 第八章 虚拟机字节码执行引擎运行时栈帧结构：下图是 JVM 的栈和栈帧的总体结构： image-20201201211749470 每个栈帧里面包含有局部变量表、操作数栈、动态连接、方法返回地址和一些额外的附加信息。 局部变量表：是一组变量值的存储空间，用于存放方法参数和方法内部定义的局部变量。变量槽空间一般是 32 位，对于long类型的变量，需要两个槽来保存。当方法被调用的时候，首先存储相关的实参，然后再存储方法内部的局部变量。比如，对于实例方法，局部变量表第0位代表的就是方法所属对象的引用，方法中通过 this 隐式访问到。另外，局部变量表中的变量槽可以被重用，这可能会带来副作用，如 gc 过程。最后，局部变量没有所谓的“准备阶段”，因此，对局部变量引用前需要先赋值。 操作数栈：用于保存相应的操作数。在进行运算的时候需要检查指令和对应的数据类型是否匹配。在概念模型上，两个不同的栈帧是完全相互独立的，但是在实际过程中，可能存在重合，这样做的好处是节约空间，同时无需进行额外的实参-形参转换。 image-20201201213604264 动态链接：每个栈帧都包含一个指向运行时常量池[1]中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接（Dynamic Linking）。 方法返回地址：正常返回上层方法调用者，可能会提供返回值，异常返回的话，不带任何返回值。推出的过程实际上等同于将当前栈帧出栈。 方法调用： 解析：在类加载的过程中，如果方法的调用版本在运行期不可变，就可以将方法的符号引用转化为直接引用，该类方法的调用称为解析。在 Java 中，这样的方法有静态方法，私有方法，实例构造器，父类方法，final 方法。这些方法称为“非虚方法”，其他的就成为“虚方法”。 分派（dispatch）： 静态分派：假设 Human man = new Man()，那么Human成为变量的静态类型，或者是外观类型，后面的Man则称为变量的实际类型或者运行时类型。所有依赖静态类型来决定方法执行版本的分派动作，都称为静态分派。静态分派的最典型应用表现就是方法重载。静态分派发生在编译阶段。虽然编译器能够在确定方法重载版本，但是实际上只是选择一个相对更合适的版本。假设有一个类实现了sayHello方法，重载了所有类型的参数。那么对应sayHello(&#39;a&#39;)中的a的类型被解析为char，如果注释掉char类型的重载，那么a会被解析成int类型，依次往后是：Character，Serializable(Character的一个接口)，Object（父类），变长参数。 动态分派：与重写有关。Human man = new Man()，man执行重写方法的时候，会执行Man类里面的对应的方法，而不是Woman里面的重载方法，这与变量的实际类型有关。调用重写方法的时候，执行指令是invokevirtual，其运行过程如下： 找到变量指向对象的实际类型，记做C 如果在C中找到与方法签名一直的方法，进行访问权限校验，通过直接返回这个方法的直接调用，否则返回java.lang.IllegalAccessError 否则，按照继承关系从下往上依次对C的各个父类进行2操作 如果始终没有找到合适方法，抛出java.lang.AbstractMethodError异常 注意，方法存在多态，但是字段不存在多态。 虚拟机动态分派的实现：通常虚拟机会创建一个虚方法表（vtable，对应的还有接口方法表itable），使用虚方法表索引来代替元数据查找以提高性能。 image-20201201230836050 动态类型语言支持： 动态类型语言：动态类型语言的关键特征是它的类型检查的主体过程是在运行期而不是编译期进行的，如Javascript等。那相对地，在编译期就进行类型检查过程的语言，譬如C++和Java等就是最常用的静态类型语言。 Java与动态类型：在 Java7 之前的4条方法调用指令（invoke*）的第一个参数都是被调用方法的符号引用。前面已经提到过，方法的符号引用在编译时产生，而动态类型语言只有在运行期才能确定方法的接收者。这样，在Java虚拟机上实现的动态类型语言就不得不使用“曲线救国”的方式（如编译时留个占位符类型，运行时动态生成字节码实现具体类型到占位符类型的适配）来实现，但这样势必会让动态类型语言实现的复杂度增加，也会带来额外的性能和内存开销。 java.lang.invoke：该包提供了一种新的动态确定方法的机制，称为方法句柄。 1234567891011121314151617181920212223import static java.lang.invoke.MethodHandles.lookup;import java.lang.invoke.MethodHandle;import java.lang.invoke.MethodType;public class MethodHandleTest &#123; static class ClassA &#123; public void println(String s) &#123; System.out.println(s); &#125; &#125; public static void main(String[] args) throws Throwable &#123; Object obj = System.currentTimeMillis() % 2 == 0 ? System.out : new ClassA(); // 无论obj最终是哪个实现类，下面这句都能正确调用到println方法。 getPrintlnMH(obj).invokeExact(&quot;icyfenix&quot;); &#125; private static MethodHandle getPrintlnMH(Object reveiver) throws Throwable &#123; // MethodType：代表“方法类型”，包含了方法的返回值（methodType()的第一个参数）和具体参数（methodType()第二个及以后的参数）。 MethodType mt = MethodType.methodType(void.class, String.class); // lookup()方法来自于MethodHandles.lookup，这句的作用是在指定类中查找符合给定的方法名称、方法类型，并且符合调用权限的方法句柄。 // 因为这里调用的是一个虚方法，按照Java语言的规则，方法第一个参数是隐式的，代表该方法的接收者，也即this指向的对象，这个参数以前是放在参数列表中进行传递，现在提供了bindTo()方法来完成这件事情。 return lookup().findVirtual(reveiver.getClass(), &quot;println&quot;, mt).bindTo(reveiver); &#125;&#125; MethodHandle在使用方法和效果上与Reflection有众多相似之处。不过，它们也有以下这些区别： Reflection和MethodHandle机制本质上都是在模拟方法调用，但是Reflection是在模拟Java代码层次的方法调用，而MethodHandle是在模拟字节码层次的方法调用。 Reflection中的java.lang.reflect.Method对象远比MethodHandle机制中的java.lang.invoke.MethodHandle对象所包含的信息来得多。 Reflection API的设计目标是只为Java语言服务的，而MethodHandle则设计为可服务于所有Java虚拟机之上的语言。 invokedynamic指令：作为Java诞生以来唯一一条新加入的字节码指令，都是为了解决原有4条“invoke*”指令方法分派规则完全固化在虚拟机之中的问题，把如何查找目标方法的决定权从虚拟机转嫁到具体用户代码之中。invokedynamic指令的第一个参数不再是代表方法符号引用的CONSTANT_Methodref_info常量，而是变为JDK 7时新加入的CONSTANT_InvokeDynamic_info常量，从这个新常量中可以得到3项信息：引导方法（Bootstrap Method，该方法存放在新增的BootstrapMethods属性中）、方法类型（MethodType）和名称。 掌控方法分派规则：子类方法不能直接调用祖父类方法，可以通过MethodHandle来进行访问，如遇到权限问题，可以使用lookupImpl.setAccessible(true)来解决。 基于栈的字节码解释执行引擎： 解释执行：Java语言被定为解释执行的语言，这在JDK1.0时代算是准确的，但是之后Java也发展出了可以生成本地代码的编译器，这个时候说Java是解释执行的语言就不再准确了。下图中间分支指代解释执行过程，最下面分支指代编译执行过程： image-20201202095616929 基于栈的指令集与基于寄存器的指令集：Java指令基于栈结构，x86指令基于寄存器，使用栈结构带来的好处是可移植性更强，缺点是运行速度慢。 第九章 类加载案例案例分析： Tomcat：在Tomcat中一种有四种目录存放Java类库： 放置在/common目录中。类库可被Tomcat和所有的Web应用程序共同使用。 放置在/server目录中。类库可被Tomcat使用，对所有的Web应用程序都不可见。 放置在/shared目录中。类库可被所有的Web应用程序共同使用，但对Tomcat自己不可见。 放置在/WebApp/WEB-INF目录中。类库仅仅可以被该Web应用程序使用，对Tomcat和其他Web应用程序都不可见。 为了支持这套目录，并且对目录里面的类库进行加载和隔离，Tomcat实现了自定义的类加载器，按照双亲委派模型： image-20201203105110135 在Tomcat6之后，只有指定了tomcat/conf/catalina.properties配置文件的server.loader和share.loader项后才会真正建立Catalina类加载器和Shared类加载器的实例，否则会用到这两个类加载器的地方都会用Common类加载器的实例代替。同时前文提到的前三个目录也会被改为一个/lib目录。 OSGi：是OSGi联盟（OSGi Alliance）制订的一个基于Java语言的动态模块化规范。OSGi中的每个模块（Bundle）可以声明它所依赖的Package（通过Import-Package描述），也可以声明它允许导出发布的Package（通过Export-Package描述）。这和后来出现的Java模块化功能重合了。由于模块之间相依依赖的原因，加载器之间的关系不再是双亲委派模型的树形结构，而是已经进一步发展成一种更为复杂的、运行时才能确定的网状结构。在模块中相互依赖会造成死锁。 Backport工具：将高级的Java语法转化为低版本Java也能运行的语句代码的工具。 第十章 前端编译和优化Java中前端编译一般指将*.java编译为*.class字节码文件的过程，主要有下列过程： 准备过程：初始化插入式注解处理器 解析与填充符号表过程：词法，语法分析，填充符号表 插入式注解处理器的注解处理过程：插入式注解处理器的执行阶段 分析与字节码生成过程：标注检查，解语法糖，字节码生成 image-20201205124231612 image-20201205124301591 解析与填充符号表： 词法语法分析：词法分析用于生成标记（token）集合的过程，语法分析则是根据标记序列构造抽象语法树的过程。 填充符号表：符号表（Symbol Table）是由一组符号地址和符号信息构成的数据结构，符号表中所登记的信息在编译的不同阶段都要被用到，如类型检查等。 注解处理器：可以把插入式注解处理器看作是一组编译器的插件，当这些插件工作时，允许读取、修改、添加抽象语法树中的任意元素。如果这些插件在处理注解期间对语法树进行过修改，编译器将回到解析及填充符号表的过程重新处理，直到所有插入式注解处理器都没有再对语法树进行修改为止，每一次循环过程称为一个轮次（Round）。 语义分析和字节码生成： 标注检查：变量使用前是否已被声明、变量与赋值之间的数据类型是否能够匹配，等等，在该过程中顺便执行常量折叠优化。 数据及控制流分析：是对程序上下文逻辑更进一步的验证，它可以检查出诸如程序局部变量在使用前是否有赋值、方法的每条路径是否都有返回值、是否所有的受查异常都被正确处理了等问题。final修饰的变量不可变就是在这一阶段完成的。 解语法糖：将一些语法糖进行还原，Java中常见语法糖有泛型，变长参数，自动装箱拆箱等。 字节码生成：把前面各个步骤所生成的信息（语法树、符号表）转化成字节码指令写到磁盘中，编译器还进行了少量的代码添加和转换工作。例如前文多次登场的实例构造器&lt;init&gt;()方法和类构造器&lt;clinit&gt;()方法就是在这个阶段被添加到语法树之中的。实例构造器并不等同于默认构造函数。&lt;init&gt;()和&lt;clinit&gt;()这两个构造器的产生实际上是一种代码收敛的过程，编译器会把语句块（对于实例构造器而言是“{}”块，对于类构造器是“static{}”块）、变量初始化（实例变量和类变量）、调用父类的实例构造器（仅仅是实例构造器，&lt;clinit&gt;()方法中无须调用父类的&lt;clinit&gt;()方法，Java虚拟机会自动保证父类构造器的正确执行，但在&lt;clinit&gt;()方法中经常会生成调用java.lang.Object的&lt;init&gt;()方法的代码）等操作收敛到&lt;init&gt;()和&lt;clinit&gt;()方法之中。 Java语法糖： 泛型：Java选择的泛型实现方式是类型擦除式泛型，而C#选择的泛型实现方式是具现化式泛型。由于采用的是类型擦出式泛型，以下操作不合法： 123456789public class TypeErasureGenerics&lt;E&gt; &#123; public void doSomething(Object item) &#123; if (item instanceof E) &#123; // 不合法，无法对泛型进行实例判断 ... &#125; E newItem = new E(); // 不合法，无法使用泛型创建对象 E[] itemArray = new E[10]; // 不合法，无法使用泛型创建数组 &#125;&#125; java中的泛型只在程序源码中存在，在编译后的字节码文件中，全部泛型都被替换为原来的裸类型（Raw Type）了，并且在相应的地方插入了强制转型代码，因此对于运行期的Java语言来说，ArrayList&lt;int&gt;与ArrayList&lt;String&gt;其实是同一个类型。当初Java选择这种方式实现泛型的历史原因在于Java语言的向后兼容性。 image-20201205130631950 将这段Java代码编译成Class文件，然后再使用反编译工具： image-20201205130714474 由于类型擦除，导致的问题有：不支持原始类型的泛型，运行期无法取到泛型类型信息。 image-20201205131020421 image-20201205131111259 上述代码不能被编译，相反下列代码可以被编译，因为方法签名不同： image-20201205131152443 自动装箱，拆箱与遍历循环： image-20201205131228596 遍历循环的类需要实现Iterable接口的原因从上图可以看出。 条件编译：java语言没有预处理器，但是可以实现条件编译，使用if(true)等。 第十一章 后端编译和优化即时（JIT）编译器：在运行时，虚拟机将会把热点代码编译成本地机器码，并以各种手段尽可能地进行代码优化，运行时完成这个任务的后端编译器被称为即时编译器。 解释器和编译器：解释器与编译器两者各有优势：当程序需要迅速启动和执行的时候，解释器可以首先发挥作用，省去编译的时间，立即运行。当程序启动后，随着时间的推移，编译器逐渐发挥作用，把越来越多的代码编译成本地代码，这样可以减少解释器的中间损耗，获得更高的执行效率。在Java中，解释器和编译器是相互协作的： image-20201205155102424 为了使程序启动响应速度与运行效率之间达到最佳平衡，HotSpot虚拟机在编译子系统中加入了分层编译的功能。 image-20201205155331944 编译对象与触发条件：热点代码指的是被多次调用的方法，或者是多次执行的循环体。发现对应的热点代码后，编译的目标对象都是整个方法体。为了判断某段代码是不是热点代码，可以使用基于采样的热点探测和基于计数器的热点探测。HotSpot使用的是后者，为了实现热点计数，需要两类计数器：方法调用计数器和回边计数器。一旦超过阈值，就会触发即时编译。 编译过程：在编译请求产生时，虚拟机在编译器还未完成编译之前，都仍然将按照解释方式继续执行代码，而编译动作则在后台的编译线程中进行。下图是编译器的全过程示意图： image-20201205155947036 提前编译器：目前提前编译有两条路径：一条分支是做与传统C、C++编译器类似的，在程序运行之前把程序代码编译成机器码的静态翻译工作；另外一条分支是把原本即时编译器在运行时要做的编译工作提前做好并保存下来，下次运行到这些代码（譬如公共库代码在被同一台机器其他Java进程使用）时直接把它加载进来使用。第一条路径直指Java中的即时编译的最大弱点：即时编译需要占用程序的运行时间和运算资源。第二条路径本质上是给即时编译器做缓存加速，可以成为动态提前编译（Dynamic AOT）。即时编译器的优点在：性能分析制导优化，激进预测性优化和链接时优化。Java中的提前编译器有Jaotc。 编译器优化技术： 方法内联：是其他优化的基础，减少方法分派的开销 逃逸分析：分析对象动态作用域，当一个对象在方法里面被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他方法中，这种称为方法逃逸；甚至还有可能被外部线程访问到，譬如赋值给可以在其他线程中访问的实例变量，这种称为线程逃逸；从不逃逸、方法逃逸到线程逃逸，称为对象由低到高的不同逃逸程度。根据不同逃逸程度：可以执行栈上分配，标量替换，同步消除。 公共子表达式消除 数组边界检查消除 第十二章 Java内存模型与线程硬件的效率与一致性：由于处理器的运行速度远高于IO的速度，为此引入了高速缓存，但是引入高速缓存又造成了缓存一致性的问题。由此产生一致性协议：MSI，MESI，MOSI等。 image-20201205185105826 Java内存模型： 主内存与工作内存：规定了所有的变量都存储在主内存（Main Memory）中，每条线程还有自己的工作内存（Working Memory，可与前面讲的处理器高速缓存类比），线程的工作内存中保存了被该线程使用的变量的主内存副本，线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，而不能直接读写主内存中的数据。 image-20201205185323393 内存间交互操作：lock，unlock，read，load，store，write，use，assign。伤处操作都是原子的，不可再分的。 对于volatile型变量的特殊规则：当一个变量被volatile定义的时候，将具有： 保证此变量对所有线程的可见性，这里的“可见性”是指当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。 禁止指令重排序优化，普通的变量仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执行顺序一致。 针对long和double型变量的特殊规则：允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32位的操作来进行，即允许虚拟机实现自行选择是否要保证64位数据类型的load、store、read和write这四个操作的原子性。 原子性，可见性与有序性：由Java内存模型来直接保证的原子性变量操作包括read、load、assign、use、store和write这六个；可见性就是指当一个线程修改了共享变量的值时，其他线程能够立即得知这个修改；如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的。 先行发生原则： 程序次序规则：在一个线程内，按照控制流顺序，书写在前面的操作先行发生于书写在后面的操作，是控制流顺序。 管程锁定规则：一个unlock操作先行发生于后面对同一个锁的lock操作。 volatile变量规则：对一个volatile变量的写操作先行发生于后面对这个变量的读操作。 线程启动规则：Thread对象的start()方法先行发生于此线程的每一个动作。 线程终止规则：线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread::join()方法是否结束、Thread::isAlive()的返回值等手段检测线程是否已经终止执行。 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread::interrupted()方法检测到是否有中断发生。 对象终结规则：一个对象的初始化完成（构造函数执行结束）先行发生于它的finalize()方法的开始。 传递性：如果操作A先行发生于操作B，操作B先行发生于操作C，那就可以得出操作A先行发生于操作C的结论。 Java与线程： 线程的实现：内核线程实现，用户线程实现，混合实现。 Java线程调度： 协同式线程调度：线程的执行时间由线程本身来控制，线程把自己的工作执行完了之后，要主动通知系统切换到另外一个线程上去。 抢占式线程：每个线程将由系统来分配执行时间，线程的切换不由线程本身来决定。 Java线程调度是由系统自动完成的，但是可以为不同的线程分配不同的优先级，来建议操作系统多分配一些时间在优先级高的线程上。 状态转换：Java中定义了6种线程状态： image-20201205194306856 Java与协程： 内核线程的局限：天然的缺陷是切换、调度成本高昂，系统能容纳的线程数量也很有限。 协程的复苏：由于最初多数的用户线程是被设计成协同式调度的，所以它有了一个别名——“协程”（Coroutine）。又由于这时候的协程会完整地做调用栈的保护、恢复工作，所以今天也被称为“有栈协程”。协程的主要优势是轻量，缺点是需要在应用层面实现的内容（调用栈、调度器这些）特别多。 Java的解决方案：纤程（fiber），一种轻量的线程，使用JVM调度，而不是操作系统。 第十三章 线程安全与锁优化线程安全：当多个线程同时访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那就称这个对象是线程安全的。 Java语言中的线程安全： 不可变：不可变的对象一定是线程安全，无论是对象的方法实现还是方法的调用者，都不需要再进行任何线程安全保障措施。基本类型数据使用final关键字修饰可以保证不可变，如果想要保证对象不可变，需要将对象的字段设置为final才可以。 绝对线程安全：不管运行时环境如何，调用者都不需要任何额外的同步措施。 相对线程安全：通常意义上所讲的线程安全，它需要保证对这个对象单次的操作是线程安全的，我们在调用的时候不需要进行额外的保障措施，但是对于一些特定顺序的连续调用，就可能需要在调用端使用额外的同步手段来保证调用的正确性。在Java语言中，大部分声称线程安全的类都属于这种类型，例如Vector。 线程兼容：指对象本身并不是线程安全的，但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全地使用。我们平常说一个类不是线程安全的，通常就是指这种情况。Java中的ArrayList就是这种情况。 线程对立：是指不管调用端是否采取了同步措施，都无法在多线程环境中并发使用代码。 线程安全的方法实现： 互斥同步：临界区（Critical Section）、互斥量（Mutex）和信号量（Semaphore）都是常见的互斥实现方式。在Java里面，互斥同步手段是synchronized关键字，这是一种块结构的同步语法。该关键字经过编译之后，会产生monitorenter和monitorexit这两个字节码指令。这两个字节码指令都需要一个reference类型的参数来指明要锁定和解锁的对象。如果Java源码中的synchronized明确指定了对象参数，那就以这个对象的引用作为reference；如果没有明确指定，那将根据synchronized修饰的方法类型（如实例方法或类方法），来决定是取代码所在的对象实例还是取类型对应的Class对象来作为线程要持有的锁。另外的话也有重入锁（ReentrantLock），相较于synchronized，重入锁提供：等待可中断，公平锁，锁绑定多个条件。 非阻塞同步：互斥同步面临的主要问题是进行线程阻塞和唤醒所带来的性能开销，因此这种同步也被称为阻塞同步。基于冲突检测的乐观并发策略，通俗地说就是不管风险，先进行操作，如果没有其他线程争用共享数据，那操作就直接成功了；如果共享的数据的确被争用，产生了冲突，那再进行其他的补偿措施，最常用的补偿措施是不断地重试，直到出现没有竞争的共享数据为止。这种乐观并发策略的实现不再需要把线程阻塞挂起，因此这种同步操作被称为非阻塞同步。这种方法需要硬件支持，因为我们必须要求操作和冲突检测这两个步骤具备原子性。如测试并设置（Test-and-Set）；获取并增加（Fetch-and-Increment）；交换（Swap）；比较并交换（Compare-and-Swap，下文称CAS）。 无同步方案：如果能让一个方法本来就不涉及共享数据，那它自然就不需要任何同步措施去保证其正确性，因此会有一些代码天生就是线程安全的。 锁优化： 自旋锁与自适应自旋：互斥同步对性能最大的影响是阻塞的实现，挂起线程和恢复线程的操作都需要转入内核态中完成，这些操作给Java虚拟机的并发性能带来了很大的压力。如果物理机器有一个以上的处理器或者处理器核心，能让两个或以上的线程同时并行执行，我们就可以让后面请求锁的那个线程“稍等一会”，但不放弃处理器的执行时间，看看持有锁的线程是否很快就会释放锁。为了让线程等待，我们只须让线程执行一个忙循环（自旋），这项技术就是所谓的自旋锁。自适应意味着自旋的时间不再是固定的了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定的。 锁消除：锁消除是指虚拟机即时编译器在运行时，对一些代码要求同步，但是对被检测到不可能存在共享数据竞争的锁进行消除。锁消除的主要判定依据来源于逃逸分析的数据支持，如果判断到一段代码中，在堆上的所有数据都不会逃逸出去被其他线程访问到，那就可以把它们当作栈上数据对待，认为它们是线程私有的，同步加锁自然就无须再进行。 锁粗化：原则上，总是推荐将同步块的作用范围限制得尽量小，这样是为了使得需要同步的操作数量尽可能变少，即使存在锁竞争，等待锁的线程也能尽可能快地拿到锁。但是如果一系列的连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体之中的，那即使没有线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗。因此可以进行锁粗化操作。 轻量级锁：HotSpot虚拟机对象头布局： image-20201206125352406 在代码即将进入同步块的时候，如果此同步对象没有被锁定（锁标志位为“01”状态），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝： image-20201206125505870 然后，虚拟机将使用CAS操作尝试把对象的Mark Word更新为指向Lock Record的指针。如果这个更新动作成功了，即代表该线程拥有了这个对象的锁，并且对象Mark Word的锁标志位将转变为“00”，表示此对象处于轻量级锁定状态。 image-20201206125621073 偏向锁：它的目的是消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能。如果说轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS操作都不去做了。偏向锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁一直没有被其他的线程获取，则持有偏向锁的线程将永远不需要再进行同步。 image-20201206125818939 在Java语言里面一个对象如果计算过哈希码，就应该一直保持该值不变，否则很多依赖对象哈希码的API都可能存在出错风险。因此，当一个对象已经计算过一致性哈希码后，它就再也无法进入偏向锁状态了；而当一个对象当前正处于偏向锁状态，又收到需要计算其一致性哈希码请求时，它的偏向状态会被立即撤销，并且锁会膨胀为重量级锁。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.zsstrike.top/tags/Java/"}]},{"title":"设计模式","slug":"设计模式","date":"2020-11-07T02:52:47.000Z","updated":"2020-12-17T10:23:43.713Z","comments":true,"path":"2020/11/07/设计模式/","link":"","permalink":"http://blog.zsstrike.top/2020/11/07/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。本文介绍设计模式。","text":"设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。本文介绍设计模式。 设计模式简介设计模式是一套被反复使用的、多数人知晓的、经过分类编目的、代码设计经验的总结。使用设计模式是为了重用代码、让代码更容易被他人理解、保证代码可靠性。 GoF：四位作者合称，他们提出的设计模式主要基于以下面向对象设计原则： 对接口编程而不是对实现编程。 优先使用对象组合而不是继承。 设计模式的用途：是开发人员的共同平台，代表着最佳的实践。 设计模式的类型：创建型模式，结构型模式，行为型模式。另外将介绍 J2EE 模式。 序号 模式 &amp; 描述 包括 1 创建型模式 这些设计模式提供了一种在创建对象的同时隐藏创建逻辑的方式，而不是使用 new 运算符直接实例化对象。这使得程序在判断针对某个给定实例需要创建哪些对象时更加灵活。 工厂模式（Factory Pattern）抽象工厂模式（Abstract Factory Pattern）单例模式（Singleton Pattern）建造者模式（Builder Pattern）原型模式（Prototype Pattern） 2 结构型模式 这些设计模式关注类和对象的组合。继承的概念被用来组合接口和定义组合对象获得新功能的方式。 适配器模式（Adapter Pattern）桥接模式（Bridge Pattern）过滤器模式（Filter、Criteria Pattern）组合模式（Composite Pattern）装饰器模式（Decorator Pattern）外观模式（Facade Pattern）享元模式（Flyweight Pattern）代理模式（Proxy Pattern） 3 行为型模式 这些设计模式特别关注对象之间的通信。 责任链模式（Chain of Responsibility Pattern）命令模式（Command Pattern）解释器模式（Interpreter Pattern）迭代器模式（Iterator Pattern）中介者模式（Mediator Pattern）备忘录模式（Memento Pattern）观察者模式（Observer Pattern）状态模式（State Pattern）空对象模式（Null Object Pattern）策略模式（Strategy Pattern）模板模式（Template Pattern）访问者模式（Visitor Pattern） 4 J2EE 模式 这些设计模式特别关注表示层。这些模式是由 Sun Java Center 鉴定的。 MVC 模式（MVC Pattern） 业务代表模式（Business Delegate Pattern） 组合实体模式（Composite Entity Pattern）数据访问对象模式（Data Access Object Pattern）前端控制器模式（Front Controller Pattern）拦截过滤器模式（Intercepting Filter Pattern）服务定位器模式（Service Locator Pattern）传输对象模式（Transfer Object Pattern） 设计模式六大原则： 开闭原则：对扩展开放，对修改关闭，实现热插拔，提高扩展性 里氏代换原则：任何基类可以出现的地方，子类一定可以出现，实现抽象的规范，实现子父类互相替换 依赖倒转原则：针对接口编程，依赖于抽象而不依赖于具体 接口隔离原则：使用多个隔离的接口，比使用单个接口要好，降低类之间的耦合度 迪米特法则：一个实体应当尽量少地与其他实体之间发生相互作用，使得系统功能模块相对独立 合成复用原则：尽量使用合成/聚合的方式，而不是使用继承 工厂模式介绍：工厂模式（Factory Pattern）是 Java 中最常用的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。 优点：1、一个调用者想创建一个对象，只要知道其名称就可以了。 2、扩展性高，如果想增加一个产品，只要扩展一个工厂类就可以。 3、屏蔽产品的具体实现，调用者只关心产品的接口。 缺点：每次增加一个产品时，都需要增加一个具体类和对象实现工厂，使得系统中类的个数成倍增加，在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。 使用场景： 1、日志记录器：记录可能记录到本地硬盘、系统事件、远程服务器等，用户可以选择记录日志到什么地方。 2、数据库访问，当用户不知道最后系统采用哪一类数据库，以及数据库可能有变化时。 3、设计一个连接服务器的框架，需要三个协议，”POP3”、”IMAP”、”HTTP”，可以把这三个作为产品类，共同实现一个接口。 实现： 工厂模式的 UML 图 1234567891011121314151617public class ShapeFactory &#123; //使用 getShape 方法获取形状类型的对象 public Shape getShape(String shapeType)&#123; if(shapeType == null)&#123; return null; &#125; if(shapeType.equalsIgnoreCase(&quot;CIRCLE&quot;))&#123; return new Circle(); &#125; else if(shapeType.equalsIgnoreCase(&quot;RECTANGLE&quot;))&#123; return new Rectangle(); &#125; else if(shapeType.equalsIgnoreCase(&quot;SQUARE&quot;))&#123; return new Square(); &#125; return null; &#125;&#125; 抽象工厂模式介绍：抽象工厂模式（Abstract Factory Pattern）是围绕一个超级工厂创建其他工厂。该超级工厂又称为其他工厂的工厂。在抽象工厂模式中，接口是负责创建一个相关对象的工厂，不需要显式指定它们的类。每个生成的工厂都能按照工厂模式提供对象。 优点：当一个产品族中的多个对象被设计成一起工作时，它能保证客户端始终只使用同一个产品族中的对象。 缺点：产品族扩展非常困难，要增加一个系列的某一产品，既要在抽象的 Creator 里加代码，又要在具体的里面加代码。 使用场景： 1、QQ 换皮肤，一整套一起换。 2、生成不同操作系统的程序。 实现： 抽象工厂模式的 UML 图 12345678910public class FactoryProducer &#123; public static AbstractFactory getFactory(String choice)&#123; if(choice.equalsIgnoreCase(&quot;SHAPE&quot;))&#123; return new ShapeFactory(); &#125; else if(choice.equalsIgnoreCase(&quot;COLOR&quot;))&#123; return new ColorFactory(); &#125; return null; &#125;&#125; 单例模式介绍：单例模式（Singleton Pattern）是 Java 中最简单的设计模式之一。这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。 优点：1、在内存里只有一个实例，减少了内存的开销，尤其是频繁的创建和销毁实例（比如管理学院首页页面缓存）。2、避免对资源的多重占用（比如写文件操作）。 缺点：没有接口，不能继承，与单一职责原则冲突，一个类应该只关心内部逻辑，而不关心外面怎么样来实例化。 使用场景：1、要求生产唯一序列号。2、WEB 中的计数器，不用每次刷新都在数据库里加一次，用单例先缓存起来。3、创建的一个对象需要消耗的资源过多，比如 I/O 与数据库的连接等。 实现： 单例模式的 UML 图 懒汉式，线程不安全 1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 懒汉式，线程安全 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 饿汉式 1234567public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; 双重校验锁（DCL，即 double-checked locking）：采用双锁机制，安全且在多线程情况下能保持高性能。 1234567891011121314public class Singleton &#123; private volatile static Singleton singleton; private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; &#125; 静态内部类：能达到和双重校验锁一样的效果，但是实现更加简单 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 枚举：这种实现方式还没有被广泛采用，但这是实现单例模式的最佳方法。它更简洁，自动支持序列化机制，绝对防止多次实例化。 12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125; &#125; 建造者模式介绍：建造者模式（Builder Pattern）使用多个简单的对象一步一步构建成一个复杂的对象。一个 Builder 类会一步一步构造最终的对象。该 Builder 类是独立于其他对象的。 优点： 1、建造者独立，易扩展。 2、便于控制细节风险。 缺点： 1、产品必须有共同点，范围有限制。 2、如内部变化复杂，会有很多的建造类。 使用场景： 1、需要生成的对象具有复杂的内部结构。 2、需要生成的对象内部属性本身相互依赖。 实现： 建造者模式的 UML 图 12345678910111213141516public class MealBuilder &#123; public Meal prepareVegMeal ()&#123; Meal meal = new Meal(); meal.addItem(new VegBurger()); meal.addItem(new Coke()); return meal; &#125; public Meal prepareNonVegMeal ()&#123; Meal meal = new Meal(); meal.addItem(new ChickenBurger()); meal.addItem(new Pepsi()); return meal; &#125;&#125; 原型模式介绍：原型模式（Prototype Pattern）是用于创建重复的对象，同时又能保证性能。这种模式是实现了一个原型接口，该接口用于创建当前对象的克隆。当直接创建对象的代价比较大时，则采用这种模式。 优点： 1、性能提高。 2、逃避构造函数的约束。 缺点： 1、配备克隆方法需要对类的功能进行通盘考虑，这对于全新的类不是很难，但对于已有的类不一定很容易，特别当一个类引用不支持串行化的间接对象，或者引用含有循环结构的时候。 2、必须实现 Cloneable 接口。 使用场景： 1、资源优化场景。 2、类初始化需要消化非常多的资源，这个资源包括数据、硬件资源等。 3、性能和安全要求的场景。 4、通过 new 产生一个对象需要非常繁琐的数据准备或访问权限，则可以使用原型模式。 5、一个对象多个修改者的场景。 6、一个对象需要提供给其他对象访问，而且各个调用者可能都需要修改其值时，可以考虑使用原型模式拷贝多个对象供调用者使用。 7、在实际项目中，原型模式很少单独出现，一般是和工厂方法模式一起出现，通过 clone 的方法创建一个对象，然后由工厂方法提供给调用者。原型模式已经与 Java 融为浑然一体，大家可以随手拿来使用。 实现： 原型模式的 UML 图 1234567891011121314151617181920212223242526272829import java.util.Hashtable; public class ShapeCache &#123; private static Hashtable&lt;String, Shape&gt; shapeMap = new Hashtable&lt;String, Shape&gt;(); public static Shape getShape(String shapeId) &#123; Shape cachedShape = shapeMap.get(shapeId); return (Shape) cachedShape.clone(); &#125; // 对每种形状都运行数据库查询，并创建该形状 // shapeMap.put(shapeKey, shape); // 例如，我们要添加三种形状 public static void loadCache() &#123; Circle circle = new Circle(); circle.setId(&quot;1&quot;); shapeMap.put(circle.getId(),circle); Square square = new Square(); square.setId(&quot;2&quot;); shapeMap.put(square.getId(),square); Rectangle rectangle = new Rectangle(); rectangle.setId(&quot;3&quot;); shapeMap.put(rectangle.getId(),rectangle); &#125;&#125; 1234567891011121314151617181920212223242526272829public abstract class Shape implements Cloneable &#123; private String id; protected String type; abstract void draw(); public String getType()&#123; return type; &#125; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public Object clone() &#123; // clone 方法实现 Object clone = null; try &#123; clone = super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return clone; &#125;&#125; 适配器模式介绍：适配器模式（Adapter Pattern）是作为两个不兼容的接口之间的桥梁。这种模式涉及到一个单一的类，该类负责加入独立的或不兼容的接口功能。举个真实的例子，读卡器是作为内存卡和笔记本之间的适配器。您将内存卡插入读卡器，再将读卡器插入笔记本，这样就可以通过笔记本来读取内存卡。 优点： 1、可以让任何两个没有关联的类一起运行。 2、提高了类的复用。 3、增加了类的透明度。 4、灵活性好。 缺点： 1、过多地使用适配器，会让系统非常零乱，不易整体进行把握。比如，明明看到调用的是 A 接口，其实内部被适配成了 B 接口的实现，一个系统如果太多出现这种情况，无异于一场灾难。因此如果不是很有必要，可以不使用适配器，而是直接对系统进行重构。 2.由于 JAVA 至多继承一个类，所以至多只能适配一个适配者类，而且目标类必须是抽象类。 使用场景：有动机地修改一个正常运行的系统的接口，这时应该考虑使用适配器模式。 实现： 适配器模式的 UML 图 123456789101112131415161718192021public class MediaAdapter implements MediaPlayer &#123; AdvancedMediaPlayer advancedMusicPlayer; public MediaAdapter(String audioType)&#123; if(audioType.equalsIgnoreCase(&quot;vlc&quot;) )&#123; advancedMusicPlayer = new VlcPlayer(); &#125; else if (audioType.equalsIgnoreCase(&quot;mp4&quot;))&#123; advancedMusicPlayer = new Mp4Player(); &#125; &#125; @Override public void play(String audioType, String fileName) &#123; if(audioType.equalsIgnoreCase(&quot;vlc&quot;))&#123; advancedMusicPlayer.playVlc(fileName); &#125;else if(audioType.equalsIgnoreCase(&quot;mp4&quot;))&#123; advancedMusicPlayer.playMp4(fileName); &#125; &#125;&#125; 12345678910111213141516171819202122public class AudioPlayer implements MediaPlayer &#123; MediaAdapter mediaAdapter; @Override public void play(String audioType, String fileName) &#123; //播放 mp3 音乐文件的内置支持 if(audioType.equalsIgnoreCase(&quot;mp3&quot;))&#123; System.out.println(&quot;Playing mp3 file. Name: &quot;+ fileName); &#125; //mediaAdapter 提供了播放其他文件格式的支持 else if(audioType.equalsIgnoreCase(&quot;vlc&quot;) || audioType.equalsIgnoreCase(&quot;mp4&quot;))&#123; mediaAdapter = new MediaAdapter(audioType); mediaAdapter.play(audioType, fileName); &#125; else&#123; System.out.println(&quot;Invalid media. &quot;+ audioType + &quot; format not supported&quot;); &#125; &#125; &#125; 桥接模式介绍：桥接（Bridge）是用于把抽象化与实现化解耦，使得二者可以独立变化。这种模式涉及到一个作为桥接的接口，使得实体类的功能独立于接口实现类。这两种类型的类可被结构化改变而互不影响。 优点： 1、抽象和实现的分离。 2、优秀的扩展能力。 3、实现细节对客户透明。 缺点：桥接模式的引入会增加系统的理解与设计难度，由于聚合关联关系建立在抽象层，要求开发者针对抽象进行设计与编程。 使用场景： 1、如果一个系统需要在构件的抽象化角色和具体化角色之间增加更多的灵活性，避免在两个层次之间建立静态的继承联系，通过桥接模式可以使它们在抽象层建立一个关联关系。 2、对于那些不希望使用继承或因为多层次继承导致系统类的个数急剧增加的系统，桥接模式尤为适用。 3、一个类存在两个独立变化的维度，且这两个维度都需要进行扩展。 实现： 桥接模式的 UML 图 1234567public abstract class Shape &#123; protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI)&#123; this.drawAPI = drawAPI; &#125; public abstract void draw(); &#125; 过滤器模式介绍：过滤器模式（Filter Pattern）或标准模式（Criteria Pattern）是一种设计模式，这种模式允许开发人员使用不同的标准来过滤一组对象，通过逻辑运算以解耦的方式把它们连接起来。 实现： 过滤器模式的 UML 图 12345import java.util.List; public interface Criteria &#123; public List&lt;Person&gt; meetCriteria(List&lt;Person&gt; persons);&#125; 组合模式介绍：组合模式（Composite Pattern），又叫部分整体模式，是用于把一组相似的对象当作一个单一的对象。组合模式依据树形结构来组合对象，用来表示部分以及整体层次。这种模式创建了一个包含自己对象组的类。该类提供了修改相同对象组的方式。所谓组合模式，其实说的是对象包含对象的问题，通过组合的方式（在对象内部引用对象）来进行布局。 优点： 1、高层模块调用简单。 2、节点自由增加。 缺点：在使用组合模式时，其叶子和树枝的声明都是实现类，而不是接口，违反了依赖倒置原则。 使用场景：部分、整体场景，如树形菜单，文件、文件夹的管理。 实现： 组合模式的 UML 图 1234567891011121314151617181920212223242526272829303132333435import java.util.ArrayList;import java.util.List; public class Employee &#123; private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; //构造函数 public Employee(String name,String dept, int sal) &#123; this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList&lt;Employee&gt;(); &#125; public void add(Employee e) &#123; subordinates.add(e); &#125; public void remove(Employee e) &#123; subordinates.remove(e); &#125; public List&lt;Employee&gt; getSubordinates()&#123; return subordinates; &#125; public String toString()&#123; return (&quot;Employee :[ Name : &quot;+ name +&quot;, dept : &quot;+ dept + &quot;, salary :&quot; + salary+&quot; ]&quot;); &#125; &#125; 装饰器模式介绍：装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种模式创建了一个装饰类，用来包装原有的类，并在保持类方法签名完整性的前提下，提供了额外的功能。 优点：装饰类和被装饰类可以独立发展，不会相互耦合，装饰模式是继承的一个替代模式，装饰模式可以动态扩展一个实现类的功能。 缺点：多层装饰比较复杂。 使用场景： 1、扩展一个类的功能。 2、动态增加功能，动态撤销。 实现： 装饰器模式的 UML 图 1234567891011public abstract class ShapeDecorator implements Shape &#123; protected Shape decoratedShape; public ShapeDecorator(Shape decoratedShape)&#123; this.decoratedShape = decoratedShape; &#125; public void draw()&#123; decoratedShape.draw(); &#125; &#125; 12345678910111213141516public class RedShapeDecorator extends ShapeDecorator &#123; public RedShapeDecorator(Shape decoratedShape) &#123; super(decoratedShape); &#125; @Override public void draw() &#123; decoratedShape.draw(); setRedBorder(decoratedShape); &#125; private void setRedBorder(Shape decoratedShape)&#123; System.out.println(&quot;Border Color: Red&quot;); &#125;&#125; 外观模式介绍：外观模式（Facade Pattern）隐藏系统的复杂性，并向客户端提供了一个客户端可以访问系统的接口。这种模式涉及到一个单一的类，该类提供了客户端请求的简化方法和对现有系统类方法的委托调用。 优点： 1、减少系统相互依赖。 2、提高灵活性。 3、提高了安全性。 缺点：不符合开闭原则，如果要改东西很麻烦，继承重写都不合适。 使用场景： 1、为复杂的模块或子系统提供外界访问的模块。 2、子系统相对独立。 3、预防低水平人员带来的风险。 实现： 外观模式的 UML 图 123456789101112131415161718192021public class ShapeMaker &#123; private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() &#123; circle = new Circle(); rectangle = new Rectangle(); square = new Square(); &#125; public void drawCircle()&#123; circle.draw(); &#125; public void drawRectangle()&#123; rectangle.draw(); &#125; public void drawSquare()&#123; square.draw(); &#125;&#125; 享元模式介绍：享元模式（Flyweight Pattern）主要用于减少创建对象的数量，以减少内存占用和提高性能。享元模式尝试重用现有的同类对象，如果未找到匹配的对象，则创建新对象。 优点：大大减少对象的创建，降低系统的内存，使效率提高。 缺点：提高了系统的复杂度，需要分离出外部状态和内部状态，而且外部状态具有固有化的性质，不应该随着内部状态的变化而变化，否则会造成系统的混乱。 使用场景： 1、系统有大量相似对象。 2、需要缓冲池的场景。 实现： 享元模式的 UML 图 12345678910111213141516import java.util.HashMap; public class ShapeFactory &#123; private static final HashMap&lt;String, Shape&gt; circleMap = new HashMap&lt;&gt;(); public static Shape getCircle(String color) &#123; Circle circle = (Circle)circleMap.get(color); if(circle == null) &#123; circle = new Circle(color); circleMap.put(color, circle); System.out.println(&quot;Creating circle of color : &quot; + color); &#125; return circle; &#125;&#125; 代理模式介绍：在代理模式（Proxy Pattern）中，一个类代表另一个类的功能。这种类型的设计模式属于结构型模式。在代理模式中，我们创建具有现有对象的对象，以便向外界提供功能接口。 优点： 1、职责清晰。 2、高扩展性。 3、智能化。 缺点： 1、由于在客户端和真实主题之间增加了代理对象，因此有些类型的代理模式可能会造成请求的处理速度变慢。 2、实现代理模式需要额外的工作，有些代理模式的实现非常复杂。 使用场景：按职责来划分，通常有以下使用场景： 1、远程代理。 2、虚拟代理。 3、Copy-on-Write 代理。 4、保护（Protect or Access）代理。 5、Cache代理。 6、防火墙（Firewall）代理。 7、同步化（Synchronization）代理。 8、智能引用（Smart Reference）代理。 实现： 代理模式的 UML 图 1234567891011121314151617public class ProxyImage implements Image&#123; private RealImage realImage; private String fileName; public ProxyImage(String fileName)&#123; this.fileName = fileName; &#125; @Override public void display() &#123; if(realImage == null)&#123; realImage = new RealImage(fileName); &#125; realImage.display(); &#125;&#125; 责任链模式介绍：责任链模式（Chain of Responsibility Pattern）为请求创建了一个接收者对象的链。这种模式给予请求的类型，对请求的发送者和接收者进行解耦。在这种模式中，通常每个接收者都包含对另一个接收者的引用。如果一个对象不能处理该请求，那么它会把相同的请求传给下一个接收者，依此类推。 优点： 1、降低耦合度。它将请求的发送者和接收者解耦。 2、简化了对象。使得对象不需要知道链的结构。 3、增强给对象指派职责的灵活性。通过改变链内的成员或者调动它们的次序，允许动态地新增或者删除责任。 4、增加新的请求处理类很方便。 缺点： 1、不能保证请求一定被接收。 2、系统性能将受到一定影响，而且在进行代码调试时不太方便，可能会造成循环调用。 3、可能不容易观察运行时的特征，有碍于除错。 使用场景： 1、有多个对象可以处理同一个请求，具体哪个对象处理该请求由运行时刻自动确定。 2、在不明确指定接收者的情况下，向多个对象中的一个提交一个请求。 3、可动态指定一组对象处理请求。 实现： 责任链模式的 UML 图 1234567891011121314151617181920212223242526public abstract class AbstractLogger &#123; public static int INFO = 1; public static int DEBUG = 2; public static int ERROR = 3; protected int level; //责任链中的下一个元素 protected AbstractLogger nextLogger; public void setNextLogger(AbstractLogger nextLogger)&#123; this.nextLogger = nextLogger; &#125; public void logMessage(int level, String message)&#123; if(this.level &lt;= level)&#123; write(message); &#125; if(nextLogger !=null)&#123; nextLogger.logMessage(level, message); // 向下层传播 &#125; &#125; abstract protected void write(String message); &#125; 命令模式介绍：命令模式（Command Pattern）是一种数据驱动的设计模式，它属于行为型模式。请求以命令的形式包裹在对象中，并传给调用对象。调用对象寻找可以处理该命令的合适的对象，并把该命令传给相应的对象，该对象执行命令。 优点： 1、降低了系统耦合度。 2、新的命令可以很容易添加到系统中去。 缺点：使用命令模式可能会导致某些系统有过多的具体命令类。 使用场景：认为是命令的地方都可以使用命令模式，比如： 1、GUI 中每一个按钮都是一条命令。 2、模拟 CMD。 实现： img 1234567891011121314151617import java.util.ArrayList;import java.util.List; public class Broker &#123; private List&lt;Order&gt; orderList = new ArrayList&lt;Order&gt;(); public void takeOrder(Order order)&#123; orderList.add(order); &#125; public void placeOrders()&#123; for (Order order : orderList) &#123; order.execute(); &#125; orderList.clear(); &#125;&#125; 解释器模式介绍：解释器模式（Interpreter Pattern）提供了评估语言的语法或表达式的方式，它属于行为型模式。这种模式实现了一个表达式接口，该接口解释一个特定的上下文。这种模式被用在 SQL 解析、符号处理引擎等。 优点： 1、可扩展性比较好，灵活。 2、增加了新的解释表达式的方式。 3、易于实现简单文法。 缺点： 1、可利用场景比较少。 2、对于复杂的文法比较难维护。 3、解释器模式会引起类膨胀。 4、解释器模式采用递归调用方法。 使用场景： 1、可以将一个需要解释执行的语言中的句子表示为一个抽象语法树。 2、一些重复出现的问题可以用一种简单的语言来进行表达。 3、一个简单语法需要解释的场景。 实现： 解释器模式的 UML 图 123456789101112131415public class OrExpression implements Expression &#123; private Expression expr1 = null; private Expression expr2 = null; public OrExpression(Expression expr1, Expression expr2) &#123; this.expr1 = expr1; this.expr2 = expr2; &#125; @Override public boolean interpret(String context) &#123; return expr1.interpret(context) || expr2.interpret(context); &#125;&#125; 迭代器模式介绍：迭代器模式（Iterator Pattern）是 Java 和 .Net 编程环境中非常常用的设计模式。这种模式用于顺序访问集合对象的元素，不需要知道集合对象的底层表示。 优点： 1、它支持以不同的方式遍历一个聚合对象。 2、迭代器简化了聚合类。 3、在同一个聚合上可以有多个遍历。 4、在迭代器模式中，增加新的聚合类和迭代器类都很方便，无须修改原有代码。 缺点：由于迭代器模式将存储数据和遍历数据的职责分离，增加新的聚合类需要对应增加新的迭代器类，类的个数成对增加，这在一定程度上增加了系统的复杂性。 使用场景： 1、访问一个聚合对象的内容而无须暴露它的内部表示。 2、需要为聚合对象提供多种遍历方式。 3、为遍历不同的聚合结构提供一个统一的接口。 实现： 迭代器模式的 UML 图 1234567891011121314151617181920212223242526272829public class NameRepository implements Container &#123; public String names[] = &#123;&quot;Robert&quot; , &quot;John&quot; ,&quot;Julie&quot; , &quot;Lora&quot;&#125;; @Override public Iterator getIterator() &#123; return new NameIterator(); &#125; private class NameIterator implements Iterator &#123; int index; @Override public boolean hasNext() &#123; if(index &lt; names.length)&#123; return true; &#125; return false; &#125; @Override public Object next() &#123; if(this.hasNext())&#123; return names[index++]; &#125; return null; &#125; &#125;&#125; 中介者模式介绍：中介者模式（Mediator Pattern）是用来降低多个对象和类之间的通信复杂性。这种模式提供了一个中介类，该类通常处理不同类之间的通信，并支持松耦合，使代码易于维护。 优点： 1、降低了类的复杂度，将一对多转化成了一对一。 2、各个类之间的解耦。 3、符合迪米特原则。 缺点：中介者会庞大，变得复杂难以维护。 使用场景： 1、系统中对象之间存在比较复杂的引用关系，导致它们之间的依赖关系结构混乱而且难以复用该对象。 2、想通过一个中间类来封装多个类中的行为，而又不想生成太多的子类。 实现： 中介者模式的 UML 图 12345678import java.util.Date; public class ChatRoom &#123; public static void showMessage(User user, String message)&#123; System.out.println(new Date().toString() + &quot; [&quot; + user.getName() +&quot;] : &quot; + message); &#125;&#125; 备忘录模式介绍：备忘录模式（Memento Pattern）保存一个对象的某个状态，以便在适当的时候恢复对象。 优点： 1、给用户提供了一种可以恢复状态的机制，可以使用户能够比较方便地回到某个历史的状态。 2、实现了信息的封装，使得用户不需要关心状态的保存细节。 缺点：消耗资源。如果类的成员变量过多，势必会占用比较大的资源，而且每一次保存都会消耗一定的内存。 使用场景： 1、需要保存/恢复数据的相关状态场景。 2、提供一个可回滚的操作。 实现： 备忘录模式的 UML 图 1234567891011121314import java.util.ArrayList;import java.util.List; public class CareTaker &#123; private List&lt;Memento&gt; mementoList = new ArrayList&lt;Memento&gt;(); public void add(Memento state)&#123; mementoList.add(state); &#125; public Memento get(int index)&#123; return mementoList.get(index); &#125;&#125; 观察者模式介绍：当对象间存在一对多关系时，则使用观察者模式（Observer Pattern）。比如，当一个对象被修改时，则会自动通知依赖它的对象。 优点： 1、观察者和被观察者是抽象耦合的。 2、建立一套触发机制。 缺点： 1、如果一个被观察者对象有很多的直接和间接的观察者的话，将所有的观察者都通知到会花费很多时间。 2、如果在观察者和观察目标之间有循环依赖的话，观察目标会触发它们之间进行循环调用，可能导致系统崩溃。 3、观察者模式没有相应的机制让观察者知道所观察的目标对象是怎么发生变化的，而仅仅只是知道观察目标发生了变化。 使用场景： 一个抽象模型有两个方面，其中一个方面依赖于另一个方面。将这些方面封装在独立的对象中使它们可以各自独立地改变和复用。 一个对象的改变将导致其他一个或多个对象也发生改变，而不知道具体有多少对象将发生改变，可以降低对象之间的耦合度。 一个对象必须通知其他对象，而并不知道这些对象是谁。 需要在系统中创建一个触发链，A对象的行为将影响B对象，B对象的行为将影响C对象……，可以使用观察者模式创建一种链式触发机制。 实现： 观察者模式的 UML 图 12345678910111213141516171819202122232425262728import java.util.ArrayList;import java.util.List; public class Subject &#123; private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); private int state; public int getState() &#123; return state; &#125; public void setState(int state) &#123; this.state = state; notifyAllObservers(); &#125; public void attach(Observer observer)&#123; observers.add(observer); &#125; public void notifyAllObservers()&#123; for (Observer observer : observers) &#123; observer.update(); &#125; &#125; &#125; 状态模式介绍：在状态模式（State Pattern）中，类的行为是基于它的状态改变的。在状态模式中，我们创建表示各种状态的对象和一个行为随着状态对象改变而改变的 context 对象。 优点： 1、封装了转换规则。 2、枚举可能的状态，在枚举状态之前需要确定状态种类。 3、将所有与某个状态有关的行为放到一个类中，并且可以方便地增加新的状态，只需要改变对象状态即可改变对象的行为。 4、允许状态转换逻辑与状态对象合成一体，而不是某一个巨大的条件语句块。 5、可以让多个环境对象共享一个状态对象，从而减少系统中对象的个数。 缺点： 1、状态模式的使用必然会增加系统类和对象的个数。 2、状态模式的结构与实现都较为复杂，如果使用不当将导致程序结构和代码的混乱。 3、状态模式对”开闭原则”的支持并不太好，对于可以切换状态的状态模式，增加新的状态类需要修改那些负责状态转换的源代码，否则无法切换到新增状态，而且修改某个状态类的行为也需修改对应类的源代码。 使用场景： 1、行为随状态改变而改变的场景。 2、条件、分支语句的代替者。 实现： 状态模式的 UML 图 1234567891011public class StopState implements State &#123; public void doAction(Context context) &#123; System.out.println(&quot;Player is in stop state&quot;); context.setState(this); &#125; public String toString()&#123; return &quot;Stop State&quot;; &#125;&#125; 空对象模式介绍：在空对象模式（Null Object Pattern）中，一个空对象取代 NULL 对象实例的检查。Null 对象不是检查空值，而是反应一个不做任何动作的关系。这样的 Null 对象也可以在数据不可用的时候提供默认的行为。在空对象模式中，我们创建一个指定各种要执行的操作的抽象类和扩展该类的实体类，还创建一个未对该类做任何实现的空对象类，该空对象类将无缝地使用在需要检查空值的地方。 实现： 空对象模式的 UML 图 123456789101112public class NullCustomer extends AbstractCustomer &#123; @Override public String getName() &#123; return &quot;Not Available in Customer Database&quot;; &#125; @Override public boolean isNil() &#123; return true; &#125;&#125; 策略模式介绍：在策略模式（Strategy Pattern）中，一个类的行为或其算法可以在运行时更改。在策略模式中，我们创建表示各种策略的对象和一个行为随着策略对象改变而改变的 context 对象。策略对象改变 context 对象的执行算法。 优点： 1、算法可以自由切换。 2、避免使用多重条件判断。 3、扩展性良好。 缺点： 1、策略类会增多。 2、所有策略类都需要对外暴露。 使用场景： 1、如果在一个系统里面有许多类，它们之间的区别仅在于它们的行为，那么使用策略模式可以动态地让一个对象在许多行为中选择一种行为。 2、一个系统需要动态地在几种算法中选择一种。 3、如果一个对象有很多的行为，如果不用恰当的模式，这些行为就只好使用多重的条件选择语句来实现。 实现： 策略模式的 UML 图 1234567891011public class Context &#123; private Strategy strategy; public Context(Strategy strategy)&#123; this.strategy = strategy; &#125; public int executeStrategy(int num1, int num2)&#123; return strategy.doOperation(num1, num2); &#125;&#125; 模板模式介绍：在模板模式（Template Pattern）中，一个抽象类公开定义了执行它的方法的方式/模板。它的子类可以按需要重写方法实现，但调用将以抽象类中定义的方式进行。 优点： 1、封装不变部分，扩展可变部分。 2、提取公共代码，便于维护。 3、行为由父类控制，子类实现。 缺点：每一个不同的实现都需要一个子类来实现，导致类的个数增加，使得系统更加庞大。 使用场景： 1、有多个子类共有的方法，且逻辑相同。 2、重要的、复杂的方法，可以考虑作为模板方法。 实现： 模板模式的 UML 图 123456789101112131415161718public abstract class Game &#123; abstract void initialize(); abstract void startPlay(); abstract void endPlay(); //模板 public final void play()&#123; //初始化游戏 initialize(); //开始游戏 startPlay(); //结束游戏 endPlay(); &#125;&#125; 访问者模式介绍：在访问者模式（Visitor Pattern）中，我们使用了一个访问者类，它改变了元素类的执行算法。通过这种方式，元素的执行算法可以随着访问者改变而改变。这种类型的设计模式属于行为型模式。根据模式，元素对象已接受访问者对象，这样访问者对象就可以处理元素对象上的操作。 优点： 1、符合单一职责原则。 2、优秀的扩展性。 3、灵活性。 缺点： 1、具体元素对访问者公布细节，违反了迪米特原则。 2、具体元素变更比较困难。 3、违反了依赖倒置原则，依赖了具体类，没有依赖抽象。 使用场景： 1、对象结构中对象对应的类很少改变，但经常需要在此对象结构上定义新的操作。 2、需要对一个对象结构中的对象进行很多不同的并且不相关的操作，而需要避免让这些操作”污染”这些对象的类，也不希望在增加新操作时修改这些类。 实现： 访问者模式的 UML 图 1234567public class Mouse implements ComputerPart &#123; @Override public void accept(ComputerPartVisitor computerPartVisitor) &#123; computerPartVisitor.visit(this); &#125;&#125; MVC模式介绍：MVC 模式代表 Model-View-Controller（模型-视图-控制器） 模式。这种模式用于应用程序的分层开发。 Model（模型） - 模型代表一个存取数据的对象或 JAVA POJO。它也可以带有逻辑，在数据变化时更新控制器。 View（视图） - 视图代表模型包含的数据的可视化。 Controller（控制器） - 控制器作用于模型和视图上。它控制数据流向模型对象，并在数据变化时更新视图。它使视图与模型分离开。 实现： MVC 模式的 UML 图 1234567891011121314151617181920212223242526272829public class StudentController &#123; private Student model; private StudentView view; public StudentController(Student model, StudentView view)&#123; this.model = model; this.view = view; &#125; public void setStudentName(String name)&#123; model.setName(name); &#125; public String getStudentName()&#123; return model.getName(); &#125; public void setStudentRollNo(String rollNo)&#123; model.setRollNo(rollNo); &#125; public String getStudentRollNo()&#123; return model.getRollNo(); &#125; public void updateView()&#123; view.printStudentDetails(model.getName(), model.getRollNo()); &#125; &#125; 业务代表模式介绍：业务代表模式（Business Delegate Pattern）用于对表示层和业务层解耦。它基本上是用来减少通信或对表示层代码中的业务层代码的远程查询功能。在业务层中我们有以下实体。 客户端（Client） - 表示层代码可以是 JSP、servlet 或 UI java 代码。 业务代表（Business Delegate） - 一个为客户端实体提供的入口类，它提供了对业务服务方法的访问。 查询服务（LookUp Service） - 查找服务对象负责获取相关的业务实现，并提供业务对象对业务代表对象的访问。 业务服务（Business Service） - 业务服务接口。实现了该业务服务的实体类，提供了实际的业务实现逻辑。 实现： 业务代表模式的 UML 图 1234567891011121314public class BusinessDelegate &#123; private BusinessLookUp lookupService = new BusinessLookUp(); private BusinessService businessService; private String serviceType; public void setServiceType(String serviceType)&#123; this.serviceType = serviceType; &#125; public void doTask()&#123; businessService = lookupService.getBusinessService(serviceType); businessService.doProcessing(); &#125;&#125; 组合实体模式介绍：组合实体模式（Composite Entity Pattern）用在 EJB 持久化机制中。一个组合实体是一个 EJB 实体 bean，代表了对象的图解。当更新一个组合实体时，内部依赖对象 beans 会自动更新，因为它们是由 EJB 实体 bean 管理的。以下是组合实体 bean 的参与者。 组合实体（Composite Entity） - 它是主要的实体 bean。它可以是粗粒的，或者可以包含一个粗粒度对象，用于持续生命周期。 粗粒度对象（Coarse-Grained Object） - 该对象包含依赖对象。它有自己的生命周期，也能管理依赖对象的生命周期。 依赖对象（Dependent Object） - 依赖对象是一个持续生命周期依赖于粗粒度对象的对象。 策略（Strategies） - 策略表示如何实现组合实体。 实现： 组合实体模式的 UML 图 123456789101112public class CompositeEntity &#123; private CoarseGrainedObject cgo = new CoarseGrainedObject(); public void setData(String data1, String data2)&#123; cgo.setData(data1, data2); &#125; public String[] getData()&#123; return cgo.getData(); &#125;&#125; 数据访问对象模式介绍：数据访问对象模式（Data Access Object Pattern）或 DAO 模式用于把低级的数据访问 API 或操作从高级的业务服务中分离出来。以下是数据访问对象模式的参与者。 数据访问对象接口（Data Access Object Interface） - 该接口定义了在一个模型对象上要执行的标准操作。 数据访问对象实体类（Data Access Object concrete class） - 该类实现了上述的接口。该类负责从数据源获取数据，数据源可以是数据库，也可以是 xml，或者是其他的存储机制。 模型对象/数值对象（Model Object/Value Object） - 该对象是简单的 POJO，包含了 get/set 方法来存储通过使用 DAO 类检索到的数据。 实现： 数据访问对象模式的 UML 图 12345678910111213141516171819202122232425262728293031323334353637383940import java.util.ArrayList;import java.util.List; public class StudentDaoImpl implements StudentDao &#123; //列表是当作一个数据库 List&lt;Student&gt; students; public StudentDaoImpl()&#123; students = new ArrayList&lt;Student&gt;(); Student student1 = new Student(&quot;Robert&quot;,0); Student student2 = new Student(&quot;John&quot;,1); students.add(student1); students.add(student2); &#125; @Override public void deleteStudent(Student student) &#123; students.remove(student.getRollNo()); System.out.println(&quot;Student: Roll No &quot; + student.getRollNo() +&quot;, deleted from database&quot;); &#125; //从数据库中检索学生名单 @Override public List&lt;Student&gt; getAllStudents() &#123; return students; &#125; @Override public Student getStudent(int rollNo) &#123; return students.get(rollNo); &#125; @Override public void updateStudent(Student student) &#123; students.get(student.getRollNo()).setName(student.getName()); System.out.println(&quot;Student: Roll No &quot; + student.getRollNo() +&quot;, updated in the database&quot;); &#125;&#125; 前端控制器模式介绍：前端控制器模式（Front Controller Pattern）是用来提供一个集中的请求处理机制，所有的请求都将由一个单一的处理程序处理。该处理程序可以做认证/授权/记录日志，或者跟踪请求，然后把请求传给相应的处理程序。以下是这种设计模式的实体。 前端控制器（Front Controller） - 处理应用程序所有类型请求的单个处理程序，应用程序可以是基于 web 的应用程序，也可以是基于桌面的应用程序。 调度器（Dispatcher） - 前端控制器可能使用一个调度器对象来调度请求到相应的具体处理程序。 视图（View） - 视图是为请求而创建的对象。 实现： 前端控制器模式的 UML 图 1234567891011121314151617181920212223242526public class FrontController &#123; private Dispatcher dispatcher; public FrontController()&#123; dispatcher = new Dispatcher(); &#125; private boolean isAuthenticUser()&#123; System.out.println(&quot;User is authenticated successfully.&quot;); return true; &#125; private void trackRequest(String request)&#123; System.out.println(&quot;Page requested: &quot; + request); &#125; public void dispatchRequest(String request)&#123; //记录每一个请求 trackRequest(request); //对用户进行身份验证 if(isAuthenticUser())&#123; dispatcher.dispatch(request); &#125; &#125; 拦截过滤器模式介绍：拦截过滤器模式（Intercepting Filter Pattern）用于对应用程序的请求或响应做一些预处理/后处理。定义过滤器，并在把请求传给实际目标应用程序之前应用在请求上。过滤器可以做认证/授权/记录日志，或者跟踪请求，然后把请求传给相应的处理程序。以下是这种设计模式的实体。 过滤器（Filter） - 过滤器在请求处理程序执行请求之前或之后，执行某些任务。 过滤器链（Filter Chain） - 过滤器链带有多个过滤器，并在 Target 上按照定义的顺序执行这些过滤器。 Target - Target 对象是请求处理程序。 过滤管理器（Filter Manager） - 过滤管理器管理过滤器和过滤器链。 客户端（Client） - Client 是向 Target 对象发送请求的对象。 实现： 拦截过滤器模式的 UML 图 123456789101112131415public class FilterManager &#123; FilterChain filterChain; public FilterManager(Target target)&#123; filterChain = new FilterChain(); filterChain.setTarget(target); &#125; public void setFilter(Filter filter)&#123; filterChain.addFilter(filter); &#125; public void filterRequest(String request)&#123; filterChain.execute(request); &#125;&#125; 服务定位器模式介绍：服务定位器模式（Service Locator Pattern）用在我们想使用 JNDI 查询定位各种服务的时候。考虑到为某个服务查找 JNDI 的代价很高，服务定位器模式充分利用了缓存技术。在首次请求某个服务时，服务定位器在 JNDI 中查找服务，并缓存该服务对象。当再次请求相同的服务时，服务定位器会在它的缓存中查找，这样可以在很大程度上提高应用程序的性能。以下是这种设计模式的实体。 服务（Service） - 实际处理请求的服务。对这种服务的引用可以在 JNDI 服务器中查找到。 Context / 初始的 Context - JNDI Context 带有对要查找的服务的引用。 服务定位器（Service Locator） - 服务定位器是通过 JNDI 查找和缓存服务来获取服务的单点接触。 缓存（Cache） - 缓存存储服务的引用，以便复用它们。 客户端（Client） - Client 是通过 ServiceLocator 调用服务的对象。 实现： 服务定位器模式的 UML 图 123456789101112131415161718192021222324252627282930313233import java.util.ArrayList;import java.util.List; public class Cache &#123; private List&lt;Service&gt; services; public Cache()&#123; services = new ArrayList&lt;Service&gt;(); &#125; public Service getService(String serviceName)&#123; for (Service service : services) &#123; if(service.getName().equalsIgnoreCase(serviceName))&#123; System.out.println(&quot;Returning cached &quot;+serviceName+&quot; object&quot;); return service; &#125; &#125; return null; &#125; public void addService(Service newService)&#123; boolean exists = false; for (Service service : services) &#123; if(service.getName().equalsIgnoreCase(newService.getName()))&#123; exists = true; &#125; &#125; if(!exists)&#123; services.add(newService); &#125; &#125;&#125; 123456789101112131415161718192021public class ServiceLocator &#123; private static Cache cache; static &#123; cache = new Cache(); &#125; public static Service getService(String jndiName)&#123; Service service = cache.getService(jndiName); if(service != null)&#123; return service; &#125; InitialContext context = new InitialContext(); Service service1 = (Service)context.lookup(jndiName); cache.addService(service1); return service1; &#125;&#125; 传输对象模式介绍：传输对象模式（Transfer Object Pattern）用于从客户端向服务器一次性传递带有多个属性的数据。传输对象也被称为数值对象。传输对象是一个具有 getter/setter 方法的简单的 POJO 类，它是可序列化的，所以它可以通过网络传输。它没有任何的行为。服务器端的业务类通常从数据库读取数据，然后填充 POJO，并把它发送到客户端或按值传递它。对于客户端，传输对象是只读的。客户端可以创建自己的传输对象，并把它传递给服务器，以便一次性更新数据库中的数值。以下是这种设计模式的实体。 业务对象（Business Object） - 为传输对象填充数据的业务服务。 传输对象（Transfer Object） - 简单的 POJO，只有设置/获取属性的方法。 客户端（Client） - 客户端可以发送请求或者发送传输对象到业务对象。 实现： 传输对象模式的 UML 图 123456789101112131415161718192021222324252627282930313233343536import java.util.ArrayList;import java.util.List; public class StudentBO &#123; //列表是当作一个数据库 List&lt;StudentVO&gt; students; public StudentBO()&#123; students = new ArrayList&lt;StudentVO&gt;(); StudentVO student1 = new StudentVO(&quot;Robert&quot;,0); StudentVO student2 = new StudentVO(&quot;John&quot;,1); students.add(student1); students.add(student2); &#125; public void deleteStudent(StudentVO student) &#123; students.remove(student.getRollNo()); System.out.println(&quot;Student: Roll No &quot; + student.getRollNo() +&quot;, deleted from database&quot;); &#125; //从数据库中检索学生名单 public List&lt;StudentVO&gt; getAllStudents() &#123; return students; &#125; public StudentVO getStudent(int rollNo) &#123; return students.get(rollNo); &#125; public void updateStudent(StudentVO student) &#123; students.get(student.getRollNo()).setName(student.getName()); System.out.println(&quot;Student: Roll No &quot; + student.getRollNo() +&quot;, updated in the database&quot;); &#125;&#125;","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.zsstrike.top/tags/Java/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.zsstrike.top/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"OnJava8笔记","slug":"OnJava8笔记","date":"2020-10-25T15:15:42.000Z","updated":"2020-12-17T10:23:43.510Z","comments":true,"path":"2020/10/25/OnJava8笔记/","link":"","permalink":"http://blog.zsstrike.top/2020/10/25/OnJava8%E7%AC%94%E8%AE%B0/","excerpt":"本文主要整理了 OnJava8 的阅读笔记。","text":"本文主要整理了 OnJava8 的阅读笔记。 第三章 万物皆对象对象操纵：在 Java 中程序员实际操作的是对象的引用，方法参数中传递的也只是对象的引用。 对象创建：new。 数据存储： 寄存器：Java 中不存在该方式。 栈内存：存放一些 Java 数据，比如对象的引用。 堆内存：Java 对象都存于其中。 常量内存：程序代码中，不会改变。 非 RAM 存储：序列化对象（用于传送）和持久化对象（用于恢复）。 基本类型的存储：不是通过 new 创建，变量直接存储值。有 boolean，byte，short，char，int，float，long，double，void。boolean 类型的大小没有明确规定。 高精度数值：BigInteger 和 BigDecimal。 数组的存储：当创建对象数组的时候，实际上是对象引用的数组，初始化为 null。 代码注释：/* ... */ 和 //。 对象清理： 作用域：&#123;&#125; 决定，不允许父作用域和子作用域声明相同的变量。 对象作用域：使用 new 关键字创建的 Java 对象生命周期超出作用域。 类的创建： 类型：class 字段：类里面声明的变量 方法：类里面定义的函数 基本类型的默认值：全 0，但是不适用与局部变量。 方法签名：方法名和参数列表统称为方法签名。 程序编写： 命名可见性：反向使用自己的网络域名，但是存在空文件夹 使用其他组件：import static 关键字：类变量和类方法声明，在没有对象时候也可以进行调用，另外类变量在所有的对象中共享 第四章 运算符赋值：=，基本类型的赋值都是直接的，而不像对象，赋予的只是其内存的引用。在方法的参数中传递一个对象，在方法体里面对其进行修改，那么在该对象在外部也会被修改。 算术运算符：+，-，*，/，%，其中+, -可以作为一元运算符。 递增和递减：++,--。前缀递增和递减立即修改变量的值，后缀则是使用变量的值，然后再修改。 关系运算符：&gt;, &gt;=, &lt;, &lt;=, ==, !=。判断基本对象的时候使用==，判断对象的使用 equals 方法，判断对象时使用 == 比较的只是引用。 逻辑运算符：&amp;&amp;, OR, !。Java 支持短路。 字面量常量：0x, 0, 0b, L, F, L, F 可以默认不写。 下划线：用于分割数字字面量。 指数计数法：e。 位运算符：&amp;, |, ^, ~。 移位运算符：&lt;&lt;, &gt;&gt;, &gt;&gt;&gt;。注意 &gt;&gt; 是算术右移，&gt;&gt;&gt; 是逻辑右移（首位添0）。 三目运算符：&lt;boolean condition&gt; ? &lt;value1&gt; : &lt;value2&gt;。 字符串运算符：+。 类型转换：向上转换是安全的，向下转换需要显式说明(type)。对于浮点数向整数转换，小数总是被截断。 Java 没有 sizeof 运算符，因为每种类型的值都是固定的。 第五章 控制流true 和 false：所有关系运算符都能产生条件语句，注意在 Java 中使用数值作为布尔值是非法的。 条件控制：if-else。 迭代语句：while，do-while，for，for-in。 return：退出当前的方法，放回一个方法值。 break 和 continue：break 用于中止内层循环，continue 用于跳过此次迭代。 goto：Java 不支持 goto 语句，但是支持标签语法，可以和 break 和 continue 一起使用。 switch-case：每个 case 后面跟上 break，同时在 Java7 的时候开始支持字符串匹配。 第六章 初始化和清理使用构造器保证初始化：构造器名称和类名相同，每次创建一个对象的时候，自动调用构造器进行初始化。构造器并没有返回值。 方法重载：每个被重载的方法需要具有一个独一无二的参数列表。返回值并不能用来区分重载的方法。 无参构造器：一个无参构造器就是不接受参数的构造器，如果没有显式提供任何构造器，那么编译器会自动提供一个无参构造器。 this：this 关键字只能在非静态的方法内部使用，等同于当前方法所属的对象引用。 在构造器中调用构造器：通过 this(param list) 实现。注意只能通过 this 调用一次构造器，不可重复多次调用构造器。并且，只能在构造器首行进行调用。 static 的含义：static 修饰的方法中不存在 this。静态方法和静态变量是为了类而创建的。 垃圾回收器：在 Java 中，对象并非总是被垃圾回收： 对象可能不被垃圾回收。 垃圾回收不等同于析构。 垃圾回收只和内存有关。 在 Java 中，虽然提供了一个 finialize() 的方法用于清理对象，但是事实上我们并不需要过多使用该方法。记住，无论是”垃圾回收”还是”终结”，都不保证一定会发生。如果 Java 虚拟机（JVM）并未面临内存耗尽的情形，它可能不会浪费时间执行垃圾回收以恢复内存。 垃圾回收器如何工作： 引用计数：每个对象有一个引用计数器，每次有引用指向该对象的时候，引用计数加 1.当引用离开作用域或者被置为 null 的时候，引用计数减一。垃圾回收器会遍历含有全部对象的列表，当发现某个对象的引用计数为 0 时，就释放其占用的空间（但是，引用计数模式经常会在计数为 0 时立即释放对象）。这个机制存在一个缺点：如果对象之间存在循环引用，那么它们的引用计数都不为 0，就会出现应该被回收但无法被回收的情况。 自适应的垃圾回收技术：对于任意“活”的对象，总是可以追溯到其存活在栈或者静态区的引用，从栈或者静态存储区出发，将会发现所有的活的对象。至于如何处理找到的存活对象，取决于不同的 Java 虚拟机实现。其中有一种做法叫做停止-复制（stop-and-copy）。顾名思义，这需要先暂停程序的运行（不属于后台回收模式），然后将所有存活的对象从当前堆复制到另一个堆，没有复制的就是需要被垃圾回收的。另外，当对象被复制到新堆时，它们是一个挨着一个紧凑排列，然后就可以按照前面描述的那样简单、直接地分配新空间了。上述方法存在缺点：需要两个堆，然后再两个堆之间折腾，得维护比实际空间多一倍的空间；另外在于复制本身，一旦程序进入稳定状态之后，可能只会产生少量垃圾，甚至没有垃圾。尽管如此，复制回收器仍然会将所有内存从一处复制到另一处，这很浪费。为了避免这种状况，一些 Java 虚拟机会进行检查：要是没有新垃圾产生，就会转换到另一种模式（即”自适应”）。这种模式称为标记-清扫（mark-and-sweep）。对一般用途而言，”标记-清扫”方式速度相当慢，但是当你知道程序只会产生少量垃圾甚至不产生垃圾时，它的速度就很快了。”标记-清扫”所依据的思路仍然是从栈和静态存储区出发，遍历所有的引用，找出所有存活的对象。但是，每当找到一个存活对象，就给对象设一个标记，并不回收它。只有当标记过程完成后，清理动作才开始。在清理过程中，没有标记的对象将被释放，不会发生任何复制动作。”标记-清扫”后剩下的堆空间是不连续的，垃圾回收器要是希望得到连续空间的话，就需要重新整理剩下的对象。 成员初始化：在方法中的变量没有默认值，需要手动指定一个值之后才能使用；在类中的变量则会赋予默认值。 初始化的顺序：假设有个名为 Dog 的类： 即使没有显式地使用 static 关键字，构造器实际上也是静态方法。所以，当首次创建 Dog 类型的对象或是首次访问 Dog 类的静态方法或属性时，Java 解释器必须在类路径中查找，以定位 Dog.class。 当加载完 Dog.class 后（后面会学到，这将创建一个 Class 对象），有关静态初始化的所有动作都会执行。因此，静态初始化只会在首次加载 Class 对象时初始化一次。 当用 new Dog() 创建对象时，首先会在堆上为 Dog 对象分配足够的存储空间。 分配的存储空间首先会被清零，即会将 Dog 对象中的所有基本类型数据设置为默认值（数字会被置为 0，布尔型和字符型也相同），引用被置为 null。 执行所有出现在字段定义处的初始化动作。 执行构造器。你将会在”复用”这一章看到，这可能会牵涉到很多动作，尤其当涉及继承的时候。 显式的静态初始化：static &#123; statements； &#125;，与其他静态初始化动作一样，这段代码仅执行一次：当首次创建这个类的对象或首次访问这个类的静态成员（甚至不需要创建该类的对象）时。 实例初始化：&#123; statements; &#125;，实例初始化子句是在两个构造器之前执行的。 数组初始化：Type[] arg = new Type[length]，Type[] arg = &#123;value1, value2,,,&#125; 可变参数列表：void method(int t, char... args) 枚举类型：enum Type &#123;&#125; 第七章 封装包的概念：包内包含有一组类，它们被组织在一个单独的命名空间下。对于单文件的程序，该文件在默认包（default package）下。另外，每个 Java 源文件只能有一个 public 类。 代码组织：为了将功能相近的 Java 源文件组织到一起，可以使用关键字 package。该关键字必须是文件中除了注释的第一行代码。当需要使用到某个包中的类时，可以使用 import 关键字。 独一无二的包名：通常选择反转的域名。 冲突：当两个包下面含有相同的类时，就会出现名称冲突的问题，可以将特定的类写全名称，比如java.util.ArrayList。 使用包的注意事项：当创建一个包的时候，包名实际上就隐含了目录结构。 访问权限修饰符：Java 访问权限控制符 public，protected，private 位于定义的类名，属性名和方法名前。 public：当使用 public 关键字的时候，意味着 public 后声明的成员对于每个人都是可用的。 默认包：指不加修饰符定义的成员，可以被相同包下的文件访问。 private：除了包含成员的类，其他任何类都无法访问这个成员。 protected：继承的类可以访问父类中对应的成员，同时也提供了包访问权限。 类访问权限：类既不能是 private，也不能是 protected 的，只能使用 public 或者 是包访问权限。 第八章 复用复用方式： 组合：在新类里面创建现有类的对象 继承：创建现有类型的子类 委托：介于继承和组合之间，将一个成员对象放在正在构建的类中，但同时又在新的类中公开来自成员对象的所有方法（Java 中不直接支持） 组合语法：将对象的引用放在一个新的类里面，就算是使用了组合。 继承语法：使用 extends 关键字。继承后，可以在方法里面使用 super 关键字来使用父类的方法。 子类的初始化：当某个派生类被实例化的时候，会递归向上调用父类的构造器，最高层级的父类的构造器首先被执行，然后是最高层级下的子类，，，一直到该派生类构造器。 带参数的构造器：当没有有参数的基类构造器，只含有有参数的基类构造器，此时就需要通过 super 手动调用基类的构造器。 组合和继承的选择：当想要在新类中包含一个已有类的功能时，使用组合，而非继承；当使用继承时，使用一个现有类并开发出它的新版本，通常这意味着使用一个通用类，并为了某个特殊需求将其特殊化。组合用来表达“有一个”的关系，而继承则是“是一个”关系。 向上转型：派生类到基类的转型称之为向上转型，向上转型是安全的，因为子类必定包含了所有父类的方法。 final 关键字：final 修饰的数据通常指该数据不能被改变： final 数据：对于基本类型，final 使得数值恒定不变，对于对象引用，final 则是使得引用恒定不变。空白 final 是指没有初始化值的 final 属性，编译器保证在使用空白 final 之前必须被初始化，此时必须在构造器中对 final 变量进行赋值。 final 参数：在参数列表中，将参数声明为 final 意味着在方法中不能改变参数指向的对象或基本变量。 final 方法：给方法上锁，防止子类通过覆写改变方法的行为。类中所有的 private 方法都隐式地指定为 final。 final 类：当说一个类是 final ，就意味着它不能被继承。 类初始化和加载：在 Java 中，每个类的编译代码都存在于它自己独立的文件中，该文件只有在使用程序代码时才会被加载。一般可以说“类的代码在首次使用时加载”。这通常是指创建类的第一个对象，或者是访问了类的 static 属性或方法。构造器也是一个 static 方法尽管它的 static 关键字是隐式的。因此，准确地说，一个类当它任意一个 static 成员被访问时，就会被加载。 第九章 多态向上转型：当使用向上转型的时候，我们可以讲所有派生类当做是基类来看待，提高了程序的可拓展性。 方法调用绑定：当派生类重写了基类的方法时，我们使用向上转型后，调用这些被重写的方法时，编译器会动态绑定到派生类中被重写的方法，执行方法调用。Java 中除了 static 和 final 方法（private 方法也是隐式的 final）外，其他所有方法都是后期绑定。 陷阱： 试图重写私有方法 只有普通的方法调用是多态的，属性并不能多态 构造器和多态： 构造器调用顺序：首先是基类构造器被调用，然后按照顺序初始化成员，接着调用派生类构造器的方法体 继承和清理：在清理工作的时候，应该先释放派生类的对象，然后释放基类的对象 构造器内部多态方法的行为：如果在构造器中调用了正在构造的对象的动态绑定方法，就会用到那个方法的重写定义 协变返回类型：派生类的被重写方法可以返回基类方法返回类型的派生类型。 向下转型：重新将基类类型改为派生类类型，是不安全的。 第十章 接口接口和抽象类提供了一种将接口与实现分离的更加结构化的方法，抽象类是一种介于普通类和接口之间的折中手段。 抽象类和方法：抽象方法只有声明没有方法体，并且使用 abstract 关键字，包含有抽象方法的类称为抽象类，并且类本身也必须限定为抽象。抽象类不能被实例化，如果某个类继承自抽象类，就必须实现该抽象类中所有的抽象方法，如果不这么做的话，新的类也是一个抽象类。 接口创建：使用 interface 关键字创接口。一个接口表示，所有实现了该接口的类看起来都这样。在 Java8 之前，接口里面只允许抽象方法（不用加 abstract 关键字），在 Java8 里面又新增了默认方法。另外，接口同样可以包含属性，这些属性被隐式指明为 static 和 final。使用 implements 关键字使一个类遵循某个特定接口（或一组接口），它表示：接口只是外形，现在我要说明它是如何工作的。最后，接口中的方法是 public 权限的。 默认方法：当实现了某个接口的类没有实现某个方法的时候，此时可以使用接口的默认方法，使用 default 关键字，可以带有方法体。增加默认方法的极具说服力的理由是它允许在不破坏已使用接口的代码的情况下，在接口中增加新的方法。 多继承：Java 中只支持单继承，当时 Java 通过默认方法具有某种多继承的特性，结合带有默认方法的接口意味着结合了多个基类中的行为。因为接口中仍然不允许存在属性（只有静态属性，不适用），所以属性仍然只会来自单个基类或抽象类，也就是说，不会存在状态的多继承。 接口中的静态方法：Java8 中允许在接口中添加静态方法，这么做能恰当地把工具功能置于接口中，从而操作接口，或者成为通用的工具。 抽象类和接口： 特性 接口 抽象类 组合 新类可以组合多个接口 只能继承单一抽象类 状态 不能包含属性（除了静态属性，不支持对象状态） 可以包含属性，非抽象方法可能引用这些属性 默认方法和抽象方法 不需要在子类中实现默认方法。默认方法可以引用其他接口的方法 必须在子类中实现抽象方法 构造器 没有构造器 可以有构造器 可见性 隐式 public 可以是 protected 或友元 完全解耦：使用接口更有利于实现完全解耦，使得代码更具有可复用性。 多接口实现：一个类只能继承自一个父类，同时可以实现多个接口，提高类的灵活度。 使用继承扩展接口：通过继承，可以很容易在接口中增加方法声明，还可以在新的接口中实现多个接口。注意，通常来说，extends 只能用于单一类，但是在构建接口时可以引用多个基类接口。 实现接口时的命名冲突：覆写、实现和重载令人不快地搅和在一起带来了困难，当打算组合接口时，在不同的接口中使用相同的方法名通常会造成代码可读性的混乱，尽量避免这种情况。 接口适配：接口最吸引人的原因之一是相同的接口可以有多个实现。在简单情况下体现在一个方法接受接口作为参数，该接口的实现和传递对象则取决于方法的使用者。 接口字段：因为接口中的字段都自动是 static 和 final 的，所以接口就成为了创建一组常量的方便的工具。但是在 Java8 中，应尽量使用 enum 关键字来定义枚举变量。 接口嵌套：接口可以嵌套在类或者是其他接口中。 第十一章 内部类内部类创建：将类的定义放在外部类的里面。如果想从外部类的非静态方法之外的任意位置创建某个内部类的对象，那么必须具体地指明这个对象的类型：OuterClassName.InnerClassName。 链接外部类：当生成一个内部类的对象的时候，该对象能够访问到外部对象的所有成员，而不需要其他任何特殊权限。当某个外部类的对象创建了一个内部类对象时，此内部类对象必定会秘密地捕获一个指向那个外部类对象的引用。然后，在你访问此外部类的成员时，就是用那个引用来选择外部类的成员。但是这些都是编译器的细节了。 使用 .this 和 .new：如果你需要生成对外部类对象的引用，可以使用外部类的名字后面紧跟圆点和 this。有时你可能想要告知某些其他对象，去创建其某个内部类的对象。要实现此目的，你必须在 new 表达式中提供对其他外部类对象的引用，这是需要使用 .new 语法。 12345678910// innerclasses/DotNew.java// Creating an inner class directly using .new syntaxpublic class DotNew &#123; public class Inner &#123;&#125; public static void main(String[] args) &#123; DotNew dn = new DotNew(); DotNew.Inner dni = dn.new Inner(); &#125;&#125; 内部类和向上转型：当将内部类向上转型为其基类，尤其是转型为一个接口的时候，内部类就有了用武之地。这是因为此内部类-某个接口的实现-能够完全不可见，并且不可用。所得到的只是指向基类或接口的引用，所以能够很方便地隐藏实现细节。 在方法和作用域中声明内部类：可以在一个方法里面或者在任意的作用域内定义内部类。 匿名内部类：通常使用 new ClassName(params) &#123; ... &#125;;，params 用于构造器传参，后面的分号指代语句结束。另外，如果匿名类内部希望使用一个定义在其外部的对象，那么编译器要求其参数引用必须是 final 的。注意在实例化匿名类的时候，可以使用非 final 修饰的变量。匿名内部类与正规的继承相比有些受限，因为匿名内部类既可以扩展类，也可以实现接口，但是不能两者兼备。而且如果是实现接口，也只能实现一个接口。 嵌套类：如果不需要内部类对象与其外部类对象之间有联系，那么可以将内部类声明为 static，这通常称为嵌套类。想要理解 static 应用于内部类时的含义，就必须记住，普通的内部类对象隐式地保存了一个引用，指向创建它的外部类对象。然而，当内部类是 static 的时，就不是这样了。嵌套类意味着： 要创建嵌套类的对象，并不需要其外部类的对象。 不能从嵌套类的对象中访问非静态的外部类对象。 嵌套类与普通的内部类还有一个区别。普通内部类的字段与方法，只能放在类的外部层次上，所以普通的内部类不能有 static 数据和 static 字段，也不能包含嵌套类。但是嵌套类可以包含所有这些东西。 接口内部的类：嵌套类可以作为接口的一部分。你放到接口中的任何类都自动地是 public 和 static 的。 从多层嵌套类中访问外部类的成员：一个内部类被嵌套多少层并不重要——它能透明地访问所有它所嵌入的外部类的所有成员。 为什么需要内部类： 闭包和回调：在 Java8 之前，内部类是实现闭包的唯一方式，在 Java8 中，我们可以使用 lambda 表达式来实现闭包行为，并且更加优雅。 继承内部类：因为内部类的构造器必须连接到指向其外部类对象的引用，所以在继承内部类的时候，事情会变得有点复杂。问题在于，那个指向外部类对象的“秘密的”引用必须被初始化，而在派生类中不再存在可连接的默认对象。 12345678910111213141516// innerclasses/InheritInner.java// Inheriting an inner classclass WithInner &#123; class Inner &#123;&#125;&#125;public class InheritInner extends WithInner.Inner &#123; //- InheritInner() &#123;&#125; // Won&#x27;t compile InheritInner(WithInner wi) &#123; wi.super(); &#125; public static void main(String[] args) &#123; WithInner wi = new WithInner(); InheritInner ii = new InheritInner(wi); &#125;&#125; 内部类标示符：由于编译后每个类都会产生一个 .class 文件，其中包含了如何创建该类型的对象的全部信息。内部类也必须生成一个 .class 文件以包含它们的 Class 对象信息。这些类文件的命名有严格的规则：外部类的名字，加上 “$” ，再加上内部类的名字。如果内部类是匿名的，编译器会简单地产生一个数字作为其标识符。 第十二章 集合泛型和类型安全的集合：通过使用泛型，规定了向某个集合中可以添加的变量类型，方便进行处理，同时不会引发类型转型错误等问题。 Java 集合类库的两个概念：集合（Collection）和映射（Map）。 添加元素组：通过 Arrays.asList 和 Collections.addAll 方法来添加元素组。注意 Arrays.asList 的返回值是一个 List，但是这个 List 不能调整大小。 集合的打印：必须使用 Arrays.toString 来生成数组的可打印形式，但是打印集合无需任何操作。 列表 List：有 ArrayList 和 LinkedList，前者擅长随机访问，后者擅长插入删除操作。当确定元素是否是属于某个 List ，寻找某个元素的索引，以及通过引用从 List 中删除元素时，都会用到 equals() 方法。toArray() 方法将任意的 Collection 转换为数组。 迭代器 Iterators：在任何集合中，都必须有某种方式可以插入元素并再次获取它们。毕竟，保存事物是集合最基本的工作。对于 List ， add() 是插入元素的一种方式， get() 是获取元素的一种方式。如果从更高层次的角度考虑，会发现这里有个缺点：要使用集合，必须对集合的确切类型编程。为此引入迭代器，迭代器相关方法有iterator，next，hasNext，remove。迭代器统一了对集合的访问方式。 ListIterator：ListIterator 是一个更强大的 Iterator 子类型，它只能由各种 List 类生成。 Iterator 只能向前移动，而 ListIterator 可以双向移动。它可以生成迭代器在列表中指向位置的后一个和前一个元素的索引，并且可以使用 set() 方法替换它访问过的最近一个元素。 LinkedList：LinkedList 还添加了一些方法，使其可以被用作栈、队列或双端队列（deque） 。在这些方法中，有些彼此之间可能只是名称有些差异，或者只存在些许差异，以使得这些名字在特定用法的上下文环境中更加适用（特别是在 Queue 中）。如 element，peek，poll，offer 等。 栈 Stack：后进先出规则，Java 1.0 中附带了一个 Stack 类，结果设计得很糟糕（为了向后兼容，我们永远坚持 Java 中的旧设计错误）。Java 6 添加了 ArrayDeque ，其中包含直接实现堆栈功能的方法。 集合 Set：Set 不保存重复的元素，Set 具有与 Collection 相同的接口，因此没有任何额外的功能。HashSet 产生的输出没有可辨别的顺序，这是因为出于对速度的追求， HashSet 使用了散列。由 HashSet 维护的顺序与 TreeSet 或 LinkedHashSet 不同，因为它们的实现具有不同的元素存储方式。TreeSet 将元素存储在红-黑树数据结构中，而 HashSet 使用散列函数。LinkedHashSet 因为查询速度的原因也使用了散列，但是看起来使用了链表来维护元素的插入顺序。 映射 Map：根据键快速查找值的结构。Map 可以返回由其键组成的 Set ，由其值组成的 Collection ，或者其键值对的 Set 。keySet() 方法生成由在 petPeople 中的所有键组成的 Set ，它在 for-in 语句中被用来遍历该 Map 。 队列 Queue：先进先出的集合，LinkedList 实现了 Queue 接口，并且提供了一些方法以支持队列行为，因此 LinkedList 可以用作 Queue 的一种实现。offer() 是与 Queue 相关的方法之一，它在允许的情况下，在队列的尾部插入一个元素，或者返回 false 。 peek() 和 element() 都返回队头元素而不删除它，但是如果队列为空，则 element() 抛出 NoSuchElementException ，而 peek() 返回 null 。 poll() 和 remove() 都删除并返回队头元素，但如果队列为空，poll() 返回 null ，而 remove() 抛出 NoSuchElementException 。 优先级队列 PriorityQueue：先进先出（FIFO）描述了最典型的队列规则（queuing discipline）。优先级队列声明下一个弹出的元素是最需要的元素（具有最高的优先级）。当在 PriorityQueue 上调用 offer() 方法来插入一个对象时，该对象会在队列中被排序。默认的排序使用队列中对象的自然顺序（natural order），但是可以通过提供自己的 Comparator 来修改这个顺序。 PriorityQueue 确保在调用 peek()， poll() 或 remove() 方法时，获得的元素将是队列中优先级最高的元素。 集合和迭代器：Collection 是所有序列集合共有的根接口，使用接口描述的一个理由是它可以使我们创建更通用的代码。通过针对接口而非具体实现来编写代码，我们的代码可以应用于更多类型的对象。为了对集合进行遍历操作，我们可以使用迭代器来进行操作。 for-in 迭代器：到目前为止，for-in 语法主要用于数组，但它也适用于任何 Collection 对象。这样做的原因是 Java 5 引入了一个名为 Iterable 的接口，该接口包含一个能够生成 Iterator 的 iterator() 方法。for-in 使用此 Iterable 接口来遍历序列。 适配器惯用法：如果已经有一个接口并且需要另一个接口时，则编写适配器就可以解决这个问题。在这里，若希望在默认的正向迭代器的基础上，添加产生反向迭代器的能力，因此不能使用覆盖，相反，而是添加了一个能够生成 Iterable 对象的方法，该对象可以用于 for-in 语句。 注意：不要在新代码中使用遗留类 Vector ，Hashtable 和 Stack 。 Java 集合框架简图：黄色为接口，绿色为抽象类，蓝色为具体类。虚线箭头表示实现关系，实线箭头表示继承关系。 collection map 第十三章 函数式编程Lambda 表达式：(params) -&gt; &#123; statements; &#125;，只有一个参数的时候，可以省略括号，如果只有一行的话，花括号应该省略。 方法引用：ClassName::MethodName。 未绑定的方法引用：未绑定的方法引用是指没有关联对象的普通（非静态）方法。 使用未绑定的引用时，我们必须先提供对象。 构造函数的引用：ClassName::new 函数式接口：Lambda 表达式包含类型推导，但是如果存在(x, y) -&gt; x + y这样的 lambda 表达式，编译器就不能自动进行类型推导了。因为 x, y 既可以是 String 类型，也可以是 int 类型。此时引入java.util.function包，包含了一组接口，每个接口只有一个抽象方法，称为函数式方法。Java 8 允许我们将函数赋值给接口，这样的语法更加简单漂亮。 多参数函数式接口：在 function 包中，只有很少的接口，我们可以自己定义一个函数接口，如下： 1234567// functional/TriFunction.java@FunctionalInterfacepublic interface TriFunction&lt;T, U, V, R&gt; &#123; R apply(T t, U u, V v);&#125; 高阶函数：消费或产生函数的函数。 12345678910111213141516// functional/ProduceFunction.javaimport java.util.function.*;interface FuncSS extends Function&lt;String, String&gt; &#123;&#125; // [1]public class ProduceFunction &#123; static FuncSS produce() &#123; return s -&gt; s.toLowerCase(); // [2] &#125; public static void main(String[] args) &#123; FuncSS f = produce(); System.out.println(f.apply(&quot;YELLING&quot;)); &#125;&#125; 闭包：对外部变量引用的函数。 1234567891011// functional/Closure1.javaimport java.util.function.*;public class Closure1 &#123; int i; IntSupplier makeFun(int x) &#123; return () -&gt; x + i++; &#125;&#125; 柯里化和部分求值：柯里化意为：将一个多参数的函数，转换为一系列单参数函数。 第十四章 流式编程流式编程的特点：代码可读性更高；懒加载，意味着它只在绝对必要时才计算，由于计算延迟，流使我们能够表示非常大（甚至无限）的序列，而不需要考虑内存问题。 流支持：Java 8 通过在接口中添加default修饰的方法实现流的平滑嵌入。流操作有三种类型：创建流，修改流元素（中间操作），消费流元素（终端操作）。 流创建：通过 Stream.of 将一组元素转化为流，除此之外，每个集合都可以通过调用 stream 方法来产生一个流。除此以外，还有： 随机数流：new Random().ints() int 类型流：IntStream.range(start, end, step) generate：Stream.generate(obj) iterate：Stream.iterate(initValue, cb) 流的构造着模式：首先创建一个 builder 对象，然后将创建流所需的多个信息传递给它，最后builder 对象执行“创建”流的操作。 Arrays: Arrays.stream() 中间操作：用于从一个流中获取对象，并将对象作为另一个流从后端输出，以连接到其他操作。 peek：无修改地查看流中的元素 sorted：排序，可以使用 lambda 参数 distinct：消除重复元素 filter：通过过滤条件的被保存下来，否则删除 map：将函数操作应用在输入流的元素中，并将返回值传递到输出流中。还有 mapToInt, mapToLong 等 flatMap：将产生流的函数应用在每个元素上（与 map() 所做的相同），然后将每个流都扁平化为元素，因而最终产生的仅仅是元素。对应还有 flatMapToInt 等 Optimal 类：一些标准流操作返回 Optional 对象，因为它们并不能保证预期结果一定存在。当流为空的时候你会获得一个 Optional.empty 对象，而不是抛出异常。 解包 Optimal 的函数：ifPresent(Consumer), orElse(otherObject) 创建 Optimal：静态方法有empty(), of(value), ofNullable(value) Optimal 流：假设你的生成器可能产生 null 值，那么当用它来创建流时，你会自然地想到用 Optional 来包装元素。可以使用 filter() 来保留那些非空 Optional 终端操作：以下操作将会获取流的最终结果，终端操作（Terminal Operations）总是我们在流管道中所做的最后一件事。 数组：toArray 循环：forEach，forEachOrdered 集合：collect 组合：reduce 匹配：allMatch，anyMatch，noneMatch 查找：findFirst，findAny 信息：count，max，min 数字流信息：average，max，min 第十五章 异常异常概念：C 以及其他早期语言常常具有多种错误处理模式，这些模式往往建立在约定俗成的基础之上，而并不属于语言的一部分。通常会返回某个特殊值或者设置某个标志，并且假定接收者将对这个返回值或标志进行检查，以判定是否发生了错误。“异常”这个词有“我对此感到意外”的意思。问题出现了，你也许不清楚该如何处理，但你的确知道不应该置之不理，你要停下来，看看是不是有别人或在别的地方，能够处理这个问题。 异常捕获： try 语句块：对可能产生异常的语句进行捕获 catch 语句块：对每种可能出现的异常准备相应的处理语句 终止和恢复：Java 支持终止模型，这种模型假设错误非常严重，以至于程序无法返回到异常发生的地方继续执行。另外一种是恢复模型，如果 Java 想要实现类似恢复的行为，可以把 try 块放在循环里面，直到得到满意的结果 自定义异常：想要自定义异常类，必须从已有的异常类继承。 异常声明：在方法后面加上throws ExceptionType1，ExceptionType2,,, 捕获所有异常：直接捕获基类 Exception 多重捕获：Java7 中支持，使用 | 连接不同类型的异常，如catch(Except1 | Except2 | Except3 | Except4 e) &#123;&#125; 栈轨迹：printStackTrace 重新抛出异常： 1234catch(Exception e) &#123; System.out.println(&quot;An exception was thrown&quot;); throw e;&#125; 使用 finally 进行清理：不管是否发生异常，都会执行 finally 块里面的语句。 finally 作用：用于将资源恢复到初始状态 在 return 中使用 finally：从何处返回无关紧要，finally 子句永远会执行 异常丢失：finally 里面使用 return 将会导致不会抛出任何异常 Try-With-Resources 用法：try()&#123; &#125;catch(Exception e)&#123; &#125;，在 try 小括号里面的声明的对象需要实现java.lang.AutoCloseable接口，接口只有一个方法close()。退出 try 块会调用每个对象的 close() 方法，并以与创建顺序相反的顺序关闭它们。 异常匹配：异常处理系统会按照代码的书写顺序找出“最近”的处理程序。找到匹配的处理程序之后，它就认为异常将得到处理，然后就不再继续查找。 第十七章 文件文件和目录路径：一个 Path 对象表示一个文件或者目录的路径，是一个跨操作系统（OS）和文件系统的抽象，通过 Paths.get(URL) 来获得一个对应的 Path 对象。 选取路径部分片段：getName 路径分析：Files 工具类包含一系列完整的方法用于获得 Path 相关的信息，如exists，size，isDirectory Paths 的增删修改：relative，resolve 遍历目录：Files.walkFileTree，具体操作实现由参数二 FileVisitor 里面的四个抽象方法决定： preVisitDirectory()：在访问目录中条目之前在目录上运行。 visitFile()：运行目录中的每一个文件。 visitFileFailed()：调用无法访问的文件。 postVisitDirectory()：在访问目录中条目之后在目录上运行，包括所有的子目录。 文件系统：可以使用静态的 FileSystems 工具类获取默认的文件系统 路径监听：通过文件系统的 WatchService 可以设置一个进程对目录中的更改做出响应 文件查找：通过在 FileSystem 对象上调用 getPathMatcher 可以获得一个 PathMatcher，传入对应的两种模式：glob 或者 regex。 文件读写：如果文件比较小，使用 Files.readAllLines 可以一次性读取整个文件，返回一个 List&lt;String&gt;；使用 Files.write 写入 byte 数组或者任何可迭代对象。对于文件比较大的时候，可以使用 Files.lines 将文件转换为输入流。 第十八章 字符串字符串的不可变性：String 对象是不可变的，String 类中的每个看起来会修改 String 值的方法，实际上都是创建了一个全新的 String 对象。 + 的重载与 StringBuilder：在 String 中，+ 代表了字符串之间的 append 操作，每次 + 都会创建一个 String 对象，代价高昂。StringBuilder 提供了丰富全面的方法，包括insert，replace，append，delete。另外还有 StringBuffer，和 StringBuilder 不同点在于后者是线程不安全的，前者是线程安全的。 意外递归：如&quot;som string&quot; + this ，我们想要打印出某个字符串的地址，但是编译器首先辨别出左边是 String 对象，+ 要求右边的变量也是 String 对象（先进行转换），这就涉及到了意外递归。可以使用Object.toString()打印地址。 字符串操作：当需要改变字符串的内容时，String 类的方法都会返回一个新的 String 对象。同时，如果内容不改变，String 方法只是返回原始对象的一个引用而已。这可以节约存储空间以及避免额外的开销。 格式化输出： System.out.printf, System.out.format 格式化修饰符：%[argument_index$][flags][width][.precision]conversion Formatter 转换： 类型 含义 d 整型（十进制） c Unicode字符 b Boolean值 s String f 浮点数（十进制） e 浮点数（科学计数） x 整型（十六进制） h 散列码（十六进制） % 字面值“%” String.format 正则化表达式： 在正则表达式中，用 \\d 表示一位数字。如果在其他语言中使用过正则表达式，那你可能就能发现 Java 对反斜线 \\ 的不同处理方式。在其他语言中，\\\\ 表示“我想要在正则表达式中插入一个普通的（字面上的）反斜线，请不要给它任何特殊的意义。”而在Java中，\\\\ 的意思是“我要插入一个正则表达式的反斜线，所以其后的字符具有特殊的意义。”例如，如果你想表示一位数字，那么正则表达式应该是 \\\\d。如果你想插入一个普通的反斜线，应该这样写 \\\\\\。不过换行符和制表符之类的东西只需要使用单反斜线：\\n\\t。如果要表示“可能有一个负号，后面跟着一位或多位数字”，可以这样： 1-?\\\\d+ 表达式： 表达式 含义 . 任意字符 [abc] 包含a、b或c的任何字符（和`a [^abc] 除a、b和c之外的任何字符（否定） [a-zA-Z] 从a到z或从A到Z的任何字符（范围） [abc[hij]] a、b、c、h、i、j中的任意字符（与`a [a-z&amp;&amp;[hij]] 任意h、i或j（交） \\s 空白符（空格、tab、换行、换页、回车） \\S 非空白符（[^\\s]） \\d 数字（[0-9]） \\D 非数字（[^0-9]） \\w 词字符（[a-zA-Z_0-9]） \\W 非词字符（[^\\w]） CharSequence：接口从 CharBuffer，String，StringBuffer，StringBuilder 抽象出了一般化定义 1234567interface CharSequence &#123; char charAt(int i); int length(); CharSequence subSequence(int start, int end); String toString(); &#125; Pattern 和 Matcher：更具一个 String 对象生成一个 Pattern 对象，通过 Pattern 对象的 match 方法产生一个 Matcher 对象。 组（group）：A(B(C))D 中有三个组：组 0 是 ABCD，组 1 是 BC，组 2 是 C。通过 Matcher 对象的 group 方法可以获取到每个组。 第十九章 类型信息Java 在运行时识别对象和类的信息的方式：传统的 RTTI(RunTime Type Information，运行时类型信息)，反射机制。 RTTI 必要性：下面这个代码展示了 Shape 基类下的派生类的相关操作，使用 RTTI，我们可以在运行时进行类型确认，同时，编码的时候只需要注意对基类的处理就行，不会影响代码的可扩展性。 1234567public class Shapes &#123; public static void main(String[] args) &#123; Stream.of( new Circle(), new Square(), new Triangle()) .forEach(Shape::draw); &#125;&#125; 实际上，上述代码编译时候，Stream 和 Java 泛型系统确保放入的都是 Shape 对象或者其派生类，运行时，自动类型转换确保从 Stream 中取出的对象都是 Shape 类型。 Class 对象：Class 对象包含了与类有关的信息，每个类都会产生一个 Class 对象，每当编译一个新类，就会产生一个 Class 对象（保存在同名的 .class 文件中），为了生成该类的对象，JVM 首先会调用类加载器子系统将这个类加载到内存中。Java 是动态加载的，即只有在类需要的时候才会进行类的加载。所有的 Class 对象都属于 Class 类，可以通过Class.forName()来得到类的 Class 对象，或者通过someInstance.getClass()得到。 类字面常量：对于一个 FancyToy.class 的文件，我们可以直接使用FancyToy.class得到对应的类对象，相较于Class.forName的方式，该种方式更加简单和安全，并且效率更高。另外，使用类字面常量的时候，不会自动初始化该 Class 对象。为了使用类的三个步骤： 加载：查找字节码，并且创建一个 Class 对象 链接：验证字节码，为 static 字段分配存储空间，如果需要，将解析这个类对其他类的引用 初始化：先初始化基类，然后执行 static 初始化器和 static 初始化块 泛化的 Class 引用：Class 引用总是指向某个 Class 对象，而 Class 对象可以用于产生类的实例，并且包含可作用于这些实例的所有方法代码。使用 Class&lt;?&gt; 表示通配所有类型，Class&lt;? extends Sup&gt;表示通配所有 Sup 的派生类型，Class&lt;? super Sub&gt;表示通配 Sub 的基类。 cast 方法：接受参数对象，并将其类型转换为 Class 引用的类型。 类型转换检测：Java 中支持自动向上转型，但是向下转型是强制的，需要用户指代向下转换的类型，如果没有通过向下类型转换，就会报错，否则转型成功。另外，可以使用 instanceof 判断某个实例是否是某个对象的实例。Class.isInstance 可以动态测试对象类型。 类的等价比较：当查询类型信息的时候，使用 instanceof 或者 isInstance，这两种方式产生的结果相同，但是 Class 对象直接比较与上述方式不同。instanceof 说的是“你是这个类，还是从这个类派生的类？”。而如果使用 == 比较实际的Class 对象，则与继承无关 —— 它要么是确切的类型，要么不是。 反射：运行时类信息。java.lang.reflect库中包含了相关的类来实现反射这一机制。RTTI 和反射的真正区别在于，使用 RTTI 时，编译器在编译时会打开并检查 .class 文件。换句话说，你可以用“正常”的方式调用一个对象的所有方法；而通过反射，.class 文件在编译时不可用，它由运行时环境打开并检查。 类方法提取器：getMethods 和 getConstructors 获取对应的类方法 动态代理：代理是基本的设计模式之一。一个对象封装真实对象，代替其提供其他或不同的操作—这些操作通常涉及到与“真实”对象的通信，因此代理通常充当中间对象。通过调用静态方法Proxy.newProxyInstance来创建动态代理。 接口和类型：interface 关键字的一个重要目标就是允许程序员隔离组件，进而降低耦合度。 第二十章 泛型简单泛型：直接使用类型暂代符表示某种类型即可，下图是一个二元组： 12345678// onjava/Tuple2.javapackage onjava;public class Tuple2&lt;A, B&gt; &#123; public final A a1; public final B a2; public Tuple2(A a, B b) &#123; a1 = a; a2 = b; &#125;&#125; 泛型接口：泛型可以应用于接口，例如生成器，这是一种专门负责创建对象的类。注意，Java 中支持基本类型作为泛型类型。 泛型方法：泛型方法独立于类而改变方法。作为准则，请“尽可能”使用泛型方法。通常将单个方法泛型化要比将整个类泛型化更清晰易懂。 1234567// generics/GenericMethods.javapublic class GenericMethods &#123; public &lt;T&gt; void f(T x) &#123; System.out.println(x.getClass().getName()); &#125;&#125; 泛型擦除：Java 泛型是使用擦除实现的。这意味着当你在使用泛型时，任何具体的类型信息都被擦除了，你唯一知道的就是你在使用一个对象。因此，List&lt;String&gt; 和 List&lt;Integer&gt; 在运行时实际上是相同的类型。它们都被擦除成原生类型 List，使用 getClass 得到的结果相同。 特殊方式：当我们想要调用某个泛型类型的方法的时候，我们可以使用&lt;T extends Sup&gt;，这样我们才能调用 Sup 里面的相关方法。 擦除的问题：由于擦除的存在，所有关于参数的信息就丢失了。当你在编写泛型代码时，必须时刻提醒自己，你只是看起来拥有有关参数的类型信息而已。 补偿擦除：由于擦除的存在，我们无法在运行时知道参数的确切类型，为了解决这个问题，我们显式传递一个 Class 对象，以在类型表达式中使用。 创建类型的实例：直接通过new T()是行不通的，但是我们可以通过对应的 Class 对象的 newInstance 方法来创建新的实例 泛型数组：我们无法创建泛型数组，解决方式是在试图创建泛型数组的时候使用 ArrayList 边界：由于擦除会删除类型信息，因此唯一可用的无限制泛型参数的方法是那些 Object 可用的方法。但是，如果将该参数限制为某类型的子集，则可以调用该子集中的方法。为了应用约束，Java 泛型使用了 extends 关键字。 通配符：使用？表示。使用&lt;? extends Sup&gt;表示继承自 Sup 的类，使用&lt;? super Sub&gt;表示 Sub 的基类，使用&lt;?&gt;表示任意一种类型。 使用泛型的问题： 任何基本类型都不能作为类型参数 一个类不能实现同一个泛型接口的两种变体，由于擦除的原因，这两个变体会成为相同的接口 使用带有泛型类型参数的转型或 instanceof 不会有任何效果 自限定的类型：下图代码展示了这种惯用法： 1class SelfBounded&lt;T extends SelfBounded&lt;T&gt;&gt; &#123; // ... 古怪的循环泛型（CRG）： 123456// generics/CuriouslyRecurringGeneric.javaclass GenericType&lt;T&gt; &#123;&#125;public class CuriouslyRecurringGeneric extends GenericType&lt;CuriouslyRecurringGeneric&gt; &#123;&#125; 自限定： 1class A extends SelfBounded&lt;A&gt;&#123;&#125; 参数协变：自限定类型的价值在于它们可以产生协变参数类型，即方法参数类型会随子类而变化。 动态类型安全：Java5 中的 Collections 有一组便利工具函数，可以解决类型检查问题，如checkedMap等。 泛型异常：由于擦除的原因，catch 语句不能捕获泛型类型的异常，因为在编译期和运行时都必须知道异常的确切类型。 混型（Mixin）：最基本的概念是混合多个类的能力，以产生一个可以表示混型中所有类型的类。 与接口混合 使用装饰器模式 与动态代理混合 潜在类型机制：也称作鸭子类型机制，即“如果它走起来像鸭子，并且叫起来也像鸭子，那么你就可以将它当作鸭子对待”。 第二十一章 数组数组特性：效率，类型，保存基本数据类型的能力。 一等对象：不管使用什么类型的数组，数组中的数据集实际上都是对堆中真正对象的引用。注意区分聚合初始化和动态聚合初始化。 返回数组：在 Java 中，可以直接返回一个数组，而不用担心其内存消耗情况，垃圾回收期会自动处理。 多维数组：使用多层方括号界定每个维度的大小，同样的也存在有不规则的数组。可以使用 Arrays.deepToString 来查看多维数组里面的内容。Arrays.setAll 方法用于初始化数组。 泛型数组：数组和泛型并不能很好的结合，不能实例化参数化类型的数组，但是允许您创建对此类数组的引用。 Arrays 相关方法： fill：将单个值复制到整个数组，或者在对象数组的情况下，将相同的引用复制到整个数组 setAll：使用一个生成器用于生成不同的值，生成器的参数是 int 数组索引 asList：将数组转换为列表 copyOf：以新的长度复制数组 copyOfRange：复制现有数组的一部分数据 equals：判断数组是否相同 deepEquals：多维数组相等性比较 stream：生成流 sort：排序 binarySearch：二分查找 toString，deepToString：数组的字符串表示 数组元素修改：通过使用 setAll 方法来索引现有数据元素 第二十二章 枚举基本 enum 特性：调用 values 方法，可以返回对应的数组，同时调用 ordinal 方法可以知道某个 value 的次序，这个次序默认从 0 开始。可以使用 import static 导入 enum 类型。 方法添加：除了不能继承自一个 enum 之外，我们基本上可以将 enum 看作一个常规的类。如果你打算定义自己的方法，那么必须在 enum 实例序列的最后添加一个分号。 switch 语句中的 enum：enum 的 values 本来就具有顺序，可以搭配 switch 使用。 values 方法：enum 类型的对象会有一个 values 方法，这个方法是由编译器添加的 static 方法。 实现而非继承：enum 继承自 Enum 类，由于 Java 不支持多继承，enum 不能再次继承其他类，但是创建一个新的 enum 时，可以实现一个或者多个接口。 使用接口组织枚举：无法从 enum 继承子类很令人沮丧，但是我们可以尝试使用接口来组织枚举类。 使用 EnumSet 代替 Flags：EnumSet 中的元素必须来自一个 enum。EnumSet 基础是 long，只有 64 位，但是在需要的时候，会增加一个 long。 使用 EnumMap：要求键必须来自一个 enum，EnumMap 在内部使用数组实现。 使用 enum 的状态机：枚举类型很适合用于创建状态机。 第二十三章 注解java.lang 中的注解包括：@Override，@Deprecated，@SuppressWarnings，@SafeVarargs，@FunctionalInterface。 基本语法： 定义注解：注解的定义看起来很像接口的定义，事实上，他们和其他 Java 接口一样，也会被编译成 class 文件。 1234567// onjava/atunit/Test.java// The @Test tagpackage onjava.atunit;import java.lang.annotation.*;@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface Test &#123;&#125; @target 标示注解的对象，@Retention 标示注解在哪里可用。 元注解：@target，@Retention，@Documented，@Inherited，@Repeatable 编写注解处理器：使用反射机制 API 实现注解的读取。 注解元素：基本类型，String，Class，enum，Annotation，以上类型的数组 默认值限制：首先，元素不能有不确定的值。也就是说，元素要么有默认值，要么就在使用注解时提供元素的值。任何非基本类型的元素，无论是在源代码声明时还是在注解接口中定义默认值时，都不能使用 null 作为其值。 生成外部文件：Web Service，自定义标签库以及对象/关系映射工具（例如 Toplink 和 Hibernate）通常都需要 XML 描述文件，而这些文件脱离于代码之外。除了定义 Java 类，程序员还必须忍受沉闷，重复的提供某些信息，例如类名和包名等已经在原始类中提供过的信息。每当你使用外部描述文件时，他就拥有了一个类的两个独立信息源，这经常导致代码的同步问题。 注解不支持继承：不能使用 extends 关键字来继承 @interfaces。 实现处理器：通过 getAnnotation 来检查是否存在注解，如果存在的话做出相应的操作 基于注解的单元测试：如 JUnit。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.zsstrike.top/tags/Java/"}]},{"title":"hexo和typora搭配写博客","slug":"hexo和typora搭配写博客","date":"2020-10-21T06:17:54.000Z","updated":"2020-12-17T10:23:43.557Z","comments":true,"path":"2020/10/21/hexo和typora搭配写博客/","link":"","permalink":"http://blog.zsstrike.top/2020/10/21/hexo%E5%92%8Ctypora%E6%90%AD%E9%85%8D%E5%86%99%E5%8D%9A%E5%AE%A2/","excerpt":"本文介绍使用Typora写博客，使用Hexo发布文章的技巧。主要涉及图片的路径问题。","text":"本文介绍使用Typora写博客，使用Hexo发布文章的技巧。主要涉及图片的路径问题。 解决Hexo图片路径问题在使用Typora的时候，首先进入到Typora的设置里面，将图片插入格式改为如下设置： image-20201021142301183 这样的话，在写作的时候，我们就可以实时预览到自己插入的图片了。 接着，在博客仓库的_config.yml中设置post_asset_folder: true。 但是这样设置的话会产生一个问题，就是在执行hexo g的时候，得到的博客文章路径会多一个&#123;&#123;title&#125;&#125;,导致图片在发布的时候渲染不出来。为了解决这个问题，可以使用hexo-typora-img， 1npm i hexo-typora-img 这个插件会将原来的路径在渲染前将其改为Hexo可以识别的图片路径，从而在预览发布的时候也可以看到图片。","categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://blog.zsstrike.top/tags/Hexo/"},{"name":"Typora","slug":"Typora","permalink":"http://blog.zsstrike.top/tags/Typora/"}]},{"title":"Redis设计与实现笔记","slug":"Redis设计与实现笔记","date":"2020-10-18T15:14:52.000Z","updated":"2020-12-17T10:23:43.512Z","comments":true,"path":"2020/10/18/Redis设计与实现笔记/","link":"","permalink":"http://blog.zsstrike.top/2020/10/18/Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E7%AC%94%E8%AE%B0/","excerpt":"本文章是对《Redis设计与实现》书籍的一个整理笔记，记录了其中个人认为比较重要的部分。","text":"本文章是对《Redis设计与实现》书籍的一个整理笔记，记录了其中个人认为比较重要的部分。 第二章 简单动态字符串 SDS 定义： image-20201012191209564 SDS 与 C 字符串的区别： 常数复杂度获取字符串的长度 杜绝缓冲区溢出 减少修改字符串时带来的内存分配的次数，包括空间预分配和惰性空间释放 二进制安全 兼容部分 C 字符串函数 第二章 链表 Redis 的链表实现的特性可以总结如下： 双端 无环 带表头指针和表尾指针 带链表长度计数器 多态：链表节点使用void*指针来保存节点值, 并且可以通过 list 结构的dup, free, match三个属性为节点值设置类型特定函数,所以链表可以用于保存各种不同类型的值 image-20201012192406288 第三章 字典 Redis 普通状态下的字典： image-20201012194731429 哈希算法：首先计算哈希值，然后计算出索引值 image-20201012194922912 Redis 使用的 MurmurHash 算法计算建的哈希值，该算法的优点在于即使输入的键时有规律的，算法仍然能给出一个很好的随机分布性。 解决键冲突：使用链地址法解决键冲突，使用头插法进行插入。 rehash：当负载因子过大的时候，就会开始进行相应的扩展或者收缩。使用ht[1]协助扩展。 渐进式 rehash：扩展或收缩哈希表需要将ht[0]里面的所有键值对 rehash到ht[1] 里面, 但是, 这个 rehash动作并不是一次性、集中式地完成的,而是分多次、渐进式地完成。在渐进期间，字典会同时使用两个哈希表，但是插入的时候只会插入到ht[1]。 第五章 跳跃表 跳跃表：有序的数据结构，通过在节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。支持 平均O(logn)，最坏O(N)复杂度的节点查找。 跳跃表结构示意图： image-20201013191437427 header和tail分别表示表头节点和表尾节点，level表示的层数，length时跳跃表的长度。BW表示的是回退指针，指向上一个跳跃表节点。箭头线上面的数字是跨度，表示跨过了几个节点。 在Redis中，每个节点的层高是1到32之间的随机数，在同一个跳跃表中，多个节点可以包含相同的分值，但是每个节点的额成员对象必须唯一，另外，跳跃表中的节点按照分值大小排序，如果分值大小相同，则按照成员对象的大小排序。 第六章 整数集合 整数集合实现： image-20201013192842489 虽然contents是int8_t类型的数组，但实际上数组并不保存任何int8_t类型的值，该数组的真正类型取决于encoding属性的值。 升级：每当我们要将一个新元素添加到整数集合里面，并且新元素的类型比整数集合现有所有元素的类型都要长时,整数集合需要先进行升级(upgrade)，然后才能将新元素添加到整数集合里面。首先根据新元素的类型，扩展整数集合底层数组的空间大小，并且为新元素分配相应的空间；将底层数组现有的所有元素都转换成与新元素相同的类型，并且将其放到正确的位置上，保持有序性不变；将新元素添加到底层数组里面（新元素要么在最后位置，要么在首位置）。 升级的好处：提升灵活性，节约内存。 降级：整数集合不支持降级操作，一旦对数据进行了升级，编码就会一直保持升级之后的状态。 第七章 压缩列表 压缩列表的构成： image-20201013194653014 zlbytes记录整个压缩列表占用的内存字节数，zltail记录列表表位距离压缩列表的起始地址有多少字节，zllen记录节点个数，zlend特殊值0xFF，标记为压缩列表的末端。 压缩列表节点的构成： image-20201013195122854 根据previous_entry_length可以计算出上一个节点的地址，根据encoding可以知道存放的数据类型和长度，content则是一个字节数组或者整数。 连锁更新：当压缩列表的原来的节点的数值在250-254之间的时候，此时如果新增或者（删除）一个节点，会导致原来的首节点previous_entry_length大小从1字节转换为五个字节，从而引发连锁更新： image-20201013200603337 尽管连锁更新的复杂度较高，但是真正造成性能问题的几率还是很低的。 第八章 对象 对象的类型和编码：Redis中的对象的结构如下： image-20201013201146386 其中，type表示对象的类型： image-20201013201321101 对象的ptr指向对象的底层实现数据结构，而这些数据结构有对象的encoding属性决定： image-20201013201519317 每种类型的对象至少使用了两种不同的编码： image-20201013201632078 通过encoding来设定对象的编码，极大提高了Redis的灵活性和效率。 字符串对象：编码可以是int，raw或者embstr。 列表对象：编码可以是ziplist或者linkedlist。 哈希对象：编码可以是ziplist或者hashtable。 集合对象：编码可以是intset或者hashtable。 有序集合对象：编码可以是ziplist或者skiplist。有序集合同时使用跳跃表可字典来实现的原因是能够让有序集合的查找和范围型的操作都尽可能快的执行，减少时间复杂度。 类型检查和命令多态：类型检查的实现是通过键中的类型来进行的，命令的多态则是根据值对象的编码方式进行的。 image-20201013203249016 内存回收：采用引用计数的方式实现垃圾回收。 对象共享：对象的引用计数属性还有对象共享的作用。目前来说, Redis会在初始化服务器时, 创建一万个字符串对象, 这些对象包含了从0 到9999的所有整数值, 当服务器需要用到值为0到9999的字符串对象时, 服务器就会使用这些共享对象, 而不是新创建对象。 对象的空转时间：redisObject结构还包含了一个属性lru，用于记录对象最后一次被命令程序访问的时间。 第九章 数据库 服务器中的数据库：Redis服务器将所有的数据库保存在服务器状态中的redis.h/redisServer结构db数组中，每个redis.h/redisDb结构代表着一个数据库，另外程序会根据dbnum来决定应该创建多少个数据库： 1234567struct redisServer &#123; // 数据库 redisDb *db; // 数据库的数量 int dbnum; // ...&#125; 切换数据库：通过SELECT命令实现，实际上是通过修改客户端的db指针来实现的 image-20201018131936321 数据库键空间：每个数据库由RedisDb保存，其中RedisDb.dict保存了数据库中的所有键值对。 image-20201018132511374 增删查改都是在dict结构上进行的，另外，在读写键空间的时候，还会执行一些其他的额外操作，比如更新LRU时间，提前判断键是否过期，标记键为dirty等等维护数据库一致性的操作。 设置键的生存时间或过期时间：在Redis中有四个不同的命令来设置键的生存时间或者过期时间，分别是EXPIRE, PEXPIRE, EXPIREAT, PEXPIREAT命令，前三个命令都是转换为PEXPIREAT命令执行，转换图如下： image-20201018133226586 RedisDb结构中的expires字典保存着数据库中所有过期键的过期时间，称该字典是过期字典，其中键是一个指针，指向某个键空间的某个键对象，过期字典的值保存着long long类型的整数，保存着过期时间。 image-20201018133620122 由此，查看某个键对应的值之前需要先判断一下是否在过期字典中，同时检测其是否过期。 过期键删除策略：定时删除，惰性删除，定期删除。定时删除策略对内存是最友好的:通过使用定时器,定时除策略可以保证过期键会尽 可能快地被删除,并释放过期键所占用的内存。惰性删除策略对CPU时间来说是最友好的:程序只会在取出键时才对键进行过期检查，这可以保证删除过期键的操作只会在非做不可的情况下进行,并且删除的目标仅限于当前处理的键,这个策略不会在删除其他无关的过期键上花费任何CPU时间。定期删除策略每隔一段时间执行一次删除过期键操作,并通过限制删除操作执行的时长和频率来减少删除操作对CPU时间的影响 。 Redis的过期键的删除策略：使用惰性删除和定期删除两种策略。惰性删除策略通过db.c/expireIfNeeded函数实现，定期删除则是通过redis.c/activeExpireCycle函数实现（分多次遍历服务器中的各个数据库，从过期字典中随机抽查一部分键的过期时间，并且删除其中的过期键，其中current_db全部变量保存着当前指向到那个数据库中了，下次便利的时候就可以接着从上次数据库的下一个接着检查）。 AOF，RDB和复制功能对过期键的处理： RDB：在执行SAWE命令或者BGSAVE命令创建一个新的RDB文件时,程序会对数据库中的 键进行检查,已过期的键不会被保存到新创建的RDB文件中。 载入RDB文件的时候，如果服务器以主服务器模式运行，那么在载入RDB文件的时候，程序会剔除过期的键，如果是以从服务器模式运行的话，那么就会保存所有的键。 AOF：AOF文件重写的时候，会对数据库中的键进行检查，已经过期的键不会保存到AOF文件中。 复制：当服务器运行在复制模式下面的时候，从服务器的过期键的删除动作由主服务控制，从服务器不会主动删除过期的键，除非主服务器发送DEL命令来显式删除某个键。 数据库通知：通过发布订阅模式实现。 第十章 RDB持久化 RDB文件的创建和载入：有两个命令可以用于生成RDB文件，一个是SAVE，另外一个是BGSAVE，后者是非阻塞的。另外由于AOF文件的更新频率比RDB文件的更新频率高一些，会首选AOF文件来恢复数据库状态。 image-20201018141926970 虽然BGSAVE执行的时候仍然可以继续处理客户端的请求，但是SAVE，BGSAVE和BGREWRITEAOF命令却不能再次执行。在服务器载入RDB文件的时候，会一直处于阻塞状态，直到载入工作完成为止。 自动间隔性保存：当服务器满足一定的条件的时候，就会自动执行相应BGSAVE命令，来及时保存数据库的状态。默认保存条件如下： 123save 900 1save 300 10save 60 10000 上述代码的含义是900秒之内，至少修改了一次数据库，或者300秒之内，修改了至少10次数据库，或者60秒内，至少修改了10000次数据库，这些配置文件会被保存在saveparams属性中。除了saveparams数组之外，还有一个dirty计数器，以及一个lastsave属性，通过上述三个属性我们就可以判断是否存在必要来执行自动保存功能了。 123456struct redisServer &#123; struct saveparam *saveparams; long long dirty; time_t lastsave; // ...&#125; RDB文件结构如下： image-20201018143358528 image-20201018143504979 image-20201018143600755 第十一章 AOF持久化 AOF持久化的实现：当AOF的功能打开的时候，服务器在完成一个命令的之后，会将其保存在redisServer的aof_buf缓冲区的末尾。Redis的服务器就是一个时间循环，这个循环中负责接受客户端的命令请求，以及向客户端发送命令回复，而时间事件则负责执行serverCron这样需要定时运行的函数。每次结束一个事件循环的时候，需要考虑是否需要将缓冲区的内容写入到AOF文件中。 由于AOF文件包含了重建数据库的所有写命令，所以数据库只需要读入并执行一遍AOF里面的命令就可以恢复数据库关闭前的状态。 image-20201018150745174 AOF重写：随着时间的流逝，AOF文件的内容会越来越多，不加以控制的话，会很容易超过体积最大限制造成影响。为了解决这个问题，Redis提供了AOF文件重写的功能。重写功能是通过读取当前数据库的状态来实现的。另外为了提高服务器的可用性，一般执行AOF重写的时候采用的是后台重写，以此防止阻塞的问题。但是使用子进程进行AOF文件的重写的时候，服务器会接受客户端的命令，而新的命令可能会造成数据库状态的修改，从而使得当前数据库状态和重写后的AOF文件保存的数据库状态不一致。为了解决这种数据不一致问题, Redis服务器设置了一个AOF重写缓冲区,这个缓冲区在服务器创建子进程之后开始使用,当 Redis服务器执行完一个写命令之后,它会同时将这个写命令发送给AOF缓冲区和AOF重写缓冲区,如图11-4所示。 image-20201018151801586 当子进程完成重写操作的时候，会向父进程发送信号，父进程此时会将AOF重写缓冲区的内容写入到新的AOF文件中，最后执行改名覆盖现有的AOF文件，实现新旧两个AOF文件的替换。 第十二章 事件 Redis服务器是一个事件驱动程序，服务器需要处理两类事件：文件事件，时间事件。文件事件是服务器对套接字的抽象，时间事件则是定时操作的抽象。 文件事件：Redis基于Reactor模式开发出了自己的网络事件处理器，这个处理器被称作为文件事件处理器，组成如下： image-20201018153202366 IO多路复用程序的实现，通过包装常见的select，epoll，evport和kqueue来实现的。 时间事件：分为定时事件和周期性事件。目前Redis中只是用了周期性事件，没有使用定时事件。服务器将所有时间事件都放在一个无序链表中,每当时间事件执行器运行时,它就遍历整个链表,査找所有已到达的时间事件,并调用相应的事件处理器。 持续运行的Redis服务器需要对自身的资源和状态进行检查和调整，从而可以确保服务器可以长期稳定的运行，这些定期操作被封装到redis.c/serverCron函数中执行。 事件的调度与执行： image-20201018154101415 由于时间事件在文件事件之后执行，并且事件之间不会出现抢占，所以时间事件的实际处理时间，通常会比时间事件设定的到达事件稍晚一些。 第十三章 客户端 Redis服务器会为每个客户端创建一个redis.h/redisClient结构，用于保存客户端的信息，Redis服务器中还会保存着一个clients的链表，用于保存所有和服务器相连接的客户端。 image-20201018162408381 客户端属性：通用属性和特定属性。有以下几种属性： 套接字描述符：fd，为客户端为-1，否则为大于-1的整数。 名字：name 标志：flags，记录了客户端的角色 输入缓冲区：querybuf，保存客户端发送的命令请求 命令和命令参数：argc和argv，服务器对querybuf解析后将参数的个数和参数存入这两个变量 命令的实现函数：cmd，当服务器解析出来命令之后，就可以查找对应的命令的实现函数，然后将其指针值复制到client中 输出缓冲区：buf[MAX_BYTES]，命令回复会保存在这里面 身份验证：authenticated，记录客户端是否通过了身份验证 时间：包含客户端创建的时间，最后一次和服务器互动的时间 客户端的创建和关闭：如果是普通的客户端，那么就会在clients链表后面追加上一个redisClient结构。服务器使用两种模式来限制客户端缓冲区的大小：硬性限制，软性限制。另外，处理Lua脚本的为客户端在服务器初始化时创建，知道服务器关闭，而AOF文件载入的时候的伪客户端则在载入结束之后关闭。 第十四章 服务器 命令请求的执行过程：客户端发送命令请求，服务器端读取命令请求，接下来分析命令，查找命令表获取命令实现函数，调用获取到的命令实现函数，命令实现函数执行完后执行后续的工作，最后将命令回复发送给客户端，客户端接受并且打印命令回复。 ServerCron函数每隔100毫秒执行一次，这个函数负责管理服务器的资源，并且保持服务器自身的良好运转。它具有以下功能： 更新服务器事件缓存 更新LRU时钟 更新服务器每秒执行命令次数 更新服务器内存峰值记录 处理SIGTERM信号 管理客户端的资源 管理数据库的资源 执行被延迟的BGREWRITEAOF 检查持久化操作的运行状态 将AOF缓冲区的内容写入AOF文件 关闭异步客户端 增加cronloops计数器的值 初始化服务器： 初始化服务器的状态结构 载入配置选项 初始化服务器数据结构 还原数据库状态 执行事件循环 第十五章 复制 在Redis中，用户通过命令SLAVEOF让一个服务器去复制另外一个服务器，被复制的服务器成为主服务器，对主服务器进行复制的服务器称为从服务器。 旧版复制功能的实现： 同步：将从服务器的数据库状态更新至主服务器当前所处的数据库状态。通过SYNC命令完成： 从服务器向主服务器发送SYNC命令 主服务器执行BGSAVE命令，后台生成RDB文件，同时用一个缓冲区记录从现在开始执行后的所有写命令 RDB生成后，将其发送给从服务器，从服务器加载RDB文件 从服务器加载完成后，主服务器将缓冲区里面的所有写命令发送给从服务器，保持一致性 image-20201023194004571 命令传播：当主服务的数据库状态被修改的时候，此时造成数据库状态的不一致性，通过命令传播让主从服务器状态重新回到一致状态。 旧版复制功能的缺陷：复制可以分为初次复制和断线后重复制，旧版复制功能并没有很好解决断线后重复制时间长的问题：只需要将断线后的命令发送给从服务器就行，不需要全部复制一遍。 新版复制功能的实现：新版复制功能使用PSYNC命令代替SYNC命令来执行同步操作。PSYNC命令有完整重同步和部分重同步两种模式，前者用于处理初次复制的情况，后者用于处理断线后重复制情况。 image-20201023194735822 部分重同步的实现： 主从服务器的复制偏移量：主服务器每次向从服务器传播N个字节的数据时，就将自己的复制偏移量加上N，从服务器每次收到主服务器传播过来的N个字节的数据时，就将自己的复制偏移量加上N，通过对比主从服务器的复制偏移量，可以很容易知道主从服务器是否处于一致状态 服务器的复制积压缓冲区：由主服务器维护的一个固定长度的先进先出的队列，默认1MB。当主服务器进行命令传播的时候，不仅将写命令发送到从服务器，还将写命令入队到复制积压缓冲区里面。当从服务器重新连接到主服务器的时候，根据offset偏移量之后的数据在缓冲区里面，执行部分重同步，否则执行完整重同步 服务器的运行ID：每个Redis服务器都有自己的运行ID。初次复制的时候，主服务器向从服务器发送自己的ID，从服务器重新连接的时候，通过ID和重连服务器的ID比对来判断是否重连到相同的主服务器 PSYNC命令的实现： image-20201023201400872 复制的实现： 设置主服务器的地址和端口 建立套接字连接 发送PING命令，检测网络是否通畅，不通畅的话断开并且重连 身份验证 发送端口信息 同步 命令传播 心跳检测：在命令传播阶段，从服务器默认会每秒一次的频率，向主服务器发送REPLCONF ACK &lt;replication_offset&gt;，其作用有： 检测主从服务器的网络连接状态 辅助实现min-slaves选项：可以防止主服务器在不安全的情况下执行写命令，如配置min-slaves-to-write为3，那么在从服务器的数量小于3时，拒接执行写命令 检测命令丢失：如果丢包的话，通过offset实现一致性 image-20201023202834446 第十六章 Sentinel Sentinel是Redis的高可用性解决方案：由一个或多个Sentinel实例组成的Sentinel系统可以监视任意多个主服务器，以及这些主服务器属下的所有从服务器，当被监视的主服务器进入下线的状态时，自动将下线服务器的某个从服务器升级为新的主服务器，然后由新的主服务器代替已经下线的主服务器处理命令请求，当原来的主服务器又重新上线的时候，将其设置为新的主服务器的一个从服务器。 启动并初始化Sentinel： 初始化服务器：Sentinel本质上是一个运行在特殊模式下的Redis服务器，但是初始化的时候不载入数据 使用Sentinel专用代码 初始化Sentinel状态：服务器会初始化一个sentinelState结构 初始化Sentinel的masters属性：键是被监视主服务器的名字，值是被监视主服务器对应的sentinelRedisInstance结构，每个这样的结构代表一个被Sentinel监视的实例，可以是主服务器，从服务器，或者另外一个Sentinel 创建连向主服务器的网络连接：会创建两个连接，一个是命令链接，另外一个是订阅连接 image-20201023205141243 获取主服务器信息：Sentinel没十秒一次的频率，向被监视的主服务器发送INFO命令，获取当前信息。当Sentinel分析INFo命令中包含的从服务器的信息的时候，会检查从服务器对应的实例是否存在于其主服务器实例slaves字典中，存在的话更新相关信息，否则的话创建新的实例 获取从服务器信息：Sentinel没十秒一次的频率，向被监视的从服务器发送INFO命令，获取当前信息。会对从服务器实例进行更新 向主服务器和从服务器发送信息：默认情况下，PUBLISH信息到主从服务器： image-20201023210209159 接收来自主从服务器的频道信息： image-20201023210251707 更新sentinels字典 创建连接其他Sentinel的命令连接 image-20201023210330916 检测主观下线行为：默认情况下，Sentinel每秒向其创建了命令连接的实例发送PING命令，通过PONG回复检测是否在线。如果一个实例在down-after-millseconds毫秒内，连续向Sentinel返回无效回复，那么Sentinel会修改这个实例的结构，将flags属性中的SRI_S_DOWN标示，表示其主观下线。 检查客观下线状态：当Sentinel讲一个服务器判断为主观下线之后，为了确保是否真的下线了，需要向其他监视了这个服务器的Sentinel进行询问，看他们是否也认为主服务器也已经进入了下线状态。当接收到足够的下线判断之后，Sentinel就会将从服务器判定为客观下线，并对主服务器执行故障转移操作。 选举领头Sentinel：当一个主服务器被判断为客观下线时，监视这个下线服务器的各个Sentinel会进行协商，选举出一个领头Snetinel，由领头Sentinel对下线服务器执行故障转移操作。 Raft算法的领头选举方法的实现。 故障转移： 选出新的主服务器 修改从服务器的复制目标 将旧的主服务器变为从服务器 第十七章 集群 Redis 集群是 Redis 提供的分布式数据库方案, 集群通过分片(sharding)来进行数据共享, 并提供复制和故障转移功能。 节点：一个 Redis 集群通常由多个节点(node)组成，在刚开始的时候，每个节点都是相互独立的，它们都处于一个只包含自已的集群当中，要组建一个真正可工作的集群，我们必须将各个独立的节点连接起来，构成一个包含多个节点的集群。 启动节点：节点实际上就是一个运行在集群模式下的 Redis 服务器，通过 cluster-enabled 选项进行配置。 集群数据结构：clusterNode 结构保存了一个节点的当前状态，CLusterState 结构记录了在当前节点的视角下，集群目前所处的状态 image-20201025134655648 CLUSTER MEET命令实现： image-20201025134751665 槽指派：Redis 集群通过分片的方式来保存数据库中的键值对：集群的整个数据库被分为16384个槽(slot)，数据库中的每个键都属于这16384个槽的其中一个，集群中的每个节点可以处理0个或最多16384个槽。当数据库中的16384个槽都有节点在处理时，集群处于上线状态(ok)；相反地，如果数据库中有任何一个槽没有得到处理，那么集群处于下线状态(fail)。 记录节点的槽指派信息：clusterNode 结构中的slots属性记录了节点当前负责处理那些槽： image-20201025135100354 slots数组是一个二进制数组，为 1 则该节点负责该槽，否则不负责该槽。 传播节点的槽指派信息：当节点 A 通过消息从节点 B 那里接收到节点 B 的 s1ots 数组时,节点 A 会在自已的 clusterState.nodes字典中査找节点 B 对应的 clusterNode 结构,并对结构中的 slots 数组进行保存或者更新 image-20201025135431790 记录集群所有槽的指派信息：clusterState 结构中的 slots 数组记录了集群中所有的 16384 个槽的指派信息 image-20201025135633822 CLUSTER ADDSLOTS命令的实现：首先改动 clusterState.slots 指针，将对应槽指向自己，然后修改clusterNode.slots 数组，将对应的 slots 置位。 在集群中执行命令：当集群的16384个槽都指派后，集群就会进入上线状态，这时客户端就可以向集群中的节点发送命令了。 image-20201025140106534 计算键输入哪个槽：CRC16(key) &amp; 16383 判断槽是否由当前节点负责处理：clusterState.slots[i] == clusterState.myself MOVED 错误：当节点发现键所在的槽不由自己处理的时候，返回MOVED &lt;SLOT&gt; &lt;ip&gt;:&lt;addr&gt;，客户端自动转向到对应的节点，再次发送命令 节点数据库的实现：集群节点保存键值对的方式和单机 Redis 服务器保存方式完全相同。唯一区别是节点只能使用0号数据库。另外，节点会使用 clusterState.slots_of_keys 跳跃表保存槽和键之间的关系，命令CLUSTER GETKEYSINSLOT &lt;slot&gt; &lt;count&gt;就是建立在该结构上的。 重新分片：Redis 集群的重新分片操作可以将任意数量已经指派给某个节点(源节点)的槽改为指派给另一个节点(目标节点)，并且相关槽所属的键值对也会从源节点被移动到目标节点。重新分片操作可以在线(online)进行,在重新分片的过程中，集群不需要下线，并且源节点和目标节点都可以继续处理命令请求。以下是对单个槽slot流程： image-20201025141051856 ASK错误：当执行分片期间，可能会存在这样一种情况：输入被迁移槽的一部分节点键值对保存在源节点里面，另外一部分保存在目标节点里面。此时响应客户端的命令如下： image-20201025141341148 CLSUTER SETSLOT IMPORTING命令的实现：clusterState.importing_slots_from 记录了当前节点正在从其他节点导入的槽。 CLUSTER SETSLOT MIGRATING命令的实现：clusterState.migrating_slots_to 数组记录当前节点正在迁移至其他节点的槽。 ASK错误：如果槽正在迁移，会发送ASK &lt;SLOT&gt; &lt;ip&gt;:&lt;port&gt; image-20201025142210744 image-20201025142157014 ASKING命令：打开发送该命令的客户端的 REDIS_ASKING 标示，该标示是一个一次性标示，当成功执行完一次命令的时候，此时该标示就会被移除。 image-20201025142144327 ASK错误和MOVED错误：前者发生在节点间迁移槽的时候，是一种临时措施，后者发生在槽的负责权已经从一个节点迁移到了另外一个节点。 复制与故障转移：Redis 集群中的节点分为主节点和从节点，主节点用于处理槽，从节点用于复制。故障转移措施和第十五章类似。 设置从节点：CLUSTER REPLICATE &lt;node_id&gt;让接受命令的节点成为 node_id 所指向节点的从节点。此时从节点中设置 clusterState.myself.slaveof 属性，主节点设置 clusterNode.slaves 属性。 故障检测：集群中每个节点定期向其他节点发送 PING 消息，以此检测是否在线，如果超时，标记为 PFAIL（probable fail）。在集群中超过半数的节点都认为某个主节点 PFAIL，那么这个节点会被标记为下线（FAIL），同时广播这条消息。 故障转移：从FAIl 的主节点的从节点里面选择一个作为主节点，然后将原来主节点的额槽指派给自己，新的主节点广播 PONG 信息，让其他的节点知道这个节点已经选为主节点，最后新的节点开始接收和负责处理自己的槽有关的命令请求，故障转移完成。 选举新的节点：Raft 算法。 消息：节点发送的消息一般有五种：MEET 消息，PING 消息，PONG 消息，FAIL 消息，PUBLISH 消息。 第十八章 发布与订阅 Redis 的发布与订阅的功能由 PUBLISH，SUBSCRIBE，PSUBSCRIB 等命令组成，每当有其他客户端向被订阅的频道发送消息时，频道的所有订阅者都会收到这个消息。 频道的订阅与退订：Redis 将所有订阅关系保存在RedisServer.pubsub_channels 字典里面，键是某个被订阅的频道，键值则是一个链表，保存着所有订阅这个频道的客户端。 image-20201019225402536 订阅频道：如果有频道已经在字典中，直接尾部插入订阅者，否则创建字典项，键为频道，键值为该客户端 退订频道：根据被退订的频道名字，从订阅者链表中删去客户端，如果此时订阅者链表为空，则删除对应的字典项 模式的订阅与退订：Redis 将所有模式的订阅关系保存在RedisServer.pubsub_patterns 属性里面。 image-20201019225859198 订阅模式：新建一个pubsubPattern结构，设置好client和pattern属性，然后将其加入到pubsub_patterns 链表的表尾。 退订模式：从pubsub_patterns中查找对应的被退订的pubsubPattern结构，然后将其删除。 发送消息：当执行PUBLISH &lt;channel&gt; &lt;message&gt;时候，服务器需要将消息message发送到对应的channel的所有订阅者，另外如果有模式匹配这个channel，那么需要将message发送给pattern模式的订阅者。 将消息发送给频道订阅者：从pubsub_channels 字典里面找到订阅者链表，然后将消息发送给名单上的所有客户端 将消息发送给模式订阅者：遍历pubsub_patterns链表，查找那些与channel频道匹配的模式，并且将消息发送到这些模式的客户端。 查看订阅消息：PUBSUB命令可以查看频道或者模式的相关信息 PUBSUB CHANNELS [pattern]：返回与pattern匹配的频道 PUBSUB NUMSUB [channel-1 channel-2]:返回频道对应订阅者的数量 PUBSUB NUMPAT: 返回当前服务器被订阅模式的数量 第十九章 事务 Redis通过MULTI，EXEC，WATCH等命令实现事务功能。在事务执行期间，服务器不会中断事务去执行其他客户端的请求，他会将事务中的命令执行完毕之后才去处理其他的客户端的请求。 事务的实现：包括三个部分，如下： 事务开始：MULTI命令的执行标志者事务的开始，会将执行该命令的客户端从非事务状态转换为事务状态 命令入队：当客户端处于事务状态之后，如果客户端发送的指令是EXEC，DISCARD，WATCH，MULTI命令之一，那么立即执行，否则的话将其放入事务队列之中，然后返回QUEUED回复。每个Redis客户端都有自己的事务状态 mstate，里面包含一个事务队列： image-20201020233625705 执行事务：当发送EXEC命令的时候，此时就开始执行遍历这个客户端的事务队列，执行其中的所有命令，然后将得到的回复返回给客户端。 WATCH命令的实现：WATCH命令是一个乐观锁，它可以在EXEC命令执行之前，监视任意数量的数据库键，并且在EXEC命令执行的时候，检查被监视的键中是否至少有一个已经被修改过了，如果是的话，服务器将拒绝执行事务，并向客户端返回执行失败的空回复。 WATCH命令监视数据库键：每个Redis数据库都保存着一个watched_keys字典，字典的键是某个数据库键，而字典的值则是一个链表，记录所有的监视相应数据库键的客户端： 1234typedef struct redisDb &#123; dict *watched_keys; // ...&#125; 监视机制的触发：所有对数据库进行修改的命令，在执行之后会调用touchWatchKey函数对监视字典进行检查，如果某个键被修改，会将对应的客户端REDIS_DIRTY_CAS标示打开，表示客户端的安全性已经被破坏。 判断事务是否安全：服务器根据客户端的REDIS_DIRTY_CAS标识来决定是否执行事务： image-20201021120010867 事务的ACID性质：在Redis中，事务总是具有原子性，一致性，隔离性，当Redis运行在某种持久化模式下，也具有持久性。 原子性：数据库将事务中的多个操作当作一个整体来执行,服务器要么就执行事务中的所有操作,要么就一个操作也不执行。 Redis的事务和传统的关系型数据库事务的最大区别在于, Redis不支持事务回滚机制 (rollback),即使事务队列中的某个命令在执行期间出现了错误,整个事务也会继续执行下去,直到将事务队列中的所有命令都执行完毕为止。 Redis的作者在事务功能的文档中解释说,不支持事务回滚是因为这种复杂的功能和 Redis追求简单高效的设计主旨不相符,并且他认为, Redis事务的执行时错误通常都是编 程错误产生的,这种错误通常只会出现在开发环境中,而很少会在实际的生产环境中出现 所以他认为没有必要为 Redis开发事务回滚功能。 一致性：如果数据库在执行事务之前是一致的，那么在事务执行之后，不论事务是否执行成功，数据库也应该仍然是一致的。Redis事务可能出错的地方有入队错误，执行错误，服务器宕机。 隔离性：即使数据库中有多个事务并发地执行,各个事务之间也不会互相影响,并且在并发状态下执行的事务和串行执行的事务产生的结果完全相同。 因为 Redis使用单线程的方式来执行事务(以及事务队列中的命令),并且服务器保证在执行事务期间不会对事务进行中断,因此, Redis的事务总是以串行的方式运行的,并且事务也总是具有隔离性的。 持久性：当一个事务执行完毕时,执行这个事务所得的结果已经被保存到永久性存储介质(比如硬盘)里面了, 即使服务器在事务执行完毕之后停机, 执行事务所得的结果也不会丢失。只有当服务器运行在AOF持久化模式下，并且appendfsync选项的值为ALWAYS的时候，这种配置下的事务是具有持久性的。 第二十一章 排序 SORT &lt;key&gt; 命令的实现：假设已经执行RPUSH numbers 3 1 2, 现在执行SORT numbers， 首先创建一个和numbers列表长度相同的数组，数组的每一项是redis.h/redisSortObject结构 遍历数组，将数组项的obj指针指向对应的列表项，一一对应 遍历数组，将obj指针指向的列表项转换为一个double类型的浮点数，存到u.score中 根据u.score中的值，进行升序排序 遍历数组，将各个数组项的obj指针指向的列表项返回给客户端 image-20201021165824509 redisSortObject结构如下： image-20201021165903649 ALPHA选项实现：字典序排序 ASC选项和DESC选项的实现：默认是升序排序，升序和降序排序只不过是排序算法使用的对比函数不同而已。 BY选项实现：默认情况下，SORT命令使用被排序键包含的元素作为排序的权重，元素本身决定了排序所处的位置。可以使用BY选项来改变这种情况。 带有ALPHA选项的BY选项的实现：BY选项默认权重值保存的值是数字值，如果是字符串值的话，需要在使用By选项的同时，配合使用ALPHA选项。 LIMIT选项的实现：LIMIT &lt;offset&gt; &lt;count&gt;。 GET选项的实现：更具被排序结果中的元素，查找相关的信息。 STORE选项的实现：STORE选项可以保存排序结果在指定的键里面 多个选项的执行顺序：排序，限制长度，获取外部键，保存排序结果集。 image-20201021171157237 除了GET选项外，改变选项的摆放顺序不会影响SORT命令执行这些选项的顺序。 第二十三章 慢查询日志 慢查询日志：记录执行时间超过指定时长的命令请求，用户可以通过该功能产生的日志来优化查询速度。服务器有两个和慢查询日志相关的选项： slowlog-log-slower-than: 指定执行时间上限，超过上限的会被记录到日志上 slowlog-max-len: 记录慢查询日志最大条数 慢查询日志记录的保存：服务器状态中有几个和慢查询日志有关的属性： image-20201021171836148 其中slowlog链表保存所有的慢查询日志。每个节点是一个slowlogEntry： image-20201021172036568 慢查询日志的阅览和删除：遍历查询和遍历删除 添加新日志：对慢查询日志进行头插法插入 第二十四章 监视器 通过执行MONITOR命令，客户端可以将自己变为一个监视器，实时接收并打印服务器当前处理的命令请求。 成为监视器：执行MONITOR命令后。客户端的REDIS_MONITOR标志会被打开，并且这个客户端会被添加到monitors链表的表尾。 image-20201021172607447 向监视器发送命令信息：服务器每次执行命令之前，都会调用replicationFeedMonitor函数，由这个函数将命令请求信息发送给各个监视器。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.zsstrike.top/tags/Redis/"}]},{"title":"使用 Travis CI持续部署博客","slug":"使用-Travis-CI持续部署博客","date":"2020-04-11T09:38:33.000Z","updated":"2020-12-17T10:23:43.573Z","comments":true,"path":"2020/04/11/使用-Travis-CI持续部署博客/","link":"","permalink":"http://blog.zsstrike.top/2020/04/11/%E4%BD%BF%E7%94%A8-Travis-CI%E6%8C%81%E7%BB%AD%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2/","excerpt":"本文主要介绍使用 Travis 来自动将我们的博客内容 push 到 Github Page 上，也就是所谓的持续集成/持续部署。","text":"本文主要介绍使用 Travis 来自动将我们的博客内容 push 到 Github Page 上，也就是所谓的持续集成/持续部署。 持续集成Travis CI 提供的是持续集成服务（Continuous Integration，简称 CI）。它绑定 Github 上面的项目，只要有新的代码，就会自动抓取。然后，提供一个运行环境，执行测试，完成构建，还能部署到服务器。 持续集成指的是只要代码有变更，就自动运行构建和测试，反馈运行结果。确保符合预期以后，再将新代码”集成”到主干。 持续集成的好处在于，每次代码的小幅变更，就能看到运行结果，从而不断累积小的变更，而不是在开发周期结束时，一下子合并一大块代码。 使用 Travis 持续部署博客我们的博客使用 Hexo 生成，博客的仓库是名是&lt;username&gt;.github.io。有两个分支，其中 master 分支用于放置我们的内容，而 hexo-project 分支用于存储我们的 hexo 工程文件。 首先登录到官网：travis-ci.org，接着点击右上角个人头像，选择博客的仓库，并且打开开关。一旦我们激活了这个仓库，那么 Travis 就能监听这个仓库的所有变化。 .travis.ymlTravis 要求项目的根目录下面，必须有一个.travis.yml文件。这是配置文件，指定了 Travis 的行为。该文件必须保存在 Github 仓库里面，一旦代码仓库有新的 Commit，Travis 就会去找这个文件，执行里面的命令。 对于我们的要求，我们只需要配置如下就行： 12345678910111213141516171819202122232425262728293031323334# 开发语言和版本language: node_jsnode_js: stable# 监听分支branches: only: hexo-project# 缓存 cache: directories: - node_modules before_install: - npm install -g hexo-cli# 安装依赖 install: - npm install - npm install hexo-deployer-git --save# 执行脚本 script: - hexo clean - hexo g# 将博客内容部署到 master 分支中after_success: - cd ./public - git init - git add --all . - git commit -m &quot;Travis CI Auto Build&quot; - git config user.name &quot;username&quot; - git config user.email &quot;emial&quot; - git push --quiet --force https://$&#123;GH_TOKEN&#125;@$&#123;GH_REF&#125; master:master# 设置环境变量 env: global: - GH_REF: github.com/&lt;username&gt;/&lt;username&gt;.github.io.git 其中，我们需要获取到&#123;GH_TOKEN&#125;，这是用于我们能够正常访问 Github API 的基础。 Tokens you have generated that can be used to access the GitHub API. 可以在用户-&gt;setting-&gt;Personal access token中创建 Token。 之后，在 Travis 网站相应的仓库中设置环境变量就可以了。 另外，如果想要获取 build 的状态图片，可以在 Travis 中将图片的 markdown 格式复制下来，放在 readme.md 中。","categories":[],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://blog.zsstrike.top/tags/CI-CD/"}]},{"title":"Nodejs 用户注册登录和授权处理","slug":"Nodejs-用户注册登录和授权处理","date":"2020-04-11T07:27:40.000Z","updated":"2020-12-17T10:23:43.509Z","comments":true,"path":"2020/04/11/Nodejs-用户注册登录和授权处理/","link":"","permalink":"http://blog.zsstrike.top/2020/04/11/Nodejs-%E7%94%A8%E6%88%B7%E6%B3%A8%E5%86%8C%E7%99%BB%E5%BD%95%E5%92%8C%E6%8E%88%E6%9D%83%E5%A4%84%E7%90%86/","excerpt":"本文介绍 Nodejs 搭配 Express 框架实现服务器中常见的功能：用户注册登录和授权的处理。","text":"本文介绍 Nodejs 搭配 Express 框架实现服务器中常见的功能：用户注册登录和授权的处理。 用户注册逻辑首先我们新建一个 Express 服务器： 12345678// server.jsconst express = require(&#x27;express&#x27;)const app = express()// 处理 POST 中的 Body 数据app.use(express.json())app.listen(3000) 接着使用 MongoDB 来新建一个 Collection 的 Model 对象： 12345678910111213141516171819202122232425// models.jsconst mongoose = require(&#x27;mongoose&#x27;)mongoose.connect(&#x27;mongodb://localhost:27017/test&#x27;, &#123; useNewUrlParser: true, useUnifiedTopology: true, useCreateIndex: true&#125;)const UserSchema = new mongoose.Schema(&#123; username: &#123; type: String, required: true, unique: true &#125;, password: &#123; type: String, required: true, &#125;&#125;)const User = mongoose.model(&#x27;User&#x27;, UserSchema)module.exports = &#123; User &#125; 其中，username是不能重复的。 接下来，创建用户注册请求路由： 123456789101112// server.jsapp.post(&#x27;/api/register&#x27;, (req, res) =&gt; &#123; const jsonData = req.body const user = new User(jsonData) user.save((err, user) =&gt; &#123; if(err)&#123; res.json(&#123;msg: &#x27;failed to save&#x27;&#125;) return console.log(err) &#125; res.json(user) &#125;)&#125;) 为了测试接口，可以下载 VSCode 扩展商店中REST Client，用于发送 HTTP 请求和检测响应的数据。 安装完成后，编写用户注册请求： 1234567891011// test.http@baseUrl=http://localhost:3000/api### 注册POST &#123;&#123;baseUrl&#125;&#125;/register HTTP/1.1Content-Type: application/json&#123; &quot;username&quot;: &quot;user1&quot;, &quot;password&quot;: &quot;password1&quot;&#125; 当发送请求后，可以得到服务器返回的数据。 但是这样的话，我们存入的用户密码是明文存储的，不是很安全，为此我们需要在数据存入的时候，对密码进行 hash 处理，在此使用bcrypt进行哈希： 1234567891011121314151617// models.jsconst bcrypt = require(&#x27;bcrypt&#x27;)const UserSchema = new mongoose.Schema(&#123; username: &#123; type: String, required: true, unique: true &#125;, password: &#123; type: String, required: true, set(val) &#123; // 存入的时候先进行 hash return bcrypt.hashSync(val, 10) &#125; &#125;&#125;) A library to help you hash passwords. Based on the Blowfish cipher. 用户登录接下来，创建用户登录请求路由： 123456789101112131415161718// server.jsapp.post(&#x27;/api/login&#x27;, (req, res) =&gt; &#123; const userData = req.body User.findOne(&#123; username: userData.username &#125;, (err, user) =&gt; &#123; if(err || !user) &#123; return res.status(422).json(&#123;msg: &#x27;非法用户名&#x27;&#125;) &#125; const isValid = bcrypt.compareSync(userData.password, user.password) if(!isValid)&#123; return res.status(422).json(&#123;msg: &#x27;密码错误&#x27;&#125;) &#125; res.json(&#123; user &#125;) &#125;)&#125;) 根据用户输入的密码和哈希处理的密码进行比对，判断用户输入的密码是否正确。这是基本的用户登录流程的处理。 但是我们希望用户登录之后能够保存这些状态，传统处理方法是：用户登陆成功后，产生 Session_ID 并且将其发送给客户端使其保存在 Cookie 中，之后客户端每次请求都携带 Session_ID。 在此，我们使用 JsonWebToken 来保存我们的数据，将其发送到客户端，客户端保存在 LocalStorage 中，根据其中的数据来保存用户的状态。 修改用户登录路由： 1234567891011121314151617181920212223app.post(&#x27;/api/login&#x27;, (req, res) =&gt; &#123; console.log(req.body) const userData = req.body User.findOne(&#123; username: userData.username &#125;, (err, user) =&gt; &#123; if(err || !user) &#123; return res.status(422).json(&#123;msg: &#x27;非法用户名&#x27;&#125;) &#125; const isValid = bcrypt.compareSync(userData.password, user.password) if(!isValid)&#123; return res.status(422).json(&#123;msg: &#x27;密码错误&#x27;&#125;) &#125; // jwt token const token = jwt.sign(&#123; id: user._id &#125;, SECRET) res.json(&#123; user, token &#125;) &#125;)&#125;) 注意，本文中为了演示，将SECRET硬编码进了 server.js 中，更实际的情况时我们将其保存在一个被 gitignore 的文件中，通过读取文件配置 SECRET。 用户授权当用户登录之后，客户端保存下来了 token 值，接下来假设用户想要获取个人信息，这只有在用户登录之后才有权限进行操作，为此就需要 token 的帮助了： 123### 获取个人信息GET &#123;&#123;baseUrl&#125;&#125;&#x2F;profile HTTP&#x2F;1.1Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IjVlOTE2OWVhYmNlZWNkM2I2YTI0NDg2OSIsImlhdCI6MTU4NjU4ODYyMn0.VZb_0Rlw27mAShcJCRpoURenfy8IoluGgQ-VDwkqyFM 上面是用户发送的请求，其中 Authorization 头部的格式如下： Authorization: &lt;type&gt; &lt;credentials&gt; 接下来处理这个请求路由： 123456789101112app.get(&#x27;/api/profile&#x27;, (req, res) =&gt; &#123; const tokenData = req.headers.authorization.split(&#x27; &#x27;).pop() const token = jwt.verify(tokenData, SECRET) User.findOne(&#123; _id: token.id &#125;, (err, user) =&gt; &#123; if(err)&#123; return res.json(&#123;msg: &#x27;error token&#x27;&#125;) &#125; res.json(&#123;msg: &#x27;your profile&#x27;, user: req.user&#125;) &#125;)&#125;) 这样我们就能够根据请求的 Authorization 头获取到用户的信息了。这就是用户授权的基本过程。 另外，如果有很多需要用户登陆之后操作，我们需要将用户验证这个操作转换成中间件的形式，这样就能够在多个路由中使用这个中间件了： 1234567891011121314151617const auth = (req, res, next) =&gt; &#123; const tokenData = req.headers.authorization.split(&#x27; &#x27;).pop() const token = jwt.verify(tokenData, SECRET) User.findOne(&#123; _id: token.id &#125;, (err, user) =&gt; &#123; if(err)&#123; return res.json(&#123;msg: &#x27;error token&#x27;&#125;) &#125; req.user = user next() &#125;)&#125;app.get(&#x27;/api/profile&#x27;, auth, (req, res) =&gt; &#123; res.json(&#123;msg: &#x27;your profile&#x27;, user: req.user&#125;)&#125;) 至于 token 的过期时间我们可以使用预定义的键exp来设置过期时间。","categories":[],"tags":[{"name":"Nodejs","slug":"Nodejs","permalink":"http://blog.zsstrike.top/tags/Nodejs/"}]},{"title":"Vim常用命令","slug":"Vim常用命令","date":"2020-02-23T03:38:22.000Z","updated":"2020-12-17T10:23:43.556Z","comments":true,"path":"2020/02/23/Vim常用命令/","link":"","permalink":"http://blog.zsstrike.top/2020/02/23/Vim%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"本文介绍Vim中常用的命令，帮助我们快速使用Vim处理文本文档。","text":"本文介绍Vim中常用的命令，帮助我们快速使用Vim处理文本文档。 基本操作在Vim中一共有三种模式：命令模式，插入模式和编辑模式（Visual mode）。 在命令模式下可以使用h，j，k，l来移动，分别表示左下上右四个移动方向。 命令模式下使用x来删除单个字符。 命令模式下使用u来撤销，使用CTRL-R来取消撤销。 使用:wq保存文件并且退出，使用:q!强制退出。 使用i在光标前插入字符，使用a在光标后插入字符。 删除一整行使用dd命令。 在光标下插入新行使用o命令，在光标上插入新行使用O命令。 有些命令可以先输入Count值，再输入命令，表示的意思就是执行命令Count次。9k表示上移9行，3x表示删除三个字符。 快速操作 单词间移动：w表示后移到单词的开头，b表示前移到单词的开头。 行首和行末移动：$表示移动到本行行尾，^表示移动到本行开头。 行内单个字符查找：fx表示查找从光标向右查找字符x，Fx表示从光标向左查找字符x。 移动到特定行：&lt;num&gt;G表示移动到num行，gg表示移动到第一行，G表示移动到最后一行。 显示行号：:set nu。 滑动窗口：CTRL-U上滑半个屏幕，CTRL-D下滑半个屏幕。 删除文本：dw删除单个单词，d$删除光标到末尾的文本。 修改文本：cw修改单个单词，c$修改光标到末尾的文本。 重复上次删除或者修改命令：.。 连接不同行内容到一行上：使用3J将三行内容移动到一行上。 替换单个字符：rx将光标字符修改为x字符。 修改大小写：~将字符进行大小写转换。 键盘宏：使用q[a-z]开始记录，再次q结束，调用键盘宏使用@[a-z]，用于重复执行复杂操作。 搜索 搜索文本：/string搜索string文本，使用n可以跳转到下一个被搜索到的文本。 取消高亮：被搜索到的文本会被高亮，当不再需要高亮的时候使用:noh命令。 反向搜索文本：?string反向搜索string文本，使用n跳转到下一个被搜索到的文本。 改变搜索反向：n跳转到下一个被搜索到的文本，N跳转到上一个被搜索到的文本。 正则搜索：^表示行首，$表示行尾，\\c忽略大小写，同样可以使用其他的正则表达式。 多窗口与多文件 粘贴文本：p命令可以在光标后粘贴被保存的文本，P命令可以在光标前粘贴被保存的文本。被保存的文本包括用x，d删除的文本。 标记：使用m[a-z]对所在位置进行标记，``a表示移动到刚刚被标记的位置，‘a`表示移动到刚刚被标记位置的行首。 查看标记：:marks查看所作的标记。 复制文本：y命令可以复制文本，Y或者yy命令可以一整行的文本。 打开新的文件：使用:vi file.txt打开file.txt文件。 打开多个文件：使用vim one.c two.c there.c。 在多文件下转换：:next跳转到下一个文件并且打开该文件，:previous跳转到上一个文件并且打开该文件，:first跳转到第一个文件，:last跳转到最后一个文件。 查看当前所在的文件：:args可以查看自己所处的文件（用[]包括起来）。 窗口 打开新的窗口：:[count] split [filename]水平打开一个新的窗口，大小为count值，文件是filename，:[count] vsplit [filename]垂直打开一个新的窗口，大小为count值，文件是filename。 窗口间移动：CTRL-W[hjkl]根据方向键改变窗口，CTRL-W CTRL-W在不同窗口间进行移动。 改变窗口大小：CTRL-W+增加窗口大小，CTRL-W-减小窗口大小，CTRL-W=使窗口大小相同。 基本的编辑模式 三种编辑模式：v字符编辑模式，V行编辑模式，CTRL-V矩形编辑模式。 删除文本：d删除所选的文本，D删除所选文本行（从光标到末尾）。 复制文本：y复制所选的文本，Y复制所选文本行（从光标到末尾）。 修改文本：c修改所选的文本，C修改所选的文本行（从光标到末尾）。 多行合并：J将所选的文本合并到一行。 缩进：&lt;和&gt;进行左缩进和右缩进。 矩形编辑模式下的特殊操作： 插入文本：Istring&lt;Esc&gt;在矩形前面插入string文本。 修改文本：cstring&lt;Esc&gt;修改矩形中的文本。 替换文本：rchar&lt;Esc&gt;替换矩形中的文本。 程序员相关指令 打开语法高亮：:syntax on。 设置文件的格式以适应语法高亮：:set filetype=c。 行缩进：&lt;&lt;或者是&gt;&gt;。 行缩进大小：:set shiftwidth=4。 设置缩进方式：:set (cindent|smartindent|autoindent)。 自动缩进大括号内的内容：=%。 查找单词：[CTRL-I全文查找单词，]CTRL-I从光标到文件末尾查找单词。 跳转到变量定义：gd跳转到局部变量定义，gD跳转到全局变量定义。 跳转到宏定义：[CTRL-D跳转到第一个宏定义，]CTRL-D跳转到下一个宏定义。 查看宏定义：[d查找显示第一个宏定义，]d从光标处开始查找宏定义。[D显示所有匹配的宏定义列表，]D显示光标后所有匹配的宏定义的列表。 查看匹配的括号对：%查找并且跳转到匹配的括号对上。 缩进代码块：&gt;%或者&lt;%。 自动补全：CTRL-P前向搜索补全词汇，CTRL-N后向搜索补全词汇。","categories":[],"tags":[{"name":"Vim","slug":"Vim","permalink":"http://blog.zsstrike.top/tags/Vim/"}]},{"title":"MySQL基本操作","slug":"MySQL基本操作","date":"2020-02-19T03:48:27.000Z","updated":"2020-12-17T10:23:43.486Z","comments":true,"path":"2020/02/19/MySQL基本操作/","link":"","permalink":"http://blog.zsstrike.top/2020/02/19/MySQL%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","excerpt":"本文介绍MySQL数据库的简单使用方法，包括数据库的启动和连接，以及数据的增删改查等。操作环境在CentOS 7中。","text":"本文介绍MySQL数据库的简单使用方法，包括数据库的启动和连接，以及数据的增删改查等。操作环境在CentOS 7中。 数据库连接和断开连接在进行数据库的连接之前，我们需要先启动数据库服务： 1shell&gt; systemctl start mysqld.service 数据库连接命令： 12shell&gt; mysql [-h host] -u root -pEnter password: ***** 缺省host就表示连接本地的mysql数据库。 数据库断开连接： 12mysql&gt; quitBye 查询连接到mysql后，可以查询版本信息，当前日期和时间和当前用户： 1234567mysql&gt; SELECT VERSION(), CURRENT_DATE, NOW(), USER();+-----------+--------------+---------------------+----------------+| VERSION() | CURRENT_DATE | NOW() | USER() |+-----------+--------------+---------------------+----------------+| 8.0.19 | 2020-02-19 | 2020-02-19 18:03:54 | root@localhost |+-----------+--------------+---------------------+----------------+1 row in set (0.00 sec) 创建和使用数据库查询当前服务器中的数据库有哪些： 1234567891011mysql&gt; SHOW DATABASES;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys || test |+--------------------+5 rows in set (0.01 sec) 使用某个数据库： 12mysql&gt; USE test;Database changed 在test数据库中创建的任何数据可能会被别人删除，可以让管理员执行以下命令使得只有你能使用某个数据库(menagerie)： 1mysql&gt; GRANT ALL ON menagerie.* TO &#x27;your_mysql_name&#x27;@&#x27;your_client_host&#x27;; 创建和选择数据库创建新的数据库： 12mysql&gt; CREATE DATABASE menagerie;Query OK, 1 row affected (0.00 sec) 选择数据库： 12mysql&gt; USE menagerie;Database changed 查询当前使用的数据库： 1234567mysql&gt; SELECT DATABASE();+------------+| DATABASE() |+------------+| menagerie |+------------+1 row in set (0.00 sec) 创建表格展示当前数据库中有哪些表格： 12mysql&gt; SHOW TABLES;Empty set (0.00 sec) 创建新的表格： 12345678mysql&gt; CREATE TABLE pet ( -&gt; name VARCHAR(20), -&gt; owner VARCHAR(20), -&gt; species VARCHAR(20), -&gt; sex CHAR(1), -&gt; birth DATE, -&gt; death DATE);Query OK, 0 rows affected (0.06 sec) 查看表格的表头和描述： 1234567891011mysql&gt; DESCRIBE pet;+---------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+---------+-------------+------+-----+---------+-------+| name | varchar(20) | YES | | NULL | || owner | varchar(20) | YES | | NULL | || species | varchar(20) | YES | | NULL | || sex | char(1) | YES | | NULL | || birth | date | YES | | NULL | || death | date | YES | | NULL | |+---------+-------------+------+-----+---------+-------+ 写入数据使用文件载入数据： 123mysql&gt; LOAD DATA LOCAL INFILE &#x27;/usr/local/src/pet.txt&#x27; INTO TABLE pet;Query OK, 8 rows affected, 1 warning (0.00 sec)Records: 8 Deleted: 0 Skipped: 0 Warnings: 1 pet.txt 的内容如下： 12345678Fluffy Harold cat f 1993-02-04 \\NClaws Gwen cat m 1994-03-17 \\NBuffy Harold dog f 1989-05-13 \\NFang Benny dog m 1990-08-27 \\NBowser Diane dog m 1979-08-31 1995-07-29Chirpy Gwen bird f 1998-09-11 \\NWhistler Gwen bird \\N 1997-12-09 Slim Benny snake m 1996-04-29 \\N 其中每条记录之间使用TAB分割，每个行结束符是\\r，\\N表示的是NULL值。 如果执行命令出错，可能需要先开启local_infile变量： 1mysql&gt; set global local_infile = &#x27;ON&#x27;; 然后通过下述命令进入mysql交互环境： 1$ mysql --local-infile=1 -u root -p 同样地，我们可以通过INSERT语句来插入数据： 123mysql&gt; INSERT INTO pet -&gt; VALUES (&#x27;Puffball&#x27;, &#x27;Diane&#x27;, &#x27;hamster&#x27;, &#x27;f&#x27;, &#x27;1999-3-30&#x27;, NULL);Query OK, 1 row affected (0.00 sec) 从表格中获取信息从表格获取信息的方式一般形式是： 123SELECT what_to_selectFROM which_tableWHERE conditions_to_satisfy; 选择全部数据： 1234567891011121314mysql&gt; SELECT * FROM pet;+----------+--------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+----------+--------+---------+------+------------+------------+| Fluffy | Harold | cat | f | 1993-02-04 | NULL || Claws | Gwen | cat | m | 1994-03-17 | NULL || Buffy | Harold | dog | f | 1989-05-13 | NULL || Fang | Benny | dog | m | 1990-08-27 | NULL || Bowser | Diane | dog | m | 1979-08-31 | 1995-07-29 || Chirpy | Gwen | bird | f | 1998-09-11 | NULL || Whistler | Gwen | bird | NULL | 1997-12-09 | 0000-00-00 || Slim | Benny | snake | m | 1996-04-29 | NULL || Puffball | Diane | hamster | f | 1999-03-30 | NULL |+----------+--------+---------+------+------------+------------+ 从表中我们发现Bowser的birth值不太正确，我们可以 在pet.txt中修改文件，然后清空pet表格，接着载入数据 12mysql&gt; DELETE FROM pet;mysql&gt; LOAD DATA LOCAL INFILE &#x27;/usr/local/src/pet.txt&#x27; INTO TABLE pet; 使用UPDATE语句： 12mysql&gt; UPDATE pet SET birth = &#x27;1989-08-31&#x27; WHERE name = &#x27;Bowser&#x27;;Query OK, 1 row affected (0.00 sec) 有条件地选择可以通过WHERE来实现： 123456789101112131415161718192021222324252627282930313233343536373839mysql&gt; SELECT * FROM pet WHERE name = &#x27;Bowser&#x27;;+--------+-------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+--------+-------+---------+------+------------+------------+| Bowser | Diane | dog | m | 1989-08-31 | 1995-07-29 |+--------+-------+---------+------+------------+------------+mysql&gt; SELECT * FROM pet WHERE birth &gt;= &#x27;1998-1-1&#x27;;+----------+-------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+----------+-------+---------+------+------------+-------+| Chirpy | Gwen | bird | f | 1998-09-11 | NULL || Puffball | Diane | hamster | f | 1999-03-30 | NULL |+----------+-------+---------+------+------------+-------+mysql&gt; SELECT * FROM pet WHERE species = &#x27;dog&#x27; AND sex = &#x27;f&#x27;;+-------+--------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+-------+--------+---------+------+------------+-------+| Buffy | Harold | dog | f | 1989-05-13 | NULL |+-------+--------+---------+------+------------+-------+mysql&gt; SELECT * FROM pet WHERE species = &#x27;snake&#x27; OR species = &#x27;bird&#x27;;+----------+-------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+----------+-------+---------+------+------------+-------+| Chirpy | Gwen | bird | f | 1998-09-11 | NULL || Whistler | Gwen | bird | NULL | 1997-12-09 | NULL || Slim | Benny | snake | m | 1996-04-29 | NULL |+----------+-------+---------+------+------------+-------+mysql&gt; SELECT * FROM pet WHERE (species = &#x27;cat&#x27; AND sex = &#x27;m&#x27;) OR (species = &#x27;dog&#x27; AND sex = &#x27;f&#x27;);+-------+--------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+-------+--------+---------+------+------------+-------+| Claws | Gwen | cat | m | 1994-03-17 | NULL || Buffy | Harold | dog | f | 1989-05-13 | NULL |+-------+--------+---------+------+------------+-------+ 选择特定的列可以通过SELECT语句实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051mysql&gt; SELECT name, birth FROM pet;+----------+------------+| name | birth |+----------+------------+| Fluffy | 1993-02-04 || Claws | 1994-03-17 || Buffy | 1989-05-13 || Fang | 1990-08-27 || Bowser | 1989-08-31 || Chirpy | 1998-09-11 || Whistler | 1997-12-09 || Slim | 1996-04-29 || Puffball | 1999-03-30 |+----------+------------+mysql&gt; SELECT owner FROM pet;+--------+| owner |+--------+| Harold || Gwen || Harold || Benny || Diane || Gwen || Gwen || Benny || Diane |+--------+mysql&gt; SELECT DISTINCT owner FROM pet;+--------+| owner |+--------+| Benny || Diane || Gwen || Harold |+--------+mysql&gt; SELECT name, species, birth FROM pet WHERE species = &#x27;dog&#x27; OR species = &#x27;cat&#x27;;+--------+---------+------------+| name | species | birth |+--------+---------+------------+| Fluffy | cat | 1993-02-04 || Claws | cat | 1994-03-17 || Buffy | dog | 1989-05-13 || Fang | dog | 1990-08-27 || Bowser | dog | 1989-08-31 |+--------+---------+------------+ 数据间的排序可以通过ORDER BY实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445mysql&gt; SELECT name, birth FROM pet ORDER BY birth;+----------+------------+| name | birth |+----------+------------+| Buffy | 1989-05-13 || Bowser | 1989-08-31 || Fang | 1990-08-27 || Fluffy | 1993-02-04 || Claws | 1994-03-17 || Slim | 1996-04-29 || Whistler | 1997-12-09 || Chirpy | 1998-09-11 || Puffball | 1999-03-30 |+----------+------------+mysql&gt; SELECT name, birth FROM pet ORDER BY birth DESC;+----------+------------+| name | birth |+----------+------------+| Puffball | 1999-03-30 || Chirpy | 1998-09-11 || Whistler | 1997-12-09 || Slim | 1996-04-29 || Claws | 1994-03-17 || Fluffy | 1993-02-04 || Fang | 1990-08-27 || Bowser | 1989-08-31 || Buffy | 1989-05-13 |+----------+------------+mysql&gt; SELECT name, species, birth FROM pet ORDER BY species, birth DESC;+----------+---------+------------+| name | species | birth |+----------+---------+------------+| Chirpy | bird | 1998-09-11 || Whistler | bird | 1997-12-09 || Claws | cat | 1994-03-17 || Fluffy | cat | 1993-02-04 || Fang | dog | 1990-08-27 || Bowser | dog | 1989-08-31 || Buffy | dog | 1989-05-13 || Puffball | hamster | 1999-03-30 || Slim | snake | 1996-04-29 |+----------+---------+------------+ 和时间相关的处理： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081mysql&gt; SELECT name, birth, CURDATE(), TIMESTAMPDIFF(YEAR,birth,CURDATE()) AS age FROM pet;+----------+------------+------------+------+| name | birth | CURDATE() | age |+----------+------------+------------+------+| Fluffy | 1993-02-04 | 2003-08-19 | 10 || Claws | 1994-03-17 | 2003-08-19 | 9 || Buffy | 1989-05-13 | 2003-08-19 | 14 || Fang | 1990-08-27 | 2003-08-19 | 12 || Bowser | 1989-08-31 | 2003-08-19 | 13 || Chirpy | 1998-09-11 | 2003-08-19 | 4 || Whistler | 1997-12-09 | 2003-08-19 | 5 || Slim | 1996-04-29 | 2003-08-19 | 7 || Puffball | 1999-03-30 | 2003-08-19 | 4 |+----------+------------+------------+------+mysql&gt; SELECT name, birth, CURDATE(), TIMESTAMPDIFF(YEAR,birth,CURDATE()) AS age FROM pet ORDER BY name;+----------+------------+------------+------+| name | birth | CURDATE() | age |+----------+------------+------------+------+| Bowser | 1989-08-31 | 2003-08-19 | 13 || Buffy | 1989-05-13 | 2003-08-19 | 14 || Chirpy | 1998-09-11 | 2003-08-19 | 4 || Claws | 1994-03-17 | 2003-08-19 | 9 || Fang | 1990-08-27 | 2003-08-19 | 12 || Fluffy | 1993-02-04 | 2003-08-19 | 10 || Puffball | 1999-03-30 | 2003-08-19 | 4 || Slim | 1996-04-29 | 2003-08-19 | 7 || Whistler | 1997-12-09 | 2003-08-19 | 5 |+----------+------------+------------+------+mysql&gt; SELECT name, birth, CURDATE(), TIMESTAMPDIFF(YEAR,birth,CURDATE()) AS age FROM pet ORDER BY age;+----------+------------+------------+------+| name | birth | CURDATE() | age |+----------+------------+------------+------+| Chirpy | 1998-09-11 | 2003-08-19 | 4 || Puffball | 1999-03-30 | 2003-08-19 | 4 || Whistler | 1997-12-09 | 2003-08-19 | 5 || Slim | 1996-04-29 | 2003-08-19 | 7 || Claws | 1994-03-17 | 2003-08-19 | 9 || Fluffy | 1993-02-04 | 2003-08-19 | 10 || Fang | 1990-08-27 | 2003-08-19 | 12 || Bowser | 1989-08-31 | 2003-08-19 | 13 || Buffy | 1989-05-13 | 2003-08-19 | 14 |+----------+------------+------------+------+mysql&gt; SELECT name, birth, death, TIMESTAMPDIFF(YEAR,birth,death) AS age FROM pet WHERE death IS NOT NULL ORDER BY age;+--------+------------+------------+------+| name | birth | death | age |+--------+------------+------------+------+| Bowser | 1989-08-31 | 1995-07-29 | 5 |+--------+------------+------------+------+mysql&gt; SELECT name, birth, MONTH(birth) FROM pet;+----------+------------+--------------+| name | birth | MONTH(birth) |+----------+------------+--------------+| Fluffy | 1993-02-04 | 2 || Claws | 1994-03-17 | 3 || Buffy | 1989-05-13 | 5 || Fang | 1990-08-27 | 8 || Bowser | 1989-08-31 | 8 || Chirpy | 1998-09-11 | 9 || Whistler | 1997-12-09 | 12 || Slim | 1996-04-29 | 4 || Puffball | 1999-03-30 | 3 |+----------+------------+--------------+mysql&gt; SELECT name, birth FROM pet WHERE MONTH(birth) &#x3D; 5;+-------+------------+| name | birth |+-------+------------+| Buffy | 1989-05-13 |+-------+------------+ 模式匹配的规则如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748mysql&gt; SELECT * FROM pet WHERE name LIKE &#39;b%&#39;;+--------+--------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+--------+--------+---------+------+------------+------------+| Buffy | Harold | dog | f | 1989-05-13 | NULL || Bowser | Diane | dog | m | 1989-08-31 | 1995-07-29 |+--------+--------+---------+------+------------+------------+mysql&gt; SELECT * FROM pet WHERE name LIKE &#39;%fy&#39;;+--------+--------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+--------+--------+---------+------+------------+-------+| Fluffy | Harold | cat | f | 1993-02-04 | NULL || Buffy | Harold | dog | f | 1989-05-13 | NULL |+--------+--------+---------+------+------------+-------+mysql&gt; SELECT * FROM pet WHERE name LIKE &#39;%w%&#39;;+----------+-------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+----------+-------+---------+------+------------+------------+| Claws | Gwen | cat | m | 1994-03-17 | NULL || Bowser | Diane | dog | m | 1989-08-31 | 1995-07-29 || Whistler | Gwen | bird | NULL | 1997-12-09 | NULL |+----------+-------+---------+------+------------+------------+mysql&gt; SELECT * FROM pet WHERE name LIKE &#39;_____&#39;;+-------+--------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+-------+--------+---------+------+------------+-------+| Claws | Gwen | cat | m | 1994-03-17 | NULL || Buffy | Harold | dog | f | 1989-05-13 | NULL |+-------+--------+---------+------+------------+-------+mysql&gt; SELECT * FROM pet WHERE REGEXP_LIKE(name, &#39;^b&#39;);+--------+--------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+--------+--------+---------+------+------------+------------+| Buffy | Harold | dog | f | 1989-05-13 | NULL || Bowser | Diane | dog | m | 1979-08-31 | 1995-07-29 |+--------+--------+---------+------+------------+------------+mysql&gt; SELECT * FROM pet WHERE REGEXP_LIKE(name, &#39;fy$&#39;);+--------+--------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+--------+--------+---------+------+------------+-------+| Fluffy | Harold | cat | f | 1993-02-04 | NULL || Buffy | Harold | dog | f | 1989-05-13 | NULL |+--------+--------+---------+------+------------+-------+ 统计查询数目的条数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677mysql&gt; SELECT COUNT(*) FROM pet;+----------+| COUNT(*) |+----------+| 9 |+----------+mysql&gt; SELECT owner, COUNT(*) FROM pet GROUP BY owner;+--------+----------+| owner | COUNT(*) |+--------+----------+| Benny | 2 || Diane | 2 || Gwen | 3 || Harold | 2 |+--------+----------+mysql&gt; SELECT species, COUNT(*) FROM pet GROUP BY species;+---------+----------+| species | COUNT(*) |+---------+----------+| bird | 2 || cat | 2 || dog | 3 || hamster | 1 || snake | 1 |+---------+----------+mysql&gt; SELECT sex, COUNT(*) FROM pet GROUP BY sex;+------+----------+| sex | COUNT(*) |+------+----------+| NULL | 1 || f | 4 || m | 4 |+------+----------+mysql&gt; SELECT species, sex, COUNT(*) FROM pet GROUP BY species, sex;+---------+------+----------+| species | sex | COUNT(*) |+---------+------+----------+| bird | NULL | 1 || bird | f | 1 || cat | f | 1 || cat | m | 1 || dog | f | 1 || dog | m | 2 || hamster | f | 1 || snake | m | 1 |+---------+------+----------+mysql&gt; SELECT species, sex, COUNT(*) FROM pet WHERE species &#x3D; &#39;dog&#39; OR species &#x3D; &#39;cat&#39; GROUP BY species, sex;+---------+------+----------+| species | sex | COUNT(*) |+---------+------+----------+| cat | f | 1 || cat | m | 1 || dog | f | 1 || dog | m | 2 |+---------+------+----------+mysql&gt; SELECT species, sex, COUNT(*) FROM pet WHERE sex IS NOT NULL GROUP BY species, sex;+---------+------+----------+| species | sex | COUNT(*) |+---------+------+----------+| bird | f | 1 || cat | f | 1 || cat | m | 1 || dog | f | 1 || dog | m | 2 || hamster | f | 1 || snake | m | 1 |+---------+------+----------+ 使用多个表格： 123456789101112131415161718192021222324252627mysql&gt; LOAD DATA LOCAL INFILE &#39;event.txt&#39; INTO TABLE event;mysql&gt; SELECT pet.name, TIMESTAMPDIFF(YEAR,birth,date) AS age, remark FROM pet INNER JOIN event ON pet.name &#x3D; event.name WHERE event.type &#x3D; &#39;litter&#39;;+--------+------+-----------------------------+| name | age | remark |+--------+------+-----------------------------+| Fluffy | 2 | 4 kittens, 3 female, 1 male || Buffy | 4 | 5 puppies, 2 female, 3 male || Buffy | 5 | 3 puppies, 3 female |+--------+------+-----------------------------+mysql&gt; SELECT p1.name, p1.sex, p2.name, p2.sex, p1.species FROM pet AS p1 INNER JOIN pet AS p2 ON p1.species &#x3D; p2.species AND p1.sex &#x3D; &#39;f&#39; AND p1.death IS NULL AND p2.sex &#x3D; &#39;m&#39; AND p2.death IS NULL;+--------+------+-------+------+---------+| name | sex | name | sex | species |+--------+------+-------+------+---------+| Fluffy | f | Claws | m | cat || Buffy | f | Fang | m | dog |+--------+------+-------+------+---------+ 从数据库或者表格中获取相关信息查询当前所有的数据库： 1mysql&gt; SHOW DATABASES; 查询当前数据库中含有的表格： 1mysql&gt; SHOW TABLES; 查询某个表格的表头和属性： 1mysql&gt; DESCRIBE pet; 查询当前所在的数据库： 1mysql&gt; SELECT DATABASE(); 使用脚本运行MySQL指令运行脚本的指令如下： 12shell&gt; mysql -u root -p &lt; batch-fileEnter password: ***** 可以通过如下方法来查看或者将输出保存到文件： 12shell&gt; mysql -u root -p &lt; batch-file | moreshell&gt; mysql -u root -p &lt; batch-file &gt; mysql.out 在batch模式和交互模式输出的内容会有不同，可以使用-t参数来得到交互模式下的输出；想要在输出语句中包含执行的指令，可以使用-v参数。 1582114003008 也可以在连接mysql后通过source或者\\来运行脚本： 12mysql&gt; source batch-filemysql&gt; \\ filename 常用的查询语句首先创建shop表单： 12345mysql&gt; CREATE TABLE shop( -&gt; article INT UNSIGNED DEFAULT &#39;0000&#39; NOT NULL, -&gt; dealer CHAR(20) DEFAULT &#39;&#39; NOT NULL, -&gt; price DECIMAL(16, 2) DEFAULT &#39;0.00&#39; NOT NULL, -&gt; PRIMARY KEY(article, dealer)); 接着插入数据： 123mysql&gt; INSERT INTO shop VALUES -&gt; (1, &#39;A&#39;, 3.45), (1, &#39;B&#39;, 3.99), (2, &#39;A&#39;, 10.99), (3, &#39;B&#39;, 1.45), -&gt; (3, &#39;C&#39;, 1.69), (3, &#39;D&#39;, 1.25), (4, &#39;D&#39;, 19.95); 最大的article值： 1mysql&gt; SELECT MAX(ARTICLE) FROM shop; 找到价格最大的记录： 1mysql&gt; SELECT * FROM shop WHERE price &#x3D; (SELECT MAX(price) FROM shop); 找到每种物品的最大价格： 12mysql&gt; SELECT article, MAX(price) FROM shop -&gt; GROUP BY article; 对每种物品，找到最贵价格的dealer： 123456mysql&gt; SELECT article, dealer, price FROM shop s1 WHERE price&#x3D;(SELECT MAX(s2.price) FROM shop s2 WHERE s1.article &#x3D; s2.article) ORDER BY article; 使用用户定义的变量： 12mysql&gt; SELECT @min_price:=MIN(price),@max_price:=MAX(price) FROM shop;mysql&gt; SELECT * FROM shop WHERE price=@min_price OR price=@max_price; 使用AUTO_INCREMENT我们可以创建animals的表格： 12345CREATE TABLE animals ( id MEDIUMINT NOT NULL AUTO_INCREMENT, name CHAR(30) NOT NULL, PRIMARY KEY (id)); 然后在插入的时候可以不用设置id的值： 1INSERT INTO animals VALUES (NULL, &#39;dog&#39;), (NULL, &#39;pig&#39;); MySQL备份和恢复SQL文件格式备份全部数据库： 1shell&gt; mysqldump -u root -p --all-databases &gt; dump.sql 备份某些特定的数据库： 1shell&gt; mysqldump -u root -p --databases db1 db2 &gt; dump.sql 备份某个数据库的某些表格： 1shell&gt; mysqldump -u root -p db1 tb1 tb2 &gt; dump.sql 恢复文件内容： 1shell&gt; mysql -u root -p &lt; dump.sql 对于那些SQL文件中没有事先选择数据库的，可以先进入mysql，然后使用： 12mysql&gt; USE db1;mysql&gt; source dump.sql; 使用分割文本格式备份数据库： 1shell&gt; mysqldump -u root -p --tab&#x3D;&#x2F;dir_name db1 如果报错：Got error: 1290: The MySQL server is running with the –secure-file-priv option so it cannot execute this statement when executing ‘SELECT INTO OUTFILE’，我们可以使用如下方法来解决： mysql&gt; show global variables like &#39;%secure%&#39; 查看secure-file-priv对用的目录，然后将上面的dirname改为对应的目录就可以。 恢复文件内容： 12shell&gt; mysql -u root -p db1 &lt; t1.sqlshell&gt; mysqlimport db1 t1.txt 或者在mysql环境下： 12mysql&gt; USE db1;mysql&gt; LOAD DATA INFILE &#39;t1.txt&#39; INTO TABLE t1;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.zsstrike.top/tags/MySQL/"}]},{"title":"CentOS 7安装常用软件方法","slug":"CentOS-7安装常用软件方法","date":"2020-02-18T08:53:47.000Z","updated":"2020-12-17T10:23:43.484Z","comments":true,"path":"2020/02/18/CentOS-7安装常用软件方法/","link":"","permalink":"http://blog.zsstrike.top/2020/02/18/CentOS-7%E5%AE%89%E8%A3%85%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E6%96%B9%E6%B3%95/","excerpt":"本文将会在CentOS 7的情况下安装一下常用的开发软件，主要记录在软件安装中遇到的问题和解决问题的方法。","text":"本文将会在CentOS 7的情况下安装一下常用的开发软件，主要记录在软件安装中遇到的问题和解决问题的方法。 概述由于国内的网络等原因，国外的一些资源或者被墙，或者是网络连接的速度慢，这个时候就需要我们使用镜像等网络资源来提高自己获取资源的速度。 实例MySQL 8.0安装CentOS 7中可能已经预安装了Mariadb，我们首先可以查询一下是否安装了Mariadb，如果安装了就直接卸载这个数据库： 12rpm -qa | grep mariadb*rpm -e --nodeps mariadb* 接下来下载MySQL官方的Yum Repository并且进行安装，注意具体的版本可以自己选择： 123wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm #根据版本选择rpm -ivh mysql-community-release-el7-5.noarch.rpmyum install mysql-server # 安装 但是由于网络原因，资源下载速率很慢，这个时候我们可以根据输出信息来决定下载的包。可以在清华镜像源中下载相应的包，然后按照依赖的关系依次安装。 1582026273567 成功安装完成后，我们使用systemctl start mysqld.service来启动MySQL，然后通过下面命令登录： 1mysql -u root -p # 无密码登录，输入密码行回车就行 进入到了mysql后，首先赋予用户密码： 12mysql&gt; ALTER user &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;123456&#x27;;mysql&gt; FLUSH PRIVILEGES; 如果执行第一步报错，说密码太简单：ERROR 1819 (HY000): Your password does not satisfy the current policy requirements。我们可以设置密码的规则： 12mysql&gt; set global validate_password.policy=0;mysql&gt; set global validate_password.length=1; 需要注意的是，在MySQL 5.7中应该按照下列方法设置： 12mysql&gt; set global validate_password_policy=0;mysql&gt; set global validate_password_length=1; Python 3.7安装在CentOS 7中，安装Python 3.7的步骤通常如下： 123456789101112131415# 安装相关编译工具yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel# 下载安装包并且解压wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tar.xztar -xvJf Python-3.7.0.tar.xz# 编译安装cd Python-3.7.0./configuremake &amp;&amp; make install# 检验是否成功安装python3 -Vpip3 -V 问题的关键点在于python.org被GFW墙了，根本不能下载Python源码。为此，我们可以在淘宝镜像上先下载源码包，然后按照上述方法安装就行。 pip2安装CentOS 7中默认安装了Python 2.7，但是没有预安装pip2命令，使用下面的方法安装就行： 123456# 先安装EPEL(Extra Packages for Enterprise Linux)源yum -y install epel-release# 接下来安装pip2yum install python-pip# 检验安装是否成功pip2 -V 总结遇到外网下载资源不佳的情况下，可以考虑使用国内的镜像源，根据自己下载的软件版本和系统的架构选择相应的软件下载下来，然后编译安装就行。","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.zsstrike.top/tags/Linux/"},{"name":"MySQL","slug":"MySQL","permalink":"http://blog.zsstrike.top/tags/MySQL/"},{"name":"Python","slug":"Python","permalink":"http://blog.zsstrike.top/tags/Python/"}]},{"title":"Nginx 使用教程","slug":"Nginx-使用教程","date":"2020-02-17T04:27:05.000Z","updated":"2020-12-17T10:23:43.488Z","comments":true,"path":"2020/02/17/Nginx-使用教程/","link":"","permalink":"http://blog.zsstrike.top/2020/02/17/Nginx-%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/","excerpt":"本章学习如何在 CentOS 7下使用 Nginx 来搭建反向代理和配置动静分离以及负载均衡过程的步骤。","text":"本章学习如何在 CentOS 7下使用 Nginx 来搭建反向代理和配置动静分离以及负载均衡过程的步骤。 Nginx 基本概念Nginx简介Nginx是以一个高性能的HTTP和反向代理服务器，特点是内存占用小，并发能力强，事实上Nginx的并发能力确实在同类型的网页服务器中表现良好。Nginx专为性能优化而开发，性能是其最重要的考量，实现上非常注重效率，能够经受高负载的考验，有报告表明它能支持高达50000个并发连接数。 反向代理在介绍反向代理之前，我们先介绍一下正向代理。正向代理是一个位于客户端和目标服务器之间的代理服务器(中间服务器)。为了从原始服务器取得内容，客户端向代理服务器发送一个请求，并且指定目标服务器，之后代理向目标服务器转交并且将获得的内容返回给客户端。正向代理的情况下客户端必须要进行一些特别的设置才能使用。正向代理实际上代理的是用户。 1581916914941 反向代理正好相反。对于客户端来说，反向代理就好像目标服务器。并且客户端不需要进行任何设置。客户端向反向代理发送请求，接着反向代理判断请求走向何处，并将请求转交给客户端，使得这些内容就好似他自己一样，一次客户端并不会感知到反向代理后面的服务，也因此不需要客户端做任何设置，只需要把反向代理服务器当成真正的服务器就好了。 1581916934765 负载均衡在因特网中，用户对服务器的访问并发量是很高的，通常单个服务器不可能完成对用户的响应。此时我们可以增加服务器的数量，然后将请求分发到各个服务器上，将原来请求集中到单个服务器上的情况改为将请求分发到多个服务器上，将负载分发到不同的服务器上，这也就是通常所说的负载均衡。 动静分离动静分离是指在web服务器架构中，将静态页面与动态页面或者静态内容接口和动态内容接口分开不同系统访问的架构设计方法，进而提升整个服务访问性能和可维护性。 Nginx基本使用Nginx安装安装Nginx需要先安装该软件的依赖：pcre，openssl，zlib，最后安装Nginx。可通过如下命令安装： 1yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel 接下来安装Nginx。首先下载nginx的源码包： 1wget http://nginx.org/download/nginx-1.16.2.tar.gz 接下来，解压配置编译安装就行： 1234tar -zxvf nginx-1.16.2.tar.gzcd nginx-1.16.2./configuremake &amp;&amp; make install 至此，Nginx安装成功，转到/usr/local/nginx/sbin目录下，查看版本号： 1./nginx -v Nginx常用命令 启动Nginx：./nginx 如果没有修改Nginx的配置文件，我们现在就可以在内网中通过访问该服务器的地址获得Nginx的主页，如果没有看到Nginx的主页，此时需要配置防火墙开放80端口。 1581928110804 查看开放的端口：firewall-cmd --list-all 设置开放的端口：firewall-cmd --add-port=80/tcp --permanent 设置开放的服务：firewall-cmd --add-service=http --permanent 设置之后重启防火墙：firewall-cmd --reload 停止Nginx：./nginx -s stop 重加载Nginx：./nginx -s reload Nginx配置文件配置文件位于/usr/local/nginx/conf/nginx.conf，配置文件可以划分为三个部分： 全局块：配置服务器整体运行的配置指令 从配置文件开始到events块之间的内容，主要会设置一些影响 nginx 服务器整体运行的配置指令，主要包括配置运行 Nginx 服务器的用户（组）、允许生成的 worker process 数，进程 PID 存放路径、日志存放路径和类型以及配置文件的引入等。 1581927474071 上面的第一行配置的就是worker进程的数目，进程数目越大，相应的并发能力也就也强，通常将它的值设置为CPU的核数目。 events块：影响 Nginx 服务器与用户的网络连接 events 块涉及的指令主要影响 Nginx 服务器与用户的网络连接，常用的设置包括是否开启对多 worker进程下的网络连接进行序列化，是否允许同时接收多个网络连接，选取哪种事件驱动模型来处理连接请求，每个 worker进程可以同时支持的最大连接数等。 1581927674793 上面的例子表示的是一个worker进程支持的最大连接数。 http块 这算是 Nginx 服务器配置中最频繁的部分，代理、缓存和日志定义等绝大多数功能和第三方模块的配置都在这里。需要注意的是http块也可以进一步划分为http全局块核server块。 1581927895940 http全局块：http 全局块配置的指令包括文件引入、MIME-TYPE 定义、日志自定义、连接超时时间、单链接请求数上限等。 server块：这块和虚拟主机有密切关系，虚拟主机从用户角度看，和一台独立的硬件主机是完全一样的，该技术的产生是为了节省互联网服务器硬件成本。每个 http 块可以包括多个 server 块，而每个 server 块就相当于一个虚拟主机。而每个 server 块也分为全局 server 块，以及可以同时包含多个 locaton 块。 全局server块：最常见的配置是本虚拟机主机的监听配置和本虚拟主机的名称或 IP 配置。 location块：一个 server 块可以配置多个 location 块。这块的主要作用是基于 Nginx 服务器接收到的请求字符串（例如 server_name/uri-string），对虚拟主机名称（也可以是 IP 别名）之外的字符串（例如 前面的 /uri-string）进行匹配，对特定的请求进行处理。地址定向、数据缓存和应答控制等功能，还有许多第三方模块的配置也在这里进行。 Nginx配置实例反向代理 1 实现效果：打开浏览器，在浏览器地址栏输入地址server.test.com，跳转到 liunx 系统 tomcat 主页 面中。 准备工作 在CentOS中安装tomcat，使用默认的端口8080启动服务，即进入tomcat的bin目录中，运行./startup.sh启动tomcat服务器 修改防火墙，使其对外开放8080端口： 12firewall-cmd --add-port=8080/tcp --permanentfirewall-cmd --reload 在windows中通过浏览器ip:8080访问tomcat服务器 具体配置 打开windows的host文件，添加192.168.85.129 server.test.com。 在nginx进行请求转发的配置 1581939678141 接着重启以下nginx：./nginx -s reload。然后在浏览器中输入server.test.com，就可以得到tomcat的网页： 反向代理2 实现效果：使用nginx反向代理，根据访问的路径跳转到不同端口的服务中，nginx监听端口9001，当访问ip:9001/edu直接跳转到127.0.0.1:8080，当访问ip:9001/vod，直接跳转到127.0.0.1:8081。 准备工作 准备两个tomcat服务器，一个配置在8080端口，一个配置在8081端口。修改conf/server.xml里面的两个端口，使得两个tomcat能够同时运行起来。 创建文件夹和测试文件。创建的文件夹和文件等会在被解析的时候用到。 具体配置 找到nginx配置文件，进行反向代理配置： 1581940620095 防火墙对外开放9001，8080和8081端口。 测试如下： 1581943507419 1581943526797 负载均衡 实现效果：浏览器地址栏输入地址ip:9002/edu/，使得后台的服务器均匀负载，将请求平均分发到8080和8081端口的服务器上。 准备工作 准备两台服务器，一台8080，另外一台8081 在两台tomcat服务器的webapps目录，创建edu文件夹，同时在edu文件里面创建index.html 具体配置 找到nginx配置文件，进行负载均衡的配置 1581944619600 防火墙开放9002端口。 测试如下： 1581944727364 Nginx分配服务器策略 轮询（默认）：每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器 down 掉，能自动剔除。 权重（weight）：weight 代表权重默认为 1,权重越高被分配的客户端越多。 img ip_hash：每个请求按访问 ip 的 hash 结果分配，这样每个访客固定访问一个后端服务器。 img fair：按后端服务器的响应时间来分配请求，响应时间短的优先分配。 img 动静分离 动静分离：Nginx 动静分离简单来说就是把动态跟静态请求分开，不能理解成只是单纯的把动态页面和静态页面物理分离。严格意义上说应该是动态请求跟静态请求分开，可以理解成使用 Nginx处理静态页面，Tomcat 处理动态页面。动静分离从目前实现角度来讲大致分为两种，一种是纯粹把静态文件独立成单独的域名，放在独立的服务器上，也是目前主流推崇的方案；另外一种方法就是动态跟静态文件混合在一起发布，通过 nginx 来分开。通过 location 指定不同的后缀名实现不同的请求转发。通过 expires 参数设置，可以使浏览器缓存过期时间，减少与服务器之前的请求和流量。具体 Expires 定义：是给一个资源设定一个过期时间，也就是说无需去服务端验证，直接通过浏览器自身确认是否过期即可，所以不会产生额外的流量。此种方法非常适合不经常变动的资源。（如果经常更新的文件，不建议使用 Expires 来缓存），我这里设置 3d，表示在这 3 天之内访问这个 URL，发送一个请求，比对服务器该文件最后更新时间没有变化，则不会从服务器抓取，返回状态码 304，如果有修改，则直接从服务器重新下载，返回状态码 200。 img 准备工作：在CentOS中，创建/static/www，/static/image文件夹，接着放入静态文件。 Nginx配置： 1581946029645 autoindex能够为目录下的文件自动创建索引。 效果： 1581946053128 1581946068998 高可用集群下面将使用keepalived实现服务器的高可用性。详细的讲解见Nginx原理一节。 1581947505380 两台Nginx服务器 准备两台服务器，在此地址是192.168.85.129和192.168.85.130。然后再在两台服务器上安装Nginx软件。 keepalived软件 使用以下命令： 1yum install keepalived -y 安装完成后，可以在/etc/keepalived中找到配置文件。 完成高可用的配置 修改/etc/keepalived/keepalived.conf 1234567891011121314151617181920212223242526! Configuration File for keepalivedglobal_defs &#123; router_id LVS_DEVEL&#125;vrrp_script chk_http_port &#123; script &quot;&#x2F;usr&#x2F;local&#x2F;src&#x2F;nginx_check.sh&quot; interval 2 weight 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens33 virtual_router_id 51 priority 90 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; track_script &#123; chk_http_port &#125; virtual_ipaddress &#123; 192.168.85.120 &#125;&#125; 在/usr/local/src/中添加nginx_check.sh文件 123456789#!/bin/bashA=`ps -C nginx --no-header | wc -l`if [ $A -eq 0 ];then /usr/local/nginx/sbin/nginx sleep 2 if [`ps -C nginx --no-header | wc -l` -eq 0];then killall keepalived fifi 测试 启动nginx：./nginx 启动keepalived：systemctl start keepalived.service 1581993867642 关闭主服务器上的nginx，再次访问虚拟地址 1581993920791 Nginx原理Nginx在启动时会以daemon形式在后台运行，采用多进程+异步非阻塞IO事件模型来处理各种连接请求。多进程模型包括一个master进程，多个worker进程，一般worker进程个数是根据服务器CPU核数来决定的。master进程负责管理Nginx本身和其他worker进程。 img Master进程作用是读取并验证配置文件nginx.conf，管理worker进程；每一个Worker进程都维护一个线程（避免线程切换），处理连接和请求；注意Worker进程的个数由配置文件决定，一般和CPU个数相关（有利于进程切换），配置几个就有几个Worker进程。 Nginx热部署的方式：修改配置文件nginx.conf后，重新生成新的worker进程，当然会以新的配置进行处理请求，而且新的请求必须都交给新的worker进程，至于老的worker进程，等把那些以前的请求处理完毕后，kill掉即可。 Nginx的高并发：Nginx采用了Linux的epoll模型，epoll模型基于事件驱动机制，它可以监控多个事件是否准备完毕，如果OK，那么放入epoll队列中，这个过程是异步的。worker只需要从epoll队列循环处理即可。 Nginx的高可用性：Keepalived是一个高可用解决方案，主要是用来防止服务器单点发生故障，可以通过和Nginx配合来实现Web服务的高可用（其实，Keepalived不仅仅可以和Nginx配合，还可以和很多其他服务配合）。Keepalived+Nginx实现高可用的思路：第一：请求不要直接打到Nginx上，应该先通过Keepalived（这就是所谓虚拟IP，VIP）第二：Keepalived应该能监控Nginx的生命状态（提供一个用户自定义的脚本，定期检查Nginx进程状态，进行权重变化,，从而实现Nginx故障切换） img","categories":[],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.zsstrike.top/tags/Nginx/"}]},{"title":"训练和评估","slug":"训练和评估","date":"2020-02-15T04:18:10.000Z","updated":"2020-12-17T10:23:43.712Z","comments":true,"path":"2020/02/15/训练和评估/","link":"","permalink":"http://blog.zsstrike.top/2020/02/15/%E8%AE%AD%E7%BB%83%E5%92%8C%E8%AF%84%E4%BC%B0/","excerpt":"本节主要从两方面学习模型的训练和评估：使用内建的API进行训练和评估或者是自定义函数实现训练和评估。不管使用哪种方法，不同方式构建的模型的训练和评估方式是一样的。","text":"本节主要从两方面学习模型的训练和评估：使用内建的API进行训练和评估或者是自定义函数实现训练和评估。不管使用哪种方法，不同方式构建的模型的训练和评估方式是一样的。 使用内建API 当我们使用内建的API来进行训练和评估时，我们传入的数据必须是Numpy arrays或者是tf.data.Dataset对象，在接下来的几个小节里，我们将会使用MNIST数据集作为示例。 内建API总览首先创建一个模型，如下： 123456789from tensorflow import kerasfrom tensorflow.keras import layersinputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x)outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x)model = keras.Model(inputs=inputs, outputs=outputs) 接下来，我们定义一个如下一个数据集： 1234567891011121314(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()# Preprocess the data (these are Numpy arrays)x_train = x_train.reshape(60000, 784).astype(&#x27;float32&#x27;) / 255x_test = x_test.reshape(10000, 784).astype(&#x27;float32&#x27;) / 255y_train = y_train.astype(&#x27;float32&#x27;)y_test = y_test.astype(&#x27;float32&#x27;)# Reserve 10,000 samples for validationx_val = x_train[-10000:]y_val = y_train[-10000:]x_train = x_train[:-10000]y_train = y_train[:-10000] 然后配置训练参数： 12345model.compile(optimizer=keras.optimizers.RMSprop(), # Optimizer # Loss function to minimize loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), # List of metrics to monitor metrics=[&#x27;sparse_categorical_accuracy&#x27;]) 接下来按照小批次的数目（batch_size）来训练这个模型，迭代整个数据集次数通过epochs设置： 1234567891011print(&#x27;# Fit model on training data&#x27;)history = model.fit(x_train, y_train, batch_size=64, epochs=3, # We pass some validation for # monitoring validation loss and metrics # at the end of each epoch validation_data=(x_val, y_val))print(&#x27;\\nhistory dict:&#x27;, history.history)&gt;&gt; history dict: &#123;&#x27;loss&#x27;: [0.34013055738687514, 0.15638909303188325, 0.11687878879904746], &#x27;sparse_categorical_accuracy&#x27;: [0.90308, 0.95404, 0.96512], &#x27;val_loss&#x27;: [0.18770194243788718, 0.13478265590667723, 0.11865641107037664], &#x27;val_sparse_categorical_accuracy&#x27;: [0.9454, 0.9615, 0.9672]&#125; 返回的对象记录了训练过程中损失值（loss value）和度量值（metrics）。下面的代码用于评估和预测： 12345678910# Evaluate the model on the test data using `evaluate`print(&#x27;\\n# Evaluate on test data&#x27;)results = model.evaluate(x_test, y_test, batch_size=128)print(&#x27;test loss, test acc:&#x27;, results)# Generate predictions (probabilities -- the output of the last layer)# on new data using `predict`print(&#x27;\\n# Generate predictions for 3 samples&#x27;)predictions = model.predict(x_test[:3])print(&#x27;predictions shape:&#x27;, predictions.shape) 定义损失函数，评价指标和优化器为了训练模型，我们需要定义损失函数，评价指标和优化器。我们可以再模型编译期间传入这些参数： 123model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#x27;sparse_categorical_accuracy&#x27;]) 注意，metrics参数必须是一个列表，可以传入多个评价指标。对于含有多个输出的模型，我们可以分别为其定义损失函数，评价指标和优化器。同时，一些默认的参数值我们也可以使用字符串。为了重用，我们定义如下代码： 1234567891011121314def get_uncompiled_model(): inputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;) x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs) x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x) outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x) model = keras.Model(inputs=inputs, outputs=outputs) return modeldef get_compiled_model(): model = get_uncompiled_model() model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#x27;sparse_categorical_accuracy&#x27;]) return model 内建的损失函数，评价指标和优化器内建优化器：SGD，RMSprop，Adam；内建损失函数：MeanSquareError，KLDivergence，CosineSimilarity；内建评估指标：AUC，Precision，Recall。 自定义损失函数有两种方法来自定义我们的损失函数，第一种是定义一个函数，接受y_true和y_pred参数： 1234567def basic_loss_function(y_true, y_pred): return tf.math.reduce_mean(tf.abs(y_true - y_pred))model.compile(optimizer=keras.optimizers.Adam(), loss=basic_loss_function)model.fit(x_train, y_train, batch_size=64, epochs=3) 另外一种方法是构造tf.keras.losses.Loss的子类，并且实现以下两个方法： __init__：接受传向损失函数的参数 call(self, y_true, y_pred)：用于计算模型的损失 传向__init__的参数可以被call方法调用。以下方法实现实现了BinaryCrossEntropy损失函数： 12345678910111213141516171819202122class WeightedBinaryCrossEntropy(keras.losses.Loss): &quot;&quot;&quot; Args: pos_weight: Scalar to affect the positive labels of the loss function. weight: Scalar to affect the entirety of the loss function. from_logits: Whether to compute loss from logits or the probability. reduction: Type of tf.keras.losses.Reduction to apply to loss. name: Name of the loss function. &quot;&quot;&quot; def __init__(self, pos_weight, weight, from_logits=False, reduction=keras.losses.Reduction.AUTO, name=&#x27;weighted_binary_crossentropy&#x27;): super().__init__(reduction=reduction, name=name) self.pos_weight = pos_weight self.weight = weight self.from_logits = from_logits def call(self, y_true, y_pred): ce = tf.losses.binary_crossentropy( y_true, y_pred, from_logits=self.from_logits)[:,None] ce = self.weight * (ce*(1-y_true) + self.pos_weight*ce*(y_true)) return ce 由于数据集由10个类别，但我们使用的是二元损失，所以我们只考虑每个类别的预测，这样就能基于二元损失来计算了。首先创建独热码： 1one_hot_y_train = tf.one_hot(y_train.astype(np.int32), depth=10) 接下俩训练模型： 123456789model = get_uncompiled_model()model.compile( optimizer=keras.optimizers.Adam(), loss=WeightedBinaryCrossEntropy( pos_weight=0.5, weight = 2, from_logits=True))model.fit(x_train, one_hot_y_train, batch_size=64, epochs=5) 自定义评估指标可以通过创建Metric来实现自定义的评价指标，需要实现下列四种方法： __init__：用于创建状态变量 update_state(self, y_true, y_pred, sample_weight=None)：用于更新状态 result(self)：使用状态变量计算最终结果 reset_states(self)：重新初始化状态 下面是一个实现了CategoricalTruePositive评价指标： 123456789101112131415161718192021class CategoricalTruePositives(keras.metrics.Metric): def __init__(self, name=&#x27;categorical_true_positives&#x27;, **kwargs): super(CategoricalTruePositives, self).__init__(name=name, **kwargs) self.true_positives = self.add_weight(name=&#x27;tp&#x27;, initializer=&#x27;zeros&#x27;) def update_state(self, y_true, y_pred, sample_weight=None): y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1)) values = tf.cast(y_true, &#x27;int32&#x27;) == tf.cast(y_pred, &#x27;int32&#x27;) values = tf.cast(values, &#x27;float32&#x27;) if sample_weight is not None: sample_weight = tf.cast(sample_weight, &#x27;float32&#x27;) values = tf.multiply(values, sample_weight) self.true_positives.assign_add(tf.reduce_sum(values)) def result(self): return self.true_positives def reset_states(self): # The state of the metric will be reset at the start of each epoch. self.true_positives.assign(0.) 下面是使用评价指标的代码： 123456model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[CategoricalTruePositives()])model.fit(x_train, y_train, batch_size=64, epochs=3) 处理非常规的损失函数和评价指标可以通过y_pred和y_true来计算损失函数和评价指标，然而，并非对所有的损失函数和评价指标都是如此。比如，一个正则化的损失函数可能需要某个层的激励值，而这个激励值并非是模型的输出。处理此类问题，我们可以再自定义层中的call方法中加入self.add_loss(loss_value): 123456789101112131415161718192021222324class ActivityRegularizationLayer(layers.Layer): def call(self, inputs): self.add_loss(tf.reduce_sum(inputs) * 0.1) return inputs # Pass-through layer.inputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs)# Insert activity regularization as a layerx = ActivityRegularizationLayer()(x)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x)outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x)model = keras.Model(inputs=inputs, outputs=outputs)model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))# The displayed loss will be much higher than before# due to the regularization component.model.fit(x_train, y_train, batch_size=64, epochs=1) 同样，对于评价指标也是如此： 12345678910111213141516171819202122232425262728class MetricLoggingLayer(layers.Layer): def call(self, inputs): # The `aggregation` argument defines # how to aggregate the per-batch values # over each epoch: # in this case we simply average them. self.add_metric(keras.backend.std(inputs), name=&#x27;std_of_activation&#x27;, aggregation=&#x27;mean&#x27;) return inputs # Pass-through layer.inputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs)# Insert std logging as a layer.x = MetricLoggingLayer()(x)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x)outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x)model = keras.Model(inputs=inputs, outputs=outputs)model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))model.fit(x_train, y_train, batch_size=64, epochs=1) 再函数式API中，我们可以通过model.add_loss(loss_tensor)和model.add_metric(metric_tensor, name, aggregation)来实现： 1234567891011121314151617inputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;)x1 = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs)x2 = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x1)outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x2)model = keras.Model(inputs=inputs, outputs=outputs)model.add_loss(tf.reduce_sum(x1) * 0.1)model.add_metric(keras.backend.std(x1), name=&#x27;std_of_activation&#x27;, aggregation=&#x27;mean&#x27;)model.compile(optimizer=keras.optimizers.RMSprop(1e-3), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))model.fit(x_train, y_train, batch_size=64, epochs=1) 自动设置验证集再第一个实例中，我们使用validation_data来手动设置验证集。其实我们还可以使用validation_split参数来定义我们验证集的比例，需要注意的是，验证集在fit之前选取原数据集的前$ x% $比例的数据作为验证集。validation_split参数只能在训练Numpy数据集时使用： 12model = get_compiled_model()model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1, steps_per_epoch=1) 从Datasets中训练和评估在TF2中，tf.data下的API用于加载数据和数据预处理。我们可以直接将Dataset的实例传给fit,evaluate,predoct等函数： 1234567891011121314151617181920model = get_compiled_model()# First, let&#x27;s create a training Dataset instance.# For the sake of our example, we&#x27;ll use the same MNIST data as before.train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))# Shuffle and slice the dataset.train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)# Now we get a test dataset.test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))test_dataset = test_dataset.batch(64)# Since the dataset already takes care of batching,# we don&#x27;t pass a `batch_size` argument.model.fit(train_dataset, epochs=3)# You can also evaluate or predict on a dataset.print(&#x27;\\n# Evaluate&#x27;)result = model.evaluate(test_dataset)dict(zip(model.metrics_names, result)) 注意Dataset在每次迭代结束后都会被重置，以此让我们在下次迭代中可以重新使用。如果我们想要定义每次迭代的步数，我们可以使用take参数。在达到指定的步数时，Dataset不会重置，除非它已经被遍历完了： 12345678model = get_compiled_model()# Prepare the training datasettrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)# Only use the 100 batches per epoch (that&#x27;s 64 * 100 samples)model.fit(train_dataset.take(100), epochs=3) 使用测试数据集可以给fit函数传入validation_data: 1234567891011model = get_compiled_model()# Prepare the training datasettrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)# Prepare the validation datasetval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))val_dataset = val_dataset.batch(64)model.fit(train_dataset, epochs=3, validation_data=val_dataset) 同样，如果我们定义每次迭代时使用验证集的次数，可以定义validation_steps: 1234567891011121314model = get_compiled_model()# Prepare the training datasettrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)# Prepare the validation datasetval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))val_dataset = val_dataset.batch(64)model.fit(train_dataset, epochs=3, # Only run validation using the first 10 batches of the dataset # using the `validation_steps` argument validation_data=val_dataset, validation_steps=10) 注意，此时不管测试数据集是否遍历完，都会被重置。 其他输入格式的数据除了Numpy中的数组和TF2中的Dataset对象，我们还可以使用Pandas的dataframs，或者是Python的generator（能够yield小批次数据）。 总体来说，对于少量数据，可以在内存中保存的，推荐使用Numpy中array，否则使用TF2中Dataset对象。 使用样本权重和类标权重我们可以在使用fit方法的时候传入样本的权重和类标的权重： 当使用Numpy数据时：通过sample_weight和class_weight参数 当使用TF2的Dataset时：让其返回一个这样的元组：(input_batch, target_batch, sample_weight_batch) 下面是使用Numpy数据进行训练的例子： 1234567891011121314151617181920212223import numpy as npclass_weight = &#123;0: 1., 1: 1., 2: 1., 3: 1., 4: 1., # Set weight &quot;2&quot; for class &quot;5&quot;, # making this class 2x more important 5: 2., 6: 1., 7: 1., 8: 1., 9: 1.&#125;print(&#x27;Fit with class weight&#x27;)model.fit(x_train, y_train, class_weight=class_weight, batch_size=64, epochs=4)# Here&#x27;s the same example using `sample_weight` instead:sample_weight = np.ones(shape=(len(y_train),))sample_weight[y_train == 5] = 2.print(&#x27;\\nFit with sample weight&#x27;)model = get_compiled_model()model.fit(x_train, y_train, sample_weight=sample_weight, batch_size=64, epochs=4) 下面是使用Dataset数据集的例子： 12345678910111213sample_weight = np.ones(shape=(len(y_train),))sample_weight[y_train == 5] = 2.# Create a Dataset that includes sample weights# (3rd element in the return tuple).train_dataset = tf.data.Dataset.from_tensor_slices( (x_train, y_train, sample_weight))# Shuffle and slice the dataset.train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)model = get_compiled_model()model.fit(train_dataset, epochs=3) 将数据传入多输入输出模型考虑这样一个模型： 12345678910111213141516171819from tensorflow import kerasfrom tensorflow.keras import layersimage_input = keras.Input(shape=(32, 32, 3), name=&#x27;img_input&#x27;)timeseries_input = keras.Input(shape=(None, 10), name=&#x27;ts_input&#x27;)x1 = layers.Conv2D(3, 3)(image_input)x1 = layers.GlobalMaxPooling2D()(x1)x2 = layers.Conv1D(3, 3)(timeseries_input)x2 = layers.GlobalMaxPooling1D()(x2)x = layers.concatenate([x1, x2])score_output = layers.Dense(1, name=&#x27;score_output&#x27;)(x)class_output = layers.Dense(5, name=&#x27;class_output&#x27;)(x)model = keras.Model(inputs=[image_input, timeseries_input], outputs=[score_output, class_output]) 这个模型有两个输入两个输出。在模型编译期间，我们传入不同的损失函数： 1234model.compile( optimizer=keras.optimizers.RMSprop(1e-3), loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy(from_logits=True)]) 同样可以传入不同的评价指标： 1234567model.compile( optimizer=keras.optimizers.RMSprop(1e-3), loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy(from_logits=True)], metrics=[[keras.metrics.MeanAbsolutePercentageError(), keras.metrics.MeanAbsoluteError()], [keras.metrics.CategoricalAccuracy()]]) 由于我们已经为输出层赋予了 名字，我们可以使用字典的方式传递参数： 1234567model.compile( optimizer=keras.optimizers.RMSprop(1e-3), loss=&#123;&#x27;score_output&#x27;: keras.losses.MeanSquaredError(), &#x27;class_output&#x27;: keras.losses.CategoricalCrossentropy(from_logits=True)&#125;, metrics=&#123;&#x27;score_output&#x27;: [keras.metrics.MeanAbsolutePercentageError(), keras.metrics.MeanAbsoluteError()], &#x27;class_output&#x27;: [keras.metrics.CategoricalAccuracy()]&#125;) 同样的，我们可以定义损失权重： 12345678model.compile( optimizer=keras.optimizers.RMSprop(1e-3), loss=&#123;&#x27;score_output&#x27;: keras.losses.MeanSquaredError(), &#x27;class_output&#x27;: keras.losses.CategoricalCrossentropy(from_logits=True)&#125;, metrics=&#123;&#x27;score_output&#x27;: [keras.metrics.MeanAbsolutePercentageError(), keras.metrics.MeanAbsoluteError()], &#x27;class_output&#x27;: [keras.metrics.CategoricalAccuracy()]&#125;, loss_weights=&#123;&#x27;score_output&#x27;: 2., &#x27;class_output&#x27;: 1.&#125;) 我们可以为某个输出定义损失函数，而另外一个输出不定义损失函数： 123456789# List loss versionmodel.compile( optimizer=keras.optimizers.RMSprop(1e-3), loss=[None, keras.losses.CategoricalCrossentropy(from_logits=True)])# Or dict loss versionmodel.compile( optimizer=keras.optimizers.RMSprop(1e-3), loss=&#123;&#x27;class_output&#x27;:keras.losses.CategoricalCrossentropy(from_logits=True)&#125;) 传入训练数据集的方式和上述介绍的方式差不多： 123456789101112131415161718192021model.compile( optimizer=keras.optimizers.RMSprop(1e-3), loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy(from_logits=True)])# Generate dummy Numpy dataimg_data = np.random.random_sample(size=(100, 32, 32, 3))ts_data = np.random.random_sample(size=(100, 20, 10))score_targets = np.random.random_sample(size=(100, 1))class_targets = np.random.random_sample(size=(100, 5))# Fit on listsmodel.fit([img_data, ts_data], [score_targets, class_targets], batch_size=32, epochs=3)# Alternatively, fit on dictsmodel.fit(&#123;&#x27;img_input&#x27;: img_data, &#x27;ts_input&#x27;: ts_data&#125;, &#123;&#x27;score_output&#x27;: score_targets, &#x27;class_output&#x27;: class_targets&#125;, batch_size=32, epochs=3) 而Dataset的使用方式如下： 123456train_dataset = tf.data.Dataset.from_tensor_slices( (&#123;&#x27;img_input&#x27;: img_data, &#x27;ts_input&#x27;: ts_data&#125;, &#123;&#x27;score_output&#x27;: score_targets, &#x27;class_output&#x27;: class_targets&#125;))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)model.fit(train_dataset, epochs=3) 使用回调回调对象可以在不同的时间点（每轮迭代的开始，每个批次的结束，每个迭代的结束）被调用来实现不同的功能。回调对象可以被传入fit: 1234567891011121314151617model = get_compiled_model()callbacks = [ keras.callbacks.EarlyStopping( # Stop training when `val_loss` is no longer improving monitor=&#x27;val_loss&#x27;, # &quot;no longer improving&quot; being defined as &quot;no better than 1e-2 less&quot; min_delta=1e-2, # &quot;no longer improving&quot; being further defined as &quot;for at least 2 epochs&quot; patience=2, verbose=1)]model.fit(x_train, y_train, epochs=20, batch_size=64, callbacks=callbacks, validation_split=0.2) 内建的回调对象 ModelCheckpoint: 定时保存模型检查点 EarlyStopping: 当评估指数没有改进的时候提前停止 TensorBoard: 记录模型的参数值 CSVLogger: 将模型的损失值和评价指标保存到CSV文件中 自建回调对象我们可以通过继承Callback对象实现自定义的回调对象，回调对象可以通过self.model获取相关联的模型，下面是一个实例： 1234567class LossHistory(keras.callbacks.Callback): def on_train_begin(self, logs): self.losses = [] def on_batch_end(self, batch, logs): self.losses.append(logs.get(&#x27;loss&#x27;)) 保存模型的检查点当我们的训练集很大的时候，我们需要定期保存模型的检查点，最简单的方式是使用ModelCheckpoint回调： 123456789101112131415161718model = get_compiled_model()callbacks = [ keras.callbacks.ModelCheckpoint( filepath=&#x27;mymodel_&#123;epoch&#125;&#x27;, # Path where to save the model # The two parameters below mean that we will overwrite # the current checkpoint if and only if # the `val_loss` score has improved. save_best_only=True, monitor=&#x27;val_loss&#x27;, verbose=1)]model.fit(x_train, y_train, epochs=3, batch_size=64, callbacks=callbacks, validation_split=0.2) 使用学习速率表深度学习中一个常见的训练模式是递减我们的学习速率，学习速率递减可以实静态的或者是动态的。 将学习速率表传给优化器我们可以将一个静态的学习速率表通过参数传递给优化器： 12345678initial_learning_rate = 0.1lr_schedule = keras.optimizers.schedules.ExponentialDecay( initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True)optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule) 内建的学习速率表还有：ExponentialDecay, PiecewiseConstantDecay, PolynomialDecay, and InverseTimeDecay。 使用回调实现动态学习速率表由于优化器不能获取评估指标，所以动态的学习速率表不能通过内建的学习速率表实现。但是，我们可以通过回调来实现动态学习速率表，因为回调能够获取所有的评价指标。实际上，这已经内建在ReduceLROnPlateau 这个回调中了。 可视化损失和评估值最好的方法是使用TensorBoard，它可以帮助我们实时可视化损失值和评估值。启动Tensorboard的方法如下： 1tensorboard --logdir=/full_path_to_your_logs 使用Tensorboard回调最简单的方法就是在fit的时候传入Tensorboard回调： 12tensorboard_cbk = keras.callbacks.TensorBoard(log_dir=&#x27;/full_path_to_your_logs&#x27;)model.fit(dataset, epochs=10, callbacks=[tensorboard_cbk]) Tensorboard还有一些其他参数可供选择： 12345keras.callbacks.TensorBoard( log_dir=&#x27;/full_path_to_your_logs&#x27;, histogram_freq=0, # How often to log histogram visualizations embeddings_freq=0, # How often to log embedding visualizations update_freq=&#x27;epoch&#x27;) # How often to write logs (default: once per epoch) 编写自己的训练和评估方法使用GradientTape在GradientTape作用域中调用模型会使你很容易得到相关参数的梯度值。下面是一个MNIST模型： 12345678910111213141516# Get the model.inputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x)outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x)model = keras.Model(inputs=inputs, outputs=outputs)# Instantiate an optimizer.optimizer = keras.optimizers.SGD(learning_rate=1e-3)# Instantiate a loss function.loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)# Prepare the training dataset.batch_size = 64train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size) 训练方法如下： 1234567891011121314151617181920212223242526272829303132epochs = 3for epoch in range(epochs): print(&#x27;Start of epoch %d&#x27; % (epoch,)) # Iterate over the batches of the dataset. for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): # Open a GradientTape to record the operations run # during the forward pass, which enables autodifferentiation. with tf.GradientTape() as tape: # Run the forward pass of the layer. # The operations that the layer applies # to its inputs are going to be recorded # on the GradientTape. logits = model(x_batch_train, training=True) # Logits for this minibatch # Compute the loss value for this minibatch. loss_value = loss_fn(y_batch_train, logits) # Use the gradient tape to automatically retrieve # the gradients of the trainable variables with respect to the loss. grads = tape.gradient(loss_value, model.trainable_weights) # Run one step of gradient descent by updating # the value of the variables to minimize the loss. optimizer.apply_gradients(zip(grads, model.trainable_weights)) # Log every 200 batches. if step % 200 == 0: print(&#x27;Training loss (for one batch) at step %s: %s&#x27; % (step, float(loss_value))) print(&#x27;Seen so far: %s samples&#x27; % ((step + 1) * 64)) 实现自定义评估值接着我们添加自定义的评估值，下面是工作流： 在每次迭代前初始化评价指标 在每个批次结束时调用metric.update_state 在需要展示结果的时候调用metric.result 在需要重置的时候（如每次迭代的末尾）调用metric.reset_states 接下来手动实现SparseCategoricalAccuracy 评价指标，下面是模型创建时的代码： 12345678910111213141516171819202122232425# Get modelinputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x)outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x)model = keras.Model(inputs=inputs, outputs=outputs)# Instantiate an optimizer to train the model.optimizer = keras.optimizers.SGD(learning_rate=1e-3)# Instantiate a loss function.loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)# Prepare the metrics.train_acc_metric = keras.metrics.SparseCategoricalAccuracy()val_acc_metric = keras.metrics.SparseCategoricalAccuracy()# Prepare the training dataset.batch_size = 64train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)# Prepare the validation dataset.val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))val_dataset = val_dataset.batch(64) 自定义训练的迭代如下： 12345678910111213141516171819202122232425262728293031323334epochs = 3for epoch in range(epochs): print(&#x27;Start of epoch %d&#x27; % (epoch,)) # Iterate over the batches of the dataset. for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): with tf.GradientTape() as tape: logits = model(x_batch_train) loss_value = loss_fn(y_batch_train, logits) grads = tape.gradient(loss_value, model.trainable_weights) optimizer.apply_gradients(zip(grads, model.trainable_weights)) # Update training metric. train_acc_metric(y_batch_train, logits) # Log every 200 batches. if step % 200 == 0: print(&#x27;Training loss (for one batch) at step %s: %s&#x27; % (step, float(loss_value))) print(&#x27;Seen so far: %s samples&#x27; % ((step + 1) * 64)) # Display metrics at the end of each epoch. train_acc = train_acc_metric.result() print(&#x27;Training acc over epoch: %s&#x27; % (float(train_acc),)) # Reset training metrics at the end of each epoch train_acc_metric.reset_states() # Run a validation loop at the end of each epoch. for x_batch_val, y_batch_val in val_dataset: val_logits = model(x_batch_val) # Update val metrics val_acc_metric(y_batch_val, val_logits) val_acc = val_acc_metric.result() val_acc_metric.reset_states() print(&#x27;Validation acc: %s&#x27; % (float(val_acc),)) 处理额外的损失值在前面的小节中我们在call方法中调用self.add_loss(values)来正则化损失值，通常来说我们需要将这些额外的损失值也考虑在内，下面是我们实现的其中一个模型： 123456789101112131415class ActivityRegularizationLayer(layers.Layer): def call(self, inputs): self.add_loss(1e-2 * tf.reduce_sum(inputs)) return inputsinputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs)# Insert activity regularization as a layerx = ActivityRegularizationLayer()(x)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x)outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x)model = keras.Model(inputs=inputs, outputs=outputs) 当我们调用模型的时候： 1logits = model(x_train) 在前向传播过程中的损失值会被加到model.losses属性中。 为了将额外的损失值考虑在内，我们需要修改我们自定义的训练循环体中的代码： 123456789101112131415161718192021optimizer = keras.optimizers.SGD(learning_rate=1e-3)epochs = 3for epoch in range(epochs): print(&#x27;Start of epoch %d&#x27; % (epoch,)) for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): with tf.GradientTape() as tape: logits = model(x_batch_train) loss_value = loss_fn(y_batch_train, logits) # Add extra losses created during this forward pass: loss_value += sum(model.losses) grads = tape.gradient(loss_value, model.trainable_weights) optimizer.apply_gradients(zip(grads, model.trainable_weights)) # Log every 200 batches. if step % 200 == 0: print(&#x27;Training loss (for one batch) at step %s: %s&#x27; % (step, float(loss_value))) print(&#x27;Seen so far: %s samples&#x27; % ((step + 1) * 64))","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://blog.zsstrike.top/tags/TensorFlow/"}]},{"title":"Keras函数式API","slug":"Keras函数式API","date":"2020-02-14T06:23:53.000Z","updated":"2020-12-17T10:23:43.485Z","comments":true,"path":"2020/02/14/Keras函数式API/","link":"","permalink":"http://blog.zsstrike.top/2020/02/14/Keras%E5%87%BD%E6%95%B0%E5%BC%8FAPI/","excerpt":"本章了解Keras的函数式API以及灵活使用它们的方法。","text":"本章了解Keras的函数式API以及灵活使用它们的方法。 简介我们已经熟悉了如何使用keras.Sequential函数来创建我们的层叠模型，而函数式API是比它更加灵活的创建模型的方法：它可以允许我们创建非线性的模型，共用层的模型以及多个输入输出的模型。函数式API的基本思路是深度学习网络是一种有向无环图（DAG），我们可以使用函数式API来创建这些层。 如下一个包含了3个层的模型： 123456789(input: 784-dimensional vectors) ↧[Dense (64 units, relu activation)] ↧[Dense (64 units, relu activation)] ↧[Dense (10 units, softmax activation)] ↧(output: logits of a probability distribution over 10 classes) 为了使用函数式API来创建相同的模型，我们首先创建输入节点： 123from tensorflow import kerasinputs = keras.Input(shape=(784,)) 我们声明了输入的数据是一个784维的向量，注意这里的shape是单个样本的shape，不是批次的shape。对于图片，假设数据是（32，32，3）类型的，我们可以使用以下代码： 12# Just for demonstration purposesimg_inputs = keras.Input(shape=(32, 32, 3)) 我们得到的返回值inputs包含了输入数据的shape和类型。为了创建层节点，我们使用如下方法： 1234from tensorflow.keras import layersdense = layers.Dense(64, activation=&#x27;relu&#x27;)x = dense(inputs) 调用层函数的作用相当于在两个节点之间画一条有向线，我们得到了经过第一层处理后的返回值x，接着，我们创建完剩余的层： 12x = layers.Dense(64, activation=&#x27;relu&#x27;)(x)outputs = layers.Dense(10)(x) 到了这一步，我们现在可以创建我们的模型了： 1model = keras.Model(inputs=inputs, outputs=outputs) 至此，我们的模型就创建成功了。 训练评估和预测训练评估和预测的使用方法其实和在Sequential中创建模型一致。下面是使用我们刚刚创建的模型进行训练评估和预测的代码： 1234567891011121314(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()x_train = x_train.reshape(60000, 784).astype(&#x27;float32&#x27;) / 255x_test = x_test.reshape(10000, 784).astype(&#x27;float32&#x27;) / 255model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=keras.optimizers.RMSprop(), metrics=[&#x27;accuracy&#x27;])history = model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.2)test_scores = model.evaluate(x_test, y_test, verbose=2)print(&#x27;Test loss:&#x27;, test_scores[0])print(&#x27;Test accuracy:&#x27;, test_scores[1]) 序列化同样地，使用函数式APiece创建出来的模型序列化和反序列化和使用Sequential创建的模型一致。最常用的方法是使用save方法，它会保存： 模型的架构 模型权重值（在训练中得到） 模型训练配置（在compile的时候得到） 模型优化配置 1234model.save(&#x27;path_to_my_model&#x27;)del model# Recreate the exact same model purely from the file:model = keras.models.load_model(&#x27;path_to_my_model&#x27;) 使用相同的层来创建多个模型在使用函数式API创建模型时，我们只需要声明模型的输入和输出即可，这也就意味着我们可以使用相同的层来创建多个模型，以下是一个实例： 1234567891011121314151617181920encoder_input = keras.Input(shape=(28, 28, 1), name=&#x27;img&#x27;)x = layers.Conv2D(16, 3, activation=&#x27;relu&#x27;)(encoder_input)x = layers.Conv2D(32, 3, activation=&#x27;relu&#x27;)(x)x = layers.MaxPooling2D(3)(x)x = layers.Conv2D(32, 3, activation=&#x27;relu&#x27;)(x)x = layers.Conv2D(16, 3, activation=&#x27;relu&#x27;)(x)encoder_output = layers.GlobalMaxPooling2D()(x)encoder = keras.Model(encoder_input, encoder_output, name=&#x27;encoder&#x27;)encoder.summary()x = layers.Reshape((4, 4, 1))(encoder_output)x = layers.Conv2DTranspose(16, 3, activation=&#x27;relu&#x27;)(x)x = layers.Conv2DTranspose(32, 3, activation=&#x27;relu&#x27;)(x)x = layers.UpSampling2D(3)(x)x = layers.Conv2DTranspose(16, 3, activation=&#x27;relu&#x27;)(x)decoder_output = layers.Conv2DTranspose(1, 3, activation=&#x27;relu&#x27;)(x)autoencoder = keras.Model(encoder_input, decoder_output, name=&#x27;autoencoder&#x27;)autoencoder.summary() 模型可调用我们可以将模型看作是特殊的层，因为它接收Input或者其他层的输出作为参数。注意，我们调用模型的时候不仅仅只是使用了它的架构，还使用了它的权重： 123456789101112131415161718192021222324252627encoder_input = keras.Input(shape=(28, 28, 1), name=&#x27;original_img&#x27;)x = layers.Conv2D(16, 3, activation=&#x27;relu&#x27;)(encoder_input)x = layers.Conv2D(32, 3, activation=&#x27;relu&#x27;)(x)x = layers.MaxPooling2D(3)(x)x = layers.Conv2D(32, 3, activation=&#x27;relu&#x27;)(x)x = layers.Conv2D(16, 3, activation=&#x27;relu&#x27;)(x)encoder_output = layers.GlobalMaxPooling2D()(x)encoder = keras.Model(encoder_input, encoder_output, name=&#x27;encoder&#x27;)encoder.summary()decoder_input = keras.Input(shape=(16,), name=&#x27;encoded_img&#x27;)x = layers.Reshape((4, 4, 1))(decoder_input)x = layers.Conv2DTranspose(16, 3, activation=&#x27;relu&#x27;)(x)x = layers.Conv2DTranspose(32, 3, activation=&#x27;relu&#x27;)(x)x = layers.UpSampling2D(3)(x)x = layers.Conv2DTranspose(16, 3, activation=&#x27;relu&#x27;)(x)decoder_output = layers.Conv2DTranspose(1, 3, activation=&#x27;relu&#x27;)(x)decoder = keras.Model(decoder_input, decoder_output, name=&#x27;decoder&#x27;)decoder.summary()autoencoder_input = keras.Input(shape=(28, 28, 1), name=&#x27;img&#x27;)encoded_img = encoder(autoencoder_input)decoded_img = decoder(encoded_img)autoencoder = keras.Model(autoencoder_input, decoded_img, name=&#x27;autoencoder&#x27;)autoencoder.summary() 可以发现，模型可以包含子模型，一个常见的用途是用于模型的聚合： 123456789101112131415def get_model(): inputs = keras.Input(shape=(128,)) outputs = layers.Dense(1)(inputs) return keras.Model(inputs, outputs)model1 = get_model()model2 = get_model()model3 = get_model()inputs = keras.Input(shape=(128,))y1 = model1(inputs)y2 = model2(inputs)y3 = model3(inputs)outputs = layers.average([y1, y2, y3])ensemble_model = keras.Model(inputs=inputs, outputs=outputs) 生成复杂模型包含多个输入和输出的模型我们可以使用函数式API生成包含多个输入输出的模型，这在Sequential中是不能被实现的。接下来我们创建一个将用户问题分类并且将其转交给哪个部门的模型，这个模型含有3个输入： 问题的标题 问题的内容 用户添加的问题的标签（分类输入） 含有2个输出： 优先级[0, 1] 这个问题该交给哪个部门 下面是代码实现： 1234567891011121314151617181920212223242526272829num_tags = 12 # Number of unique issue tagsnum_words = 10000 # Size of vocabulary obtained when preprocessing text datanum_departments = 4 # Number of departments for predictionstitle_input = keras.Input(shape=(None,), name=&#x27;title&#x27;) # Variable-length sequence of intsbody_input = keras.Input(shape=(None,), name=&#x27;body&#x27;) # Variable-length sequence of intstags_input = keras.Input(shape=(num_tags,), name=&#x27;tags&#x27;) # Binary vectors of size `num_tags`# Embed each word in the title into a 64-dimensional vectortitle_features = layers.Embedding(num_words, 64)(title_input)# Embed each word in the text into a 64-dimensional vectorbody_features = layers.Embedding(num_words, 64)(body_input)# Reduce sequence of embedded words in the title into a single 128-dimensional vectortitle_features = layers.LSTM(128)(title_features)# Reduce sequence of embedded words in the body into a single 32-dimensional vectorbody_features = layers.LSTM(32)(body_features)# Merge all available features into a single large vector via concatenationx = layers.concatenate([title_features, body_features, tags_input])# Stick a logistic regression for priority prediction on top of the featurespriority_pred = layers.Dense(1, name=&#x27;priority&#x27;)(x)# Stick a department classifier on top of the featuresdepartment_pred = layers.Dense(num_departments, name=&#x27;department&#x27;)(x)# Instantiate an end-to-end model predicting both priority and departmentmodel = keras.Model(inputs=[title_input, body_input, tags_input], outputs=[priority_pred, department_pred]) 至此我们完成的模型的创建，接下里需要完成模型的编译： 1234model.compile(optimizer&#x3D;keras.optimizers.RMSprop(1e-3), loss&#x3D;[keras.losses.BinaryCrossentropy(from_logits&#x3D;True), keras.losses.CategoricalCrossentropy(from_logits&#x3D;True)], loss_weights&#x3D;[1., 0.2]) 如上，我们可以为输出赋予不同的误差函数，以帮助我们控制他们两个输出对误差的贡献。由于我们已经为输出层赋予了名字，我们也可以使用如下方式编译： 1234model.compile(optimizer=keras.optimizers.RMSprop(1e-3), loss=&#123;&#x27;priority&#x27;:keras.losses.BinaryCrossentropy(from_logits=True), &#x27;department&#x27;: keras.losses.CategoricalCrossentropy(from_logits=True)&#125;, loss_weights=[1., 0.2]) 接下来进行训练，对于在NumPy产生的数据： 1234567891011121314import numpy as np# Dummy input datatitle_data = np.random.randint(num_words, size=(1280, 10))body_data = np.random.randint(num_words, size=(1280, 100))tags_data = np.random.randint(2, size=(1280, num_tags)).astype(&#x27;float32&#x27;)# Dummy target datapriority_targets = np.random.random(size=(1280, 1))dept_targets = np.random.randint(2, size=(1280, num_departments))model.fit(&#123;&#x27;title&#x27;: title_data, &#x27;body&#x27;: body_data, &#x27;tags&#x27;: tags_data&#125;, &#123;&#x27;priority&#x27;: priority_targets, &#x27;department&#x27;: dept_targets&#125;, epochs=2, batch_size=32) 当我们使用Dataset对象时，它要么yield数组元组：([title_data, body_data, tags_data], [priority_targets, dept_targets])，要么yield字典元组：(&#123;&#39;title&#39;: title_data, &#39;body&#39;: body_data, &#39;tags&#39;: tags_data&#125;, &#123;&#39;priority&#39;: priority_targets, &#39;department&#39;: dept_targets&#125;)。 一个简单的残差网络模型函数式API还可以创建非线性的模型，一个常见的应用是构建残差模型： 123456789101112131415161718192021inputs = keras.Input(shape=(32, 32, 3), name=&#x27;img&#x27;)x = layers.Conv2D(32, 3, activation=&#x27;relu&#x27;)(inputs)x = layers.Conv2D(64, 3, activation=&#x27;relu&#x27;)(x)block_1_output = layers.MaxPooling2D(3)(x)x = layers.Conv2D(64, 3, activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(block_1_output)x = layers.Conv2D(64, 3, activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x)block_2_output = layers.add([x, block_1_output])x = layers.Conv2D(64, 3, activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(block_2_output)x = layers.Conv2D(64, 3, activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x)block_3_output = layers.add([x, block_2_output])x = layers.Conv2D(64, 3, activation=&#x27;relu&#x27;)(block_3_output)x = layers.GlobalAveragePooling2D()(x)x = layers.Dense(256, activation=&#x27;relu&#x27;)(x)x = layers.Dropout(0.5)(x)outputs = layers.Dense(10)(x)model = keras.Model(inputs, outputs, name=&#x27;toy_resnet&#x27;)model.summary() 训练方法如下： 12345678910111213(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()x_train = x_train.astype(&#x27;float32&#x27;) / 255.x_test = x_test.astype(&#x27;float32&#x27;) / 255.y_train = keras.utils.to_categorical(y_train, 10)y_test = keras.utils.to_categorical(y_test, 10)model.compile(optimizer=keras.optimizers.RMSprop(1e-3), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[&#x27;acc&#x27;])model.fit(x_train, y_train, batch_size=64, epochs=1, validation_split=0.2) 共享层函数式API的另外一个优点是我们可以使用共享层。为了创建共享层，我们只需要创建层的实例，然后再不断调用即可： 123456789101112# Embedding for 1000 unique words mapped to 128-dimensional vectorsshared_embedding = layers.Embedding(1000, 128)# Variable-length sequence of integerstext_input_a = keras.Input(shape=(None,), dtype=&#x27;int32&#x27;)# Variable-length sequence of integerstext_input_b = keras.Input(shape=(None,), dtype=&#x27;int32&#x27;)# We reuse the same layer to encode both inputsencoded_input_a = shared_embedding(text_input_a)encoded_input_b = shared_embedding(text_input_b) 提取和重用节点由于我们使用函数式API创建的模型是静态的，所以它容易被存取和检查。这个过程和画图差不多。这也就意味着我们可以获取模型中节点并且重用他们。接下来我们看一下带有权重的VGG19模型： 123from tensorflow.keras.applications import VGG19vgg19 = VGG19() 可以通过模型的结构获取到中间的层（节点）： 1features_list = [layer.output for layer in vgg19.layers] 我们可以通过这些节点来构建一个模型，用于获取通过每个层的中间值： 1234feat_extraction_model = keras.Model(inputs=vgg19.input, outputs=features_list)img = np.random.random((1, 224, 224, 3)).astype(&#x27;float32&#x27;)extracted_features = feat_extraction_model(img) 自定义层来扩展APItf.keras含有大量的内建层： 卷积层：Conv1D, Conv2D, Conv3D, Conv2DTranspose 池化层：MaxPooling1D, MaxPooling2D, MaxPooling3D, AveragePooling1D RNN层：GRU, LSTM, ConvLSTM2D BatchNormalization, Dropout, Embedding，etc. 如果这些都不能满足要求，我们可以创建Layer的子类，每个子类需要实现： call：定义这一层完成的运算 build：创建这一层的权重 下面是Dense层的简单实现： 123456789101112131415161718192021class CustomDense(layers.Layer): def __init__(self, units=32): super(CustomDense, self).__init__() self.units = units def build(self, input_shape): self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer=&#x27;random_normal&#x27;, trainable=True) self.b = self.add_weight(shape=(self.units,), initializer=&#x27;random_normal&#x27;, trainable=True) def call(self, inputs): return tf.matmul(inputs, self.w) + self.binputs = keras.Input((4,))outputs = CustomDense(10)(inputs)model = keras.Model(inputs, outputs) 如果想支持序列化，这时也需要实现get_config方法，该方法将会返回构造器的参数： 1234567891011121314151617181920212223242526272829class CustomDense(layers.Layer): def __init__(self, units=32): super(CustomDense, self).__init__() self.units = units def build(self, input_shape): self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer=&#x27;random_normal&#x27;, trainable=True) self.b = self.add_weight(shape=(self.units,), initializer=&#x27;random_normal&#x27;, trainable=True) def call(self, inputs): return tf.matmul(inputs, self.w) + self.b def get_config(self): return &#123;&#x27;units&#x27;: self.units&#125;inputs = keras.Input((4,))outputs = CustomDense(10)(inputs)model = keras.Model(inputs, outputs)config = model.get_config()new_model = keras.Model.from_config( config, custom_objects=&#123;&#x27;CustomDense&#x27;: CustomDense&#125;) 同样可以实现from_config方法来实现层的重构，默认的from_config方法如下： 12def from_config(cls, config): return cls(**config) 使用函数式API的时机什么时候该使用函数式API构建模型，什么时候使用模型子类构建模型？总体来说，函数式API是一种更加易用安全，多特性的方法，而模型子类则提供了更高的灵活性。 函数式API的优点如下： 简洁的： 1234567891011121314151617181920212223# 函数式APIinputs = keras.Input(shape=(32,))x = layers.Dense(64, activation=&#x27;relu&#x27;)(inputs)outputs = layers.Dense(10)(x)mlp = keras.Model(inputs, outputs)# 模型子类class MLP(keras.Model): def __init__(self, **kwargs): super(MLP, self).__init__(**kwargs) self.dense_1 = layers.Dense(64, activation=&#x27;relu&#x27;) self.dense_2 = layers.Dense(10) def call(self, inputs): x = self.dense_1(inputs) return self.dense_2(x)# Instantiate the model.mlp = MLP()# Necessary to create the model&#x27;s state.# The model doesn&#x27;t have a state until it&#x27;s called at least once._ = mlp(tf.zeros((1, 32))) 在构建模型的时候提供检查：每一层可以根据输入数据的shape和dtype判断是否是合法的输入 模型更易构建：构建模型就像是画图一样简单 模型可以被序列化和克隆 函数式API缺点如下： 不支持动态架构 混合模式构建模型我们可以混合使用函数式API和模型子类方式来构建模型： 123456789101112131415161718192021222324252627282930313233343536units = 32timesteps = 10input_dim = 5# Define a Functional modelinputs = keras.Input((None, units))x = layers.GlobalAveragePooling1D()(inputs)outputs = layers.Dense(1)(x)model = keras.Model(inputs, outputs)class CustomRNN(layers.Layer): def __init__(self): super(CustomRNN, self).__init__() self.units = units self.projection_1 = layers.Dense(units=units, activation=&#x27;tanh&#x27;) self.projection_2 = layers.Dense(units=units, activation=&#x27;tanh&#x27;) # Our previously-defined Functional model self.classifier = model def call(self, inputs): outputs = [] state = tf.zeros(shape=(inputs.shape[0], self.units)) for t in range(inputs.shape[1]): x = inputs[:, t, :] h = self.projection_1(x) y = h + self.projection_2(state) state = y outputs.append(y) features = tf.stack(outputs, axis=1) print(features.shape) return self.classifier(features)rnn_model = CustomRNN()_ = rnn_model(tf.zeros((1, timesteps, input_dim))) 下面是一个使用函数式模型构建RNN网络： 1234567891011121314151617181920212223242526272829303132333435363738units = 32timesteps = 10input_dim = 5batch_size = 16class CustomRNN(layers.Layer): def __init__(self): super(CustomRNN, self).__init__() self.units = units self.projection_1 = layers.Dense(units=units, activation=&#x27;tanh&#x27;) self.projection_2 = layers.Dense(units=units, activation=&#x27;tanh&#x27;) self.classifier = layers.Dense(1) def call(self, inputs): outputs = [] state = tf.zeros(shape=(inputs.shape[0], self.units)) for t in range(inputs.shape[1]): x = inputs[:, t, :] h = self.projection_1(x) y = h + self.projection_2(state) state = y outputs.append(y) features = tf.stack(outputs, axis=1) return self.classifier(features)# Note that we specify a static batch size for the inputs with the `batch_shape`# arg, because the inner computation of `CustomRNN` requires a static batch size# (when we create the `state` zeros tensor).inputs = keras.Input(batch_shape=(batch_size, timesteps, input_dim))x = layers.Conv1D(32, 3)(inputs)outputs = CustomRNN()(x)model = keras.Model(inputs, outputs)rnn_model = CustomRNN()_ = rnn_model(tf.zeros((1, 10, 5)))","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://blog.zsstrike.top/tags/TensorFlow/"}]},{"title":"Keras概览","slug":"Keras概览","date":"2020-02-13T08:22:59.000Z","updated":"2020-12-17T10:23:43.486Z","comments":true,"path":"2020/02/13/Keras概览/","link":"","permalink":"http://blog.zsstrike.top/2020/02/13/Keras%E6%A6%82%E8%A7%88/","excerpt":"本节介绍Keras及其相关模块，以帮助我们快速构建人工神经网络。","text":"本节介绍Keras及其相关模块，以帮助我们快速构建人工神经网络。 构建一个简单模型层叠式模型在Keras中，我们使用层（layers）来构建我们的模型，模型通常是一个由多个层构成的流程图，最简单模型类型是层叠式（Sequential）类型。为了构建一个简单全连接的MLP，我们用如下代码： 1234567891011import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersmodel = tf.keras.Sequential()# Adds a densely-connected layer with 64 units to the model:model.add(layers.Dense(64, activation=&#x27;relu&#x27;))# Add another:model.add(layers.Dense(64, activation=&#x27;relu&#x27;))# Add an output layer with 10 output units:model.add(layers.Dense(10)) 调整层参数有很多内建的层，它们都有一些公用的构造参数： activation：设置激活函数 kernel_initializer和bias_initializer：用于初始化权重的方法 kernel_regularizer和bias_regularizer：定义正则化的方法 下面的代码构造使用不同的参数构造层： 12345678910111213141516# Create a relu layer:layers.Dense(64, activation=&#x27;relu&#x27;)# Or:layers.Dense(64, activation=tf.nn.relu)# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l1(0.01))# A linear layer with L2 regularization of factor 0.01 applied to the bias vector:layers.Dense(64, bias_regularizer=tf.keras.regularizers.l2(0.01))# A linear layer with a kernel initialized to a random orthogonal matrix:layers.Dense(64, kernel_initializer=&#x27;orthogonal&#x27;)# A linear layer with a bias vector initialized to 2.0s:layers.Dense(64, bias_initializer=tf.keras.initializers.Constant(2.0)) 训练和评估训练时的设置当模型被构建后，我们可以通过调用compile方法来调整学习过程： 1234567891011model = tf.keras.Sequential([# Adds a densely-connected layer with 64 units to the model:layers.Dense(64, activation=&#x27;relu&#x27;, input_shape=(32,)),# Add another:layers.Dense(64, activation=&#x27;relu&#x27;),# Add an output layer with 10 output units:layers.Dense(10)])model.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;]) compile有以下重要参数： optimizer：定义优化算法 loss：定义误差函数 metrix：用于观察训练的情况 下面的示例展示了调整模型的情况： 123456789# Configure a model for mean-squared error regression.model.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss=&#x27;mse&#x27;, # mean squared error metrics=[&#x27;mae&#x27;]) # mean absolute error# Configure a model for categorical classification.model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;]) 从NumPy数据训练对于小型的数据集，我们可以用如下方法训练： 123456import numpy as npdata = np.random.random((1000, 32))labels = np.random.random((1000, 10))model.fit(data, labels, epochs=10, batch_size=32) fit方法有以下重要的参数： epochs：训练的迭代次数 batch_size：每个批次的样本数量 validation_data：用于定义验证集 12345678910import numpy as npdata = np.random.random((1000, 32))labels = np.random.random((1000, 10))val_data = np.random.random((100, 32))val_labels = np.random.random((100, 10))model.fit(data, labels, epochs=10, batch_size=32, validation_data=(val_data, val_labels)) 从tf.data中的datasets训练使用Datasets中的方法来构建训练数据集： 12345# Instantiates a toy dataset instance:dataset = tf.data.Dataset.from_tensor_slices((data, labels))dataset = dataset.batch(32)model.fit(dataset, epochs=10) Dataset数据会不断yield小批次的数据，因此不需要batch_size参数。 同样，Dataset可以用于验证： 12345678dataset = tf.data.Dataset.from_tensor_slices((data, labels))dataset = dataset.batch(32)val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))val_dataset = val_dataset.batch(32)model.fit(dataset, epochs=10, validation_data=val_dataset) 评估和预测代码如下： 1234567891011# With Numpy arraysdata = np.random.random((1000, 32))labels = np.random.random((1000, 10))model.evaluate(data, labels, batch_size=32)# With a Datasetdataset = tf.data.Dataset.from_tensor_slices((data, labels))dataset = dataset.batch(32)model.evaluate(dataset) 同样，我们可以使用以下代码进行预测： 12result = model.predict(data, batch_size=32)print(result.shape) 构建复杂模型函数式API层叠式模型是一种将多个层之间连接的简单模型，我们可以使用Keras中的函数式API来构建复杂的模型： 多个输入模型 多个输出模型 包含共享层（同一个层被多次调用）的模型 不包含顺序流的模型（如残差模型） 接下来我们使用函数式API来构建这样的一个模型： 一个层实例是可以被调用的并且可以返回一个张量 输入输出张量可以被用来定义模型实例 该模型训练方法和层叠模型一致 下面的代码用于构建一个简单全连接的网络： 123456inputs = tf.keras.Input(shape=(32,)) # Returns an input placeholder# A layer instance is callable on a tensor, and returns a tensor.x = layers.Dense(64, activation=&#x27;relu&#x27;)(inputs)x = layers.Dense(64, activation=&#x27;relu&#x27;)(x)predictions = layers.Dense(10)(x) 接下来实例化模型： 123456789model = tf.keras.Model(inputs=inputs, outputs=predictions)# The compile step specifies the training configuration.model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;])# Trains for 5 epochsmodel.fit(data, labels, batch_size=32, epochs=5) 模型子类化为了构建一个高度定制的模型，我们可以构造模型的子类。我们可以在__init__方法中定义层和层的属性，同时在call方法中定义前向传播。代码示例如下： 1234567891011121314class MyModel(tf.keras.Model): def __init__(self, num_classes=10): super(MyModel, self).__init__(name=&#x27;my_model&#x27;) self.num_classes = num_classes # Define your layers here. self.dense_1 = layers.Dense(32, activation=&#x27;relu&#x27;) self.dense_2 = layers.Dense(num_classes) def call(self, inputs): # Define your forward pass here, # using layers you previously defined (in `__init__`). x = self.dense_1(inputs) return self.dense_2(x) 接下来定义新的模型子类： 123456789model = MyModel(num_classes=10)# The compile step specifies the training configuration.model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;])# Trains for 5 epochs.model.fit(data, labels, batch_size=32, epochs=5) 自定义层自定义的层可以通过构建tf.keras.layers.Layer的子类来进行，需要实现如下方法： __init__：可选，用于定义这一层中将要使用子层 build：创建层的权重，可以通过add_weight方法来添加权重 call：定义前向传播 可选，可以通过实现get_config和from_config方法实现序列化和反序列化 下面代码实现了一个矩阵相乘的层： 123456789101112131415161718192021222324class MyLayer(layers.Layer): def __init__(self, output_dim, **kwargs): self.output_dim = output_dim super(MyLayer, self).__init__(**kwargs) def build(self, input_shape): # Create a trainable weight variable for this layer. self.kernel = self.add_weight(name=&#x27;kernel&#x27;, shape=(input_shape[1], self.output_dim), initializer=&#x27;uniform&#x27;, trainable=True) def call(self, inputs): return tf.matmul(inputs, self.kernel) def get_config(self): base_config = super(MyLayer, self).get_config() base_config[&#x27;output_dim&#x27;] = self.output_dim return base_config @classmethod def from_config(cls, config): return cls(**config) 使用自定义层来构建模型： 12345678910model = tf.keras.Sequential([ MyLayer(10)])# The compile step specifies the training configurationmodel.compile(optimizer=tf.keras.optimizers.RMSprop(0.001), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;])# Trains for 5 epochs.model.fit(data, labels, batch_size=32, epochs=5) Callbackscallback对象用于传给模型以此在训练期间被使用，可以使用自定义的callback，同样可以使用内建的callback： tf.keras.callbacks.ModelCheckpoint: 每次迭代保存检验点 tf.keras.callbacks.LearningRateScheduler: 动态改变学习速率 tf.keras.callbacks.EarlyStopping: 如果模型没有改经就停止训练 tf.keras.callbacks.TensorBoard: 监视模型的行为 使用方法如下： 12345678callbacks = [ # Interrupt training if `val_loss` stops improving for over 2 epochs tf.keras.callbacks.EarlyStopping(patience=2, monitor=&#x27;val_loss&#x27;), # Write TensorBoard logs to `./logs` directory tf.keras.callbacks.TensorBoard(log_dir=&#x27;./logs&#x27;)]model.fit(data, labels, batch_size=32, epochs=5, callbacks=callbacks, validation_data=(val_data, val_labels)) 保存和恢复保存权重值使用方法： 123456# Save weights to a TensorFlow Checkpoint filemodel.save_weights(&#x27;./weights/my_model&#x27;)# Restore the model&#x27;s state,# this requires a model with the same architecture.model.load_weights(&#x27;./weights/my_model&#x27;) 同样，可以将文件格式保存为HDF5类型： 12345# Save weights to a HDF5 filemodel.save_weights(&#x27;my_model.h5&#x27;, save_format=&#x27;h5&#x27;)# Restore the model&#x27;s statemodel.load_weights(&#x27;my_model.h5&#x27;) 保存模型配置参数一个模型的配置参数可以被保存（但是不保存权重），即使是在没有代码定义，一个模型的配置可以用于创建和初始化同样的模型。Keras支持JSON和YAML的两种序列化的格式： 1234567# Serialize a model to JSON formatjson_string = model.to_json() # savefresh_model = tf.keras.models.model_from_json(json_string) # restore# Serialize a model to YAML formatyaml_string = model.to_yaml()fresh_model = tf.keras.models.model_from_yaml(yaml_string) 保存整个模型到一个文件整个模型的权重值，模型配置参数和优化配置参数可以被保存在一个文件中，这样可以让我们的模型在检查点处保存和恢复： 12345678910111213141516# Create a simple modelmodel = tf.keras.Sequential([ layers.Dense(10, activation=&#x27;relu&#x27;, input_shape=(32,)), layers.Dense(10)])model.compile(optimizer=&#x27;rmsprop&#x27;, loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;])model.fit(data, labels, batch_size=32, epochs=5)# Save entire model to a HDF5 filemodel.save(&#x27;my_model&#x27;)# Recreate the exact same model, including weights and optimizer.model = tf.keras.models.load_model(&#x27;my_model&#x27;) 分布式运行在GPUs上运行首先需要在分布策略范围内定义模型： 12345678910111213strategy = tf.distribute.MirroredStrategy()with strategy.scope(): model = tf.keras.Sequential() model.add(layers.Dense(16, activation=&#x27;relu&#x27;, input_shape=(10,))) model.add(layers.Dense(1)) optimizer = tf.keras.optimizers.SGD(0.2) model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=optimizer)model.summary() 接下来，按照平常的方式进行训练： 1234567x = np.random.random((1024, 10))y = np.random.randint(2, size=(1024, 1))x = tf.cast(x, tf.float32)dataset = tf.data.Dataset.from_tensor_slices((x, y))dataset = dataset.shuffle(buffer_size=1024).batch(32)model.fit(dataset, epochs=1)","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://blog.zsstrike.top/tags/TensorFlow/"}]},{"title":"TF2初始教程","slug":"TF2初始教程","date":"2020-02-12T06:38:17.000Z","updated":"2020-12-17T10:23:43.554Z","comments":true,"path":"2020/02/12/TF2初始教程/","link":"","permalink":"http://blog.zsstrike.top/2020/02/12/TF2%E5%88%9D%E5%A7%8B%E6%95%99%E7%A8%8B/","excerpt":"本节我们学习一些TensorFlow的基本使用方法，包括使用TensorFlow构建神经网络来对MNIST数据集进行划分以及学习一下数据的加载方法。","text":"本节我们学习一些TensorFlow的基本使用方法，包括使用TensorFlow构建神经网络来对MNIST数据集进行划分以及学习一下数据的加载方法。 使用TensorFlow对MNIST数据集进行划分首先，我们加载MNIST数据集，同时将数据映射到$ [0, 1] $上： 12345import tensorflow as tfmnist = tf.keras.datasets.mnist(X_train, y_train), (X_test, y_test) = mnist.load_data()X_train, X_test = X_train / 255.0, X_test / 255.0 接下来将各层堆叠起来，来搭建tf.keras.Sequential模型： 123456model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation=&#x27;relu&#x27;), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10, activation=&#x27;softmax&#x27;)]) 接下来我们将已经搭建的模型进行编译： 12model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;]) 接下来，训练并且验证模型： 12model.fit(X_train, y_train, epochs=5)model.evaluate(X_test, y_test, verbose=2) 得到的结果如下： 12345678910111213Train on 60000 samplesEpoch 1&#x2F;560000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 5s 87us&#x2F;sample - loss: 0.2942 - accuracy: 0.9140Epoch 2&#x2F;560000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 5s 75us&#x2F;sample - loss: 0.1416 - accuracy: 0.9582Epoch 3&#x2F;560000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 4s 75us&#x2F;sample - loss: 0.1056 - accuracy: 0.9681Epoch 4&#x2F;560000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 4s 73us&#x2F;sample - loss: 0.0888 - accuracy: 0.9724Epoch 5&#x2F;560000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 4s 73us&#x2F;sample - loss: 0.0752 - accuracy: 0.976110000&#x2F;1 - 1s - loss: 0.0385 - accuracy: 0.9779[0.07606992674819194, 0.9779] 现在，我们得到的照片分类器的准确率已经达到了98%。相较于之前我们实现的分类器，这个分类器的准确率更加优良。 对Fashion MNIST数据集划分这一节我们会构建一个神经网络模型来区分关于衣物的图片，首先导入我们需要的库： 1234567import tensorflow as tffrom tensorflow import kerasimport numpy as npimport matplotlib.pyplot as pltprint(tf.__version__)&gt;&gt; 2.0.0 接下来导入Fashion MNIST数据集，这个数据集包含了共70000张10个类别的图片，每个图片用$ 28 \\times 28 $的矩阵来表示。我们将60000张图片用作是训练，10000章图片用作是评估。代码如下： 12fashion_mnist = keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() 该数据集的类标对应关系如下： Label Class 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot 我们可以构建一个列表，来映射相应类标对应的类别： 12class_names = [&#x27;T-shirt/top&#x27;, &#x27;Trouser&#x27;, &#x27;Pullover&#x27;, &#x27;Dress&#x27;, &#x27;Coat&#x27;, &#x27;Sandal&#x27;, &#x27;Shirt&#x27;, &#x27;Sneaker&#x27;, &#x27;Bag&#x27;, &#x27;Ankle boot&#x27;] 下面我们看一下训练集中的第一张图片： 12345plt.figure()plt.imshow(train_images[0])plt.colorbar()plt.grid(False)plt.show() 得到的图像如下： img 接下来我们需要进行特征缩放： 12train_images = train_images / 255.0test_images = test_images / 255.0 然后，看一下训练集中的前25张图片： 12345678910# first 25 picplt.figure(figsize=(10, 10))for i in range(25): plt.subplot(5, 5, i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i], cmap=plt.cm.binary) plt.xlabel(class_names[train_labels[i]])plt.show() 图像如下： img 至此，我们来构建并且编译模型： 1234567891011# build model## set up layersmodel = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(128, activation=&#x27;relu&#x27;), keras.layers.Dense(10)])## compile the modelmodel.compile(optimizer=&#x27;adam&#x27;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;]) 然后，训练这个模型： 1model.fit(train_images, train_labels, epochs=10) 运行结果如下： 12345678910111213141516171819202122Train on 60000 samplesEpoch 1&#x2F;1060000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 5s 76us&#x2F;sample - loss: 0.5019 - accuracy: 0.8227Epoch 2&#x2F;1060000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 4s 67us&#x2F;sample - loss: 0.3747 - accuracy: 0.8639Epoch 3&#x2F;1060000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 4s 68us&#x2F;sample - loss: 0.3375 - accuracy: 0.8772Epoch 4&#x2F;1060000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 4s 68us&#x2F;sample - loss: 0.3132 - accuracy: 0.8858Epoch 5&#x2F;1060000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 4s 72us&#x2F;sample - loss: 0.2913 - accuracy: 0.8926Epoch 6&#x2F;1060000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 4s 73us&#x2F;sample - loss: 0.2791 - accuracy: 0.8977Epoch 7&#x2F;1060000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 4s 74us&#x2F;sample - loss: 0.2660 - accuracy: 0.9014Epoch 8&#x2F;1060000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 5s 82us&#x2F;sample - loss: 0.2538 - accuracy: 0.9048Epoch 9&#x2F;1060000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 5s 78us&#x2F;sample - loss: 0.2454 - accuracy: 0.9086Epoch 10&#x2F;1060000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 5s 90us&#x2F;sample - loss: 0.2362 - accuracy: 0.9113 接下来，我们看一下模型在评估集上面的准确率： 123test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)print(&#x27;Acc: %.2f&#x27; % test_acc)&gt;&gt; Acc: 0.88 可以发现我们的模型在评估集上面的准确率比在训练集上面的准确率低，说明我们的模型过拟合了。 接下来，我们来进行预测： 12345predictions = model.predict(test_images)predictions[0]&gt;&gt; array([-10.688818 , -11.685984 , -11.544111 , -15.445654 , -9.708677 ,&gt;&gt; -1.1382349, -10.859651 , 2.193298 , -12.344756 , 6.487081 ],&gt;&gt; dtype=float32) 可以发现一个预测的结果是一个包含10个数字的数组，数字代表着这张图片属于某个类标的可信度。同样可以使用argmax函数来得到最高可信度对应的类标： 12np.argmax(predictions[0])&gt;&gt; 9 同样的，当我们需要对单独一个未知的数据进行预测的时候，需要将其转换为$ (n,28,28) $的shape： 1234img = test_images[1]img = np.expand_dims(img, 0)print(img.shape)&gt;&gt; (1, 28, 28) 接下里就可以进行预测了： 1234predictions_single = model.predict(img)print(predictions_single)&gt;&gt; [[ -2.5079174 -16.936686 8.989066 -12.332449 2.5185988&gt;&gt; -2.7389777 -0.61303186 -22.695055 -13.934152 -30.008425 ]] 相应的类标如下： 12np.argmax(predictions_single[0])&gt;&gt; 2 加载CSV数据本节学习如何将CSV文件加载到tf.data.Dataset中，将要使用的是泰坦尼克号乘客的数据，模型会根据乘客的年龄，性别，票务舱和是否独立旅行等特征来预测乘客生还的可能性。 首先，导入必要的库： 1234import functoolsimport numpy as npimport tensorflow as tfimport tensorflow_datasets as tfds 接下来，下载数据文件： 12345TRAIN_DATA_URL = &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;TEST_DATA_URL = &quot;https://storage.googleapis.com/tf-datasets/titanic/eval.csv&quot;train_file_path = tf.keras.utils.get_file(&quot;train.csv&quot;, TRAIN_DATA_URL)test_file_path = tf.keras.utils.get_file(&quot;eval.csv&quot;, TEST_DATA_URL) 同时设置一下numpy的输出设置，让他的输出精度是3位： 1np.set_printoptions(precision=3, suppress=True) 首先，来看一下CSV文件的前面几行： 1!head &#123;train_file_path&#125; 得到的输出如下： 12345678910survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone0,male,22.0,1,0,7.25,Third,unknown,Southampton,n1,female,38.0,1,0,71.2833,First,C,Cherbourg,n1,female,26.0,0,0,7.925,Third,unknown,Southampton,y1,female,35.0,1,0,53.1,First,C,Southampton,n0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y0,male,2.0,3,1,21.075,Third,unknown,Southampton,n1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n1,female,4.0,1,1,16.7,Third,G,Southampton,n 可以发现，CSV文件的每列都有一个列名。dataset构造函数会自动识别这些列名。如果某个CSV文件不包含列名，我们可以自己手动设置： 12345678CSV_COLUMNS = [&#x27;survived&#x27;, &#x27;sex&#x27;, &#x27;age&#x27;, &#x27;n_siblings_spouses&#x27;, &#x27;parch&#x27;, &#x27;fare&#x27;, &#x27;class&#x27;, &#x27;deck&#x27;, &#x27;embark_town&#x27;, &#x27;alone&#x27;]dataset = tf.data.experimental.make_csv_dataset( ..., column_names=CSV_COLUMNS, ...) 这个示例使用了所有的列，当然我们也可以只使用某些选中的列： 123456dataset = tf.data.experimental.make_csv_dataset( ..., select_columns = columns_to_use, ...) 对于包含模型需要预测的值的列是需要显式指定的： 12LABEL_COLUMN = &#x27;survived&#x27;LABELS = [0, 1] 现在从文件中读取CSV数据并创建dataset： 123456789101112def get_dataset(file_path): dataset = tf.data.experimental.make_csv_dataset( file_path, batch_size=12, # 为了示例更容易展示，手动设置较小的值 label_name=LABEL_COLUMN, na_value=&quot;?&quot;, num_epochs=1, ignore_errors=True) return datasetraw_train_data = get_dataset(train_file_path)raw_test_data = get_dataset(test_file_path) dataset中的每个条目都是一个批次，用一个元组表示（多个样本，多个标签）。样本中的数据组织形式是以列为主的张量，每个条目中包含的元素个数就是批次大小（本例中是12）。 我们首先获取第一个条目的数据： 123examples, labels = next(iter(raw_train_data)) # 第一个批次print(&quot;EXAMPLES: \\n&quot;, examples, &quot;\\n&quot;)print(&quot;LABELS: \\n&quot;, labels) 输出： 1234567891011121314151617181920212223EXAMPLES: OrderedDict([(&#39;sex&#39;, &lt;tf.Tensor: id&#x3D;170, shape&#x3D;(12,), dtype&#x3D;string, numpy&#x3D;array([b&#39;male&#39;, b&#39;male&#39;, b&#39;female&#39;, b&#39;female&#39;, b&#39;female&#39;, b&#39;male&#39;, b&#39;male&#39;, b&#39;male&#39;, b&#39;male&#39;, b&#39;male&#39;, b&#39;male&#39;, b&#39;male&#39;], dtype&#x3D;object)&gt;), (&#39;age&#39;, &lt;tf.Tensor: id&#x3D;162, shape&#x3D;(12,), dtype&#x3D;float32, numpy&#x3D;array([19., 17., 42., 22., 9., 24., 28., 36., 37., 32., 28., 28.], dtype&#x3D;float32)&gt;), (&#39;n_siblings_spouses&#39;, &lt;tf.Tensor: id&#x3D;168, shape&#x3D;(12,), dtype&#x3D;int32, numpy&#x3D;array([0, 0, 1, 1, 4, 1, 0, 0, 2, 0, 1, 0], dtype&#x3D;int32)&gt;), (&#39;parch&#39;, &lt;tf.Tensor: id&#x3D;169, shape&#x3D;(12,), dtype&#x3D;int32, numpy&#x3D;array([0, 2, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0], dtype&#x3D;int32)&gt;), (&#39;fare&#39;, &lt;tf.Tensor: id&#x3D;167, shape&#x3D;(12,), dtype&#x3D;float32, numpy&#x3D;array([ 6.75 , 110.883, 26. , 29. , 31.275, 16.1 , 13.863, 512.329, 7.925, 7.896, 19.967, 26.55 ], dtype&#x3D;float32)&gt;), (&#39;class&#39;, &lt;tf.Tensor: id&#x3D;164, shape&#x3D;(12,), dtype&#x3D;string, numpy&#x3D;array([b&#39;Third&#39;, b&#39;First&#39;, b&#39;Second&#39;, b&#39;Second&#39;, b&#39;Third&#39;, b&#39;Third&#39;, b&#39;Second&#39;, b&#39;First&#39;, b&#39;Third&#39;, b&#39;Third&#39;, b&#39;Third&#39;, b&#39;First&#39;], dtype&#x3D;object)&gt;), (&#39;deck&#39;, &lt;tf.Tensor: id&#x3D;165, shape&#x3D;(12,), dtype&#x3D;string, numpy&#x3D;array([b&#39;unknown&#39;, b&#39;C&#39;, b&#39;unknown&#39;, b&#39;unknown&#39;, b&#39;unknown&#39;, b&#39;unknown&#39;, b&#39;unknown&#39;, b&#39;B&#39;, b&#39;unknown&#39;, b&#39;unknown&#39;, b&#39;unknown&#39;, b&#39;C&#39;], dtype&#x3D;object)&gt;), (&#39;embark_town&#39;, &lt;tf.Tensor: id&#x3D;166, shape&#x3D;(12,), dtype&#x3D;string, numpy&#x3D;array([b&#39;Queenstown&#39;, b&#39;Cherbourg&#39;, b&#39;Southampton&#39;, b&#39;Southampton&#39;, b&#39;Southampton&#39;, b&#39;Southampton&#39;, b&#39;Cherbourg&#39;, b&#39;Cherbourg&#39;, b&#39;Southampton&#39;, b&#39;Southampton&#39;, b&#39;Southampton&#39;, b&#39;Southampton&#39;], dtype&#x3D;object)&gt;), (&#39;alone&#39;, &lt;tf.Tensor: id&#x3D;163, shape&#x3D;(12,), dtype&#x3D;string, numpy&#x3D;array([b&#39;y&#39;, b&#39;n&#39;, b&#39;n&#39;, b&#39;n&#39;, b&#39;n&#39;, b&#39;n&#39;, b&#39;y&#39;, b&#39;n&#39;, b&#39;n&#39;, b&#39;y&#39;, b&#39;n&#39;, b&#39;y&#39;], dtype&#x3D;object)&gt;)]) LABELS: tf.Tensor([0 1 1 1 0 0 1 1 0 0 0 1], shape&#x3D;(12,), dtype&#x3D;int32) 接下来，我们进行数据的预处理。 CSV数据中有些列是分类的列，也就是这些列中的值只能在有限的集合中取值。使用tf.feature_columnAPI创建一个tf.feture_column.indicator_column集合，集合中每个元素对应着一个分类的列。我们先将其转换： 123456789101112131415CATEGORIES = &#123; &#x27;sex&#x27;: [&#x27;male&#x27;, &#x27;female&#x27;], &#x27;class&#x27; : [&#x27;First&#x27;, &#x27;Second&#x27;, &#x27;Third&#x27;], &#x27;deck&#x27; : [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;, &#x27;G&#x27;, &#x27;H&#x27;, &#x27;I&#x27;, &#x27;J&#x27;], &#x27;embark_town&#x27; : [&#x27;Cherbourg&#x27;, &#x27;Southhampton&#x27;, &#x27;Queenstown&#x27;], &#x27;alone&#x27; : [&#x27;y&#x27;, &#x27;n&#x27;]&#125;categorical_columns = []for feature, vocab in CATEGORIES.items(): cat_col = tf.feature_column.categorical_column_with_vocabulary_list( key=feature, vocabulary_list=vocab) categorical_columns.append(tf.feature_column.indicator_column(cat_col) categorical_columns 得到的输出如下： 12345[IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key=&#x27;sex&#x27;, vocabulary_list=(&#x27;male&#x27;, &#x27;female&#x27;), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key=&#x27;class&#x27;, vocabulary_list=(&#x27;First&#x27;, &#x27;Second&#x27;, &#x27;Third&#x27;), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key=&#x27;deck&#x27;, vocabulary_list=(&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;, &#x27;G&#x27;, &#x27;H&#x27;, &#x27;I&#x27;, &#x27;J&#x27;), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key=&#x27;embark_town&#x27;, vocabulary_list=(&#x27;Cherbourg&#x27;, &#x27;Southhampton&#x27;, &#x27;Queenstown&#x27;), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key=&#x27;alone&#x27;, vocabulary_list=(&#x27;y&#x27;, &#x27;n&#x27;), dtype=tf.string, default_value=-1, num_oov_buckets=0))] 这是后续构建模型时处理输入数据的一部分。 而对于连续数据，我们需要将其进行标准化，写一个函数标准化这些值，然后将这些值改造成2维德张量： 1234def process_continuous_data(mean, data): # 标准化数据 data = tf.cast(data, tf.float32) * 1/(2*mean) return tf.reshape(data, [-1, 1]) 现在创建一个数值列的集合。tf.feature_columns.numeric_column API 会使用 normalizer_fn 参数。在传参的时候使用 functools.partial，functools.partial 由使用每个列的均值进行标准化的函数构成。 12345678910111213MEANS = &#123; &#x27;age&#x27; : 29.631308, &#x27;n_siblings_spouses&#x27; : 0.545455, &#x27;parch&#x27; : 0.379585, &#x27;fare&#x27; : 34.385399&#125;numerical_columns = []for feature in MEANS.keys(): num_col = tf.feature_column.numeric_column(feature, normalizer_fn=functools.partial(process_continuous_data, MEANS[feature])) numerical_columns.append(num_col) 接下来创建预处理层： 1preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numerical_columns) 然后基于预处理层构建并编译模型： 1234567891011model = tf.keras.Sequential([ preprocessing_layer, tf.keras.layers.Dense(128, activation=&#x27;relu&#x27;), tf.keras.layers.Dense(128, activation=&#x27;relu&#x27;), tf.keras.layers.Dense(1, activation=&#x27;sigmoid&#x27;),])model.compile( loss=&#x27;binary_crossentropy&#x27;, optimizer=&#x27;adam&#x27;, metrics=[&#x27;accuracy&#x27;]) 接下来，我们就可以实例化和训练模型： 123train_data = raw_train_data.shuffle(500)test_data = raw_test_datamodel.fit(train_data, epochs=20) 训练完成后，我们可以在测试集上检查准确性： 123test_loss, test_accuracy = model.evaluate(test_data)print(&#x27;\\n\\nTest Loss &#123;&#125;, Test Accuracy &#123;&#125;&#x27;.format(test_loss, test_accuracy))&gt;&gt; Test Loss 0.44521663270213385, Test Accuracy 0.814393937587738 可以发现，该模型的预测准确率是81%。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://blog.zsstrike.top/tags/TensorFlow/"}]},{"title":"SSR更新PAC文件","slug":"SSR更新PAC文件","date":"2020-02-11T05:03:23.000Z","updated":"2020-12-17T10:23:43.554Z","comments":true,"path":"2020/02/11/SSR更新PAC文件/","link":"","permalink":"http://blog.zsstrike.top/2020/02/11/SSR%E6%9B%B4%E6%96%B0PAC%E6%96%87%E4%BB%B6/","excerpt":"SSR项目已经不再维护，它的PAC文件更新功能已经失效，本文我们将gfwlist.txt转换为pac.txt给SSR软件使用。","text":"SSR项目已经不再维护，它的PAC文件更新功能已经失效，本文我们将gfwlist.txt转换为pac.txt给SSR软件使用。 虽然原来的PAC地址已经失效了，但是gfwlist项目组维护了被墙的网站，GitHub地址：https://github.com/gfwlist/gfwlist/ 。首先，我们下载gfwlist.txt： 1curl https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt &gt; gfwlist.txt 接下来，我们需要安装Python的包genpac： 1pip install genpac 安装完成后，使用genpac将文件gfwlist.txt转换为pac.txt： 1genpac --pac-proxy=&quot;SOCKS 127.0.0.1:1080&quot; --gfwlist-local=&quot;./gfwlist.txt&quot; -o pac.txt 接下来将生成的pac.txt文件覆盖掉原来SSR软件的pac.txt即可。","categories":[],"tags":[{"name":"SSR","slug":"SSR","permalink":"http://blog.zsstrike.top/tags/SSR/"}]},{"title":"实现多层人工神经网络","slug":"实现多层人工神经网络","date":"2020-02-09T08:05:41.000Z","updated":"2020-12-17T10:23:43.599Z","comments":true,"path":"2020/02/09/实现多层人工神经网络/","link":"","permalink":"http://blog.zsstrike.top/2020/02/09/%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%B1%82%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","excerpt":"本章中，我们将会学习人工神经网络的基本概念以帮助我们学习后面几章中的内容。","text":"本章中，我们将会学习人工神经网络的基本概念以帮助我们学习后面几章中的内容。 使用人工神经网络对复杂函数建模我们在第二章中从人工神经元入手，开始了机器学习算法的探索。对于本章中将要讨论的多层人工神经网络来说，人工神经元是其构建的基石。 单层神经网络回顾先来回顾一下自适应线性神经元（Adaline）算法： 1581236309775 我们实现了二分类类别的Adaline算法，并通过梯度下降优化算法来学习模型的权重系数：$$w:=w+\\Delta w,其中\\Delta w = -\\eta \\nabla J(w)$$在梯度下降优化过程中，我们在每次迭代后同时更新所有权重。此外，将激励函数定义为：$$\\phi(z)=z=a$$其中，净输入z时输入和权重的线性组合，使用激励函数来计算梯度更新时，我们定义了一个阈值函数将连续的输出值转换为二类别分类的预测类标：$$\\hat{y}=\\begin{cases}1 &amp; 若g(z) \\ge 0\\-1 &amp; 其他\\end{cases}$$ 多层神经网络架构简介本节中，我们将会看到如何将多个单独的神经元连接为一个多层前反馈神经网络。这种特殊的网络也被称作是多层感知器（MLP）。MLP的示例图如下： 1581237705175 MLP包含一个输入层，一个隐层以及一个输出层。如果这样的网络中包含不只一个隐层，我们称其为深度神经网络。如图所示，我们将第l层中第i个激励单元记作$ a_i^l $，同时我们将激励单元$ a_0^{in} $和$ a_0^{h} $为偏置单元（bias unit），我们均设定为1。输入层各单元的激励为输入加上偏置单元：$$a^{in} = \\begin{bmatrix}a^{in}0 \\a^{in}_1 \\\\vdots \\a^{in}_m\\end{bmatrix}$$对于第l层的各单元，均通过一个权重系数连接到$ l+1 $层中的所有单元上。如连接第l层中第k个单元与第$ l+1 $层中第j个单元的连接可记为$ w{j,k}^l $。下图是一个3-4-3多层感知器： 1581239165942 通过正向传播构造神经网络本节中，我们将使用正向传播来计算多层感知器（MLP）模型的输出。我们将多层感知器的学习过程总结为三个步骤： 从输入层开始，通过网络向前传播（也就是正向传播）训练数据中的模式，以生成输出 基于网络的输出，通过一个代价函数计算所需最小化的误差 反向传播误差，计算其对于网络中每个权重的导数，并且更新模型 最终通过多层感知器模型权重的多次迭代和学习，我们使用正向传播来计算输出，并使用阈值函数获得独热法所表示的预测类标。 现在，我们根据正向传播算法逐步从训练数据的模式中生成一个输出。由于隐层每个节点均完全连接到所有输入层节点，我们首先通过以下公式计算$ a_1^2 $的激励：$$z_1^2 = a_0^1w_{1,0}^1+a_1^1w_{1,1}^1+\\cdots+a_m^1w_{1,m}^1\\a_1^2 = \\phi(z_1^2)$$激励函数可以使用sigmoid激励函数以解决图像分类等复杂问题。 多层感知器是一个典型的前馈人工神经网络，此处的前馈指的是每一层的输出都直接作为下一层的输入。为了提高代码的执行效率和可读性，我们将使用线性代数中的基本概念：$$Z^2 = W^1[A^1]^T$$接下来我们可以将激励函数$ \\phi(\\cdot) $应用于净输入矩阵中的每个值，便于获取下一个激励矩阵$ A^2 $:$$A^2 = \\phi(Z^2)$$类似地，我们以向量的形式重写输入层的激励：$$Z^3 = W^2A^2$$最后，通过sigmoid激励函数，我们可以得到神经网络的连续型输出：$$A^3 = \\phi(Z^3)$$ 手写数字的识别接下来我们看一下神经网络在实际中的应用，通过MNIST数据集上对手写数字的识别，来完成我们第一个多层神经网络的训练。MNIST是机器学习算法中常用的一个基准数据集。 获取MNIST数据集MNIST数据集可以通过链接http://yann.lecun.com/exdb/mnist/下载，包含下列四个部分： 训练集图像：train-images-idx3-ubyte.gz 训练集类标：train-labels-idx1-ubyte.gz 测试集图像：t10k-images-idx3-ubyte.gz 测试集类标：t10k-labels-idx1-ubyte.gz 下载完数据后并解压，接下来将其读入数组并且用于训练感知器模型： 1234567891011121314import osimport structimport numpy as npdef load_mnist(path, kind=&#x27;train&#x27;): labels_path = os.path.join(path, &#x27;%s-labels-idx1-ubyte&#x27; % kind) images_path = os.path.join(path, &#x27;%s-images-idx3-ubyte&#x27; % kind) with open(labels_path, &#x27;rb&#x27;) as lbpath: magic, c = struct.unpack(&#x27;&gt;II&#x27;, lbpath.read(8)) labels = np.fromfile(lbpath, dtype=np.uint8) with open(images_path, &#x27;rb&#x27;) as imgpath: magic, num, rows, cols = struct.unpack(&#x27;&gt;IIII&#x27;, imgpath.read(16)) images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784) return images, labels load_mnist函数返回值返回两个数组，第一个是$ n\\times m $维NumPy数组（存储图像），返回的第二个数组（类标）包含对应的目标变量，也即手写数字对应的类标，struct.unpack函数中的fmt参数的实参值：&gt;II。&gt;这是表示大端字节序，I表示一个无符号整数。 接下来我们读取数据： 123456X_train, y_train = load_mnist(&#x27;mnist&#x27;, kind=&#x27;train&#x27;)print(&#x27;Rows: %d, columns: %d&#x27; % (X_train.shape[0], X_train.shape[1]))X_test, y_test = load_mnist(&#x27;mnist&#x27;, kind=&#x27;t10k&#x27;)print(&#x27;Rows: %d, columns: %d&#x27; % (X_test.shape[0], X_test.shape[1]))&gt;&gt; Rows: 60000, columns: 784&gt;&gt; Rows: 10000, columns: 784 为了解MNIST数据集中图像的样子，我们可以将特征矩阵中的784像素向量还原为$ 28 \\times 28 $图像： 12345678910import matplotlib.pyplot as pltfig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)ax = ax.flatten()for i in range(10): img = X_train[y_train==i][0].reshape(28, 28) ax[i].imshow(img, cmap=&#x27;Greys&#x27;, interpolation=&#x27;nearest&#x27;)ax[0].set_xticks([])ax[0].set_yticks([])plt.tight_layout()plt.show() 图像如下： img 此外，我们绘制一下相同数字的多个示例： 123456789fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)ax = ax.flatten()for i in range(25): img = X_train[y_train==7][i].reshape(28, 28) ax[i].imshow(img, cmap=&#x27;Greys&#x27;, interpolation=&#x27;nearest&#x27;)ax[0].set_xticks([])ax[0].set_yticks([])plt.tight_layout()plt.show() 图像如下： img 实现一个多层感知器接下来，我们实现一个包含一个输入层，一个隐层和一个输出层的多层感知器，并且将其用来识别MNIST数据集中的图像，整体代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596import numpy as npimport sysclass NeuralNetMLP(object): def __init__(self, n_hidden=30, l2=0., epochs=100, eta=0.001, shuffle=True, minibatch_size=1, seed=None): self.random = np.random.RandomState(seed) self.n_hidden = n_hidden self.l2 = l2 self.epochs = epochs self.eta = eta self.shuffle = shuffle self.minibatch_size = minibatch_size def _onehot(self, y, n_classes): onehot = np.zeros((n_classes, y.shape[0])) for idx, val in enumerate(y.astype(int)): onehot[val, idx] = 1 return onehot.T def _sigmoid(self, Z): return 1. / (1. + np.exp(-np.clip(z, -250, 250))) def _forward(self, X): # step 1: net input of hidden layer z_h = np.dot(X, self.w_h) + self.b_h # step 2: activation of hidden layer a_h = self._sigmoid(z_h) # step 3: net input of output layer z_out = np.dot(a_h, self.w_out) + self.b_out # step 4: activation output layer a_out = self._sigmoid(z_out) return z_h, a_h, z_out, a_out def _compute_cost(self, y_enc, output): L2_term = (self.l2 * (np.sum(self.w_h ** 2.) + np.sum(self.w_out ** 2.))) term1 = -y_enc * (np.log(output)) term2 = (1. - y_enc) * np.log(1. - output) cost = np.sum(term1 - term2) + L2_term return cost def predict(self, X): z_h, a_h, z_out, a_out = self._forward(X) y_pred = np.argmax(z_out, axis=1) return y_pred def fit(self, X_train, y_train, X_valid, y_valid): n_output = np.unique(y_train).shape[0] n_features = X_train.shape[1] # weight initialization # weights for input -&gt; hidden self.b_h = np.zeros(self.n_hidden) self.w_h = self.random.normal(loc=0.0, scale=0.1, size=(n_features, self.n_hidden)) # weights for hidden -&gt; output self.b_out = np.zeros(n_output) self.w_out = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden, n_output)) epoch_strlen = len(str(self.epochs)) self.eval_ = &#123;&#x27;cost&#x27;: [], &#x27;train_acc&#x27;: [], &#x27;valid_acc&#x27;: []&#125; y_train_enc = self._onehot(y_train, n_output) # iteration over training epochs for i in range(self.epochs): indices = np.arange(X_train.shape[0]) if self.shuffle: self.random.shuffle(indices) for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size): batch_idx = indices[start_idx:start_idx + self.minibatch_size] z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx]) # Backpropagation sigma_out = a_out - y_train_enc[batch_idx] sigmoid_derivative_h = a_h * (1. - a_h) sigma_h = (np.dot(sigma_out, self.w_out.T) * sigmoid_derivative_h) grad_w_h = np.dot(a_h.T, sigma_out) grad_b_out = np.sum(sigma_out, axis=0) delta_w_h = (grad_w_h + self.l2*self.w_h) delta_b_h = grad_b_h # bias is not regularized self.w_h -= self.eta * delta_w_h self.b_h -= self.eta * delta_b_h delta_w_out = (grad_w_out + self.l2*self.w_out) delta_b_out = grad_b_out # bias is not regularized self.w_out -= self.eta * delta_w_out self.b_out -= self.eta * delta_b_out # evaluation z_h, a_h, z_out, a_out = self._forward(X_train) cost = self._compute_cost(y_enc=y_train_enc, output=a_out) y_train_pred = self.predict(X_train) y_valid_pred = self.predict(X_valid) train_acc = ((np.sum(y_train ==y_train_pred)).astype(np.float) / X_train.shape[0]) valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) / X_valid.shape[0]) sys.stderr.write(&#x27;\\r%0*d/%d | Cost: %.2f &#x27;&#x27;| Train/Valid Acc.: %.2f%%/%.2f%% &#x27; % (epoch_strlen, i+1, self.epochs, cost, train_acc*100, valid_acc*100)) sys.stderr.flush() self.eval_[&#x27;cost&#x27;].append(cost) self.eval_[&#x27;train_acc&#x27;].append(train_acc) self.eval_[&#x27;valid_acc&#x27;].append(valid_acc) return self 接下来我们初始化一下784-100-10的MLP： 12nn = NeuralNetMLP(n_hidden=100, l2=0.01, epochs=200, eta=0.0005, minibatch_size=100, shuffle=True, seed=1) 首先看一下参数的含义： l2:：l2正则化系数$ \\lambda $ epochs：遍历训练集的次数（遍历次数） eta：学习速率$ \\eta $ shuffle：每次迭代前打乱训练集的数据 seed：打乱数据和权重初始化的随机种子 minibatch_size：在每个小批次中训练样本的数目 梯度每个批次分别计算，而不是在整个训练数据集上进行计算，这样做是为了加快学习的速率。 接下来进行训练： 123nn.fit(X_train=X_train[:55000], y_train=y_train[:55000], X_valid=X_train[55000:], y_valid=y_train[55000:])&gt;&gt; 200/200 | Cost: 15345.39 | Train/Valid Acc.: 96.10%/96.40% 我们在上述实现中，我们也定义了eval_用来保存每次迭代后的代价值，我们将其绘制出来： 12345import matplotlib.pyplot as pltplt.plot(range(nn.epochs), nn.eval_[&#x27;cost&#x27;])plt.xlabel(&#x27;Cost&#x27;)plt.ylabel(&#x27;Epochs&#x27;)plt.show() 得到的图像如下： img 可以得到前100次cost的值下降得很快，之后随着迭代次数增加，cost值下降不明显。 接下来看一下训练和验证率得变化： 123456plt.plot(range(nn.epochs), nn.eval_[&#x27;train_acc&#x27;], label=&#x27;training&#x27;)plt.plot(range(nn.epochs), nn.eval_[&#x27;valid_acc&#x27;], label=&#x27;validation&#x27;, linestyle=&#x27;--&#x27;)plt.xlabel(&#x27;Accuracy&#x27;)plt.ylabel(&#x27;Epochs&#x27;)plt.legend()plt.show() 图像如下： img 可以发现在迭代次数175之前，拟合模型有点欠拟合。最后我们看一下预测准确率： 1234y_test_pred = nn.predict(X_test)acc = (np.sum(y_test == y_test_pred)).astype(np.float) / X_test.shape[0]print(&#x27;Acc: %.3f&#x27; % acc)&gt;&gt; Acc: 0.959 可以发现我们的模型在测试集上准确率差不多是96%，在数值上接近训练集中验证的准确率，表明模型拟合程度较好。 最后，看一下一些图片和我们MLP预测结果的示例图： 12345678910111213miscl_img = X_test[y_test != y_test_pred][125:150]correct_lab = y_test[y_test != y_test_pred][125:150]miscl_lab = y_test_pred[y_test != y_test_pred][25:50]fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)ax = ax.flatten()for i in range(25): img = miscl_img[i].reshape(28, 28) ax[i].imshow(img, cmap=&#x27;Greys&#x27;, interpolation=&#x27;nearest&#x27;) ax[i].set_title(&#x27;%d) t: %d p: %d&#x27; % (i+1, correct_lab[i], miscl_lab[i]))ax[0].set_xticks([])ax[0].set_yticks([])plt.tight_layout()plt.show() 得到的图像如下： img 图片上的第二个数字表示的是正确的类标（true class），第三个数字表示的是预测的类标（predicted class）。可以发现，某些图像即便让人工分类也存在一定的困难度。 训练人工神经网络接下来我们看一下人工神经网络的一些深层的概念，如用于权值更新过程中的逻辑斯蒂代价函数和反向传播算法。 计算逻辑斯蒂代价函数在_compute_cost方法中实现的逻辑斯蒂代价函数如下：$$J(w) = -\\sum_{i=1}^{n}y^ilog(a^i)+(1-y^i)log(1-a^i)$$其中，$ a^i $是前向传播过程中，用来计算第i个单元的sigmoid激励函数：$$a^i = \\phi(z^i)$$接下来，我们添加一个正则化项，它可以降低过拟合的程度，L2正则化定义如下：$$L2：=\\lambda ||w||^2_2 = \\lambda\\sum_{j=1}^{m}w_j^2$$通过在逻辑斯蒂代价函数中加入L2正则化项，得到：$$J(w) = -\\sum_{i=1}^{n}y^ilog(a^i)+(1-y^i)log(1-a^i) =\\lambda ||w||^2_2$$我们已经实现了一个用于多分类的MLP，它返回一个包含t个元素的输出向量，我们需要将这个输出向量和使用独热编码表示的 $ t \\times 1 $维目标向量进行比较。例如，对于一个样本，它在第三层的激励和目标类别（此处是2）可能如下：$$a^3 = \\begin{bmatrix}0.1 \\0.9 \\\\vdots \\0.3\\end{bmatrix},y = \\begin{bmatrix}0 \\1 \\\\vdots \\0\\end{bmatrix}$$由此，我们需要逻辑斯蒂函数应用到网络中的所有激励单元j中。因此代价函数（未增加正则化项）：$$J(w) = -\\sum_{i=1}^{n}\\sum_{j=1}^{t}y^i_jlog(a^i_j)+(1-y^i_j)log(1-a^i_j)$$这里，上标i表示的是第在训练集中的第i个样本。加入正则化项的公式如下：$$J(w) = -\\left[\\sum_{i=1}^{n}\\sum_{j=1}^{t}y^i_jlog(a^i_j)+(1-y^i_j)log(1-a^i_j)\\right] \\frac{\\lambda}{2}\\sum_{l=1}^{L-1}\\sum_{i=1}^{u_l}\\sum_{j=1}^{u_l+1}(w_{j,i}^l)^2$$在这里，$ u_l $表示第$ l $层的数目。我们的目标是最小化$ j(W) $代价函数，因此我们需要计算出网络中各层权重的偏导：$$\\frac{\\partial}{\\partial{w_{j,i}^l}}J(W)$$注意$ W $包含多个矩阵，在一个仅仅包含一个隐层单元的MLP中，$ W^h $连接输入层和隐层，$ W^{out} $连接隐层和输出层。下图对$ W $进行可视化： 1581315911089 通过反向传播来训练神经网络回忆本章中介绍的内容，我们需要通过正向传播来获得输出层的激励：$$Z^h = A^{in}W^h (隐层的净输入)\\A^h = \\phi(Z^h) (隐层的激励)\\Z^{out} = A^hW^{out} (输出层的净输出)\\A^{out} = \\phi(Z^{out})(输出层的激励)$$简单说，我们按照下图处理输入： 1581317066461 后向传播中，我们将误差从右向左传递。首先计算输出层的误差向量：$$\\delta^{out} = a^{out} - y$$其中，$ y $是真实类标的向量。接下来，我们计算隐层的误差项：$$\\delta^{h} = \\delta^{out}(W^{out})^T \\odot \\frac{\\partial\\phi(z^h)}{\\partial z^h}$$这里，$ \\frac{\\partial\\phi(z^h)}{\\partial z^h} $计算公式如下：$$\\frac{\\partial\\phi(z^h)}{\\partial z^h} = \\left(a^h \\odot (1-a^h)\\right)$$在这里，$ \\odot $表示的是数组元素依次相乘符号。 相应的，$ \\delta^h $计算公式如下：$$\\delta^{h} = \\delta^{out}(W^{out})^T \\odot \\left(a^h \\odot (1-a^h)\\right)$$在得到$ \\delta $后，我们可以将代价函数的偏导记作：$$\\frac{\\partial}{\\partial w_{i, j}^{out}}J(W) = a_j^h\\delta_i^{out}\\\\frac{\\partial}{\\partial w_{i, j}^h}J(W) = a_j^{in}\\delta_i^h$$综上，我们通过下图进行反向传播总结： 1581317962693 神经网络的收敛性在前面实现的训练手写数字的神经网络过程中，没有使用传统的梯度下降，而是使用小批次样本学习来替代。随机梯度下降每次仅使用一个样本（k=1）更新权重来进行，虽然这是一种随机的方法，但相较于传统梯度下降，它通常嗯能得到精度极高的训练结果，并且收敛速度更快。子批次学习是随机梯度下降的一个特例：从包含n个样本的训练数据中随机抽取k个用于训练，其中1&lt;k&lt;n。 神经网络的输出函数的曲线并不平滑，而且容易陷入局部最优值，如下图： 1581318745918","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"聚类分析之处理无类标数据","slug":"聚类分析之处理无类标数据","date":"2020-02-09T03:36:44.000Z","updated":"2020-12-17T10:23:43.706Z","comments":true,"path":"2020/02/09/聚类分析之处理无类标数据/","link":"","permalink":"http://blog.zsstrike.top/2020/02/09/%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E4%B9%8B%E5%A4%84%E7%90%86%E6%97%A0%E7%B1%BB%E6%A0%87%E6%95%B0%E6%8D%AE/","excerpt":"前面几章中，我们使用的数据都是事先已经直到预测结果的，即训练数据中已提供了数据的类标。在本章中，我们转而研究聚类分析，它是一种无监督学习技术，可以在事先不知道正确结果的情况下，发现数据本身所蕴含的结构等信息。","text":"前面几章中，我们使用的数据都是事先已经直到预测结果的，即训练数据中已提供了数据的类标。在本章中，我们转而研究聚类分析，它是一种无监督学习技术，可以在事先不知道正确结果的情况下，发现数据本身所蕴含的结构等信息。 使用k-means算法对相似对象进行分组本节讨论最流行的聚类算法：k-means算法，它在学术邻域及业界都得到了广泛应用。聚类是一种可以找到相似对象群组的技术，与组间对象相比，组内对象之间具有更高的相似度。 尽管k-means算法适用于高维数据，但出于可视化需要，我们使用一个二维数据集的例子演示： 123456789101112from sklearn.datasets import make_blobsimport matplotlib.pyplot as pltX, y = make_blobs(n_samples=150, n_features=2, centers=3, cluster_std=0.5, shuffle=True, random_state=0)plt.scatter(X[:, 0], X[:, 1], c=&#x27;blue&#x27;, marker=&#x27;o&#x27;, s=50)plt.grid()plt.show() 图像如下： img k-means算法具体有四个步骤： 从样本点随机选择k个点作为初始簇中心 将每个样本点划分到距离它最近的中心点$ \\mu^j, j\\in{1,\\cdots ,k} $所代表的簇中 用各簇中所有样本的中心点替代原有的中心点 重复步骤2和3，直到中心点不变或者达到预定迭代次数时，算法终止 度量对象之间的相似性可以用欧几里得距离的平方：$$d(x, y)^2 = \\sum_{j=1}^{m}(x_j-y_j)^2=||x-y||^2_2$$基于欧几里得标准，我们可以将k-means算法描述为一个简单的优化问题，也就是使得簇内误差平方和（SSE）最小：$$SSE = \\sum_{j=1}^n\\sum_{j=1}^{k}w^{i,j}=||x^i-\\mu^j||_2^2$$现在借助scikit-learn中的KMeans类将k-means算法应用于我们的示例数据集： 12345678from sklearn.cluster import KMeanskm = KMeans(n_clusters=3, init=&#x27;random&#x27;, n_init=10, max_iter=300, tol=1e-04, random_state=0)y_km = km.fit_predict(X) 在k-means算法的某次迭代中，可能会发生无法收敛的问题，特别是我们设置了较大的max_iter。解决这个问题的方法是为tol参数设置一个较大的值，上述容忍度为1e-04。 k-means++我们讨论了经典的k-means算法，它使用随机点作为初始中心点，但是初始中心点选择不当，就会导致收敛速度慢的问题。解决此问题的方法是在数据集上多次运行k-mean算法，并且根据SSE选择性能更好的模型。另外一种方案是使用k-means++算法让初始中心点彼此尽可能远离，相比传统的k-means算法，它能够产生更好的结果。k-means++算法的初始化过程如下： 初始化一个空的集合M，用于存储选定的k个中心点 从输入样本中随机选定第一个中心点$ \\mu^j $，并且将其加入到集合M中 对于集合M之外的任一样本点$ x^i $，通过计算找到与其平方距离最小的样本$ d(x^i, M)^2 $ 使用加权概率分布$ \\frac{d(\\mu, M)^2}{\\sum_id(x^i, M)^2} $来随机选择下一个中心点$ \\mu^p $ 重复步骤2，3，直到选定k个中心点 基于选定的中心点执行k-means算法 现在对k-means算法的结果做可视化展示： 123456789101112131415161718192021222324252627plt.scatter(X[y_km==0, 0], X[y_km==0, 1], s=50, c=&#x27;lightgreen&#x27;, marker=&#x27;s&#x27;, label=&#x27;cluster 1&#x27;)plt.scatter(X[y_km==1, 0], X[y_km==1, 1], s=50, c=&#x27;orange&#x27;, marker=&#x27;o&#x27;, label=&#x27;cluster 2&#x27;)plt.scatter(X[y_km==2, 0], X[y_km==2, 1], s=50, c=&#x27;lightblue&#x27;, marker=&#x27;v&#x27;, label=&#x27;cluster 3&#x27;)plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=250, marker=&#x27;*&#x27;, c=&#x27;red&#x27;, label=&#x27;centroids&#x27;)plt.legend()plt.grid()plt.show() 图像如下： img 散点图显示的结果中3个中心点位于各个簇的中心，分组结果看起来是合理的。 k-means算法的一个缺点是我们必须先指定一个簇数量k，但是在实际应用中，簇数量并不总是显而易见的。 硬聚类与软聚类硬聚类指每个样本只能划至一个簇的算法，如k-means算法；软聚类算法可以将样本划分到一个或多个簇，如FCM算法。 FCM算法和k-means算法十分相似，k-means算法某个样本预测结果是$ [0,1,0] $，表明该样本属于簇2。FCM中可以允许预测的结果中含有分数，如$ [0.7, 0.2, 0.1] $表明该样本属于簇1的概率是0.7，簇2的概率是0.2。FCM的步骤如下： 指定k个中心点，并随机将样本点划分至某个簇 计算各个簇的中心$ \\mu^j ,j\\in{1, \\cdots,k}$ 更新各样本点所属簇的成员隶属度 重复步骤2，3，直到各个样本点所属簇成员隶属度不变或者是达到最大的迭代次数 FCM的目标函数如下：$$J_m = \\sum_{i=1}^{n}\\sum_{j=1}^{m}w^m(i,j)||x^i-\\mu^j||^2_2$$ 使用肘方法确定簇的最佳数量簇内误差平方和可以通过inertia访问，基于簇内误差平方和，我们可以使用图形工具，即所谓的肘方法，针对特定任务估计出最优的簇数量k。 12345678910111213distortions = []for i in range(1, 11): km = KMeans(n_clusters=i, init=&#x27;k-means++&#x27;, n_init=10, max_iter=300, random_state=0) km.fit(X) distortions.append(km.inertia_)plt.plot(range(1, 11), distortions, marker=&#x27;o&#x27;)plt.xlabel(&#x27;Number of clusters&#x27;)plt.ylabel(&#x27;Distortion&#x27;)plt.show() 图像如下： img 如图，当k=3时团呈现肘形，这表明对于此数据来说，k=3是一个好的选择。 通过轮廓图定量分析聚类质量另外一种聚类质量的评估方法时轮廓分析，此方法用于k-means算法之外的其他聚类方法。我们通过以下步骤计算轮廓系数： 将某样本$ x^i $与簇内其他点之间的平均距离看作是簇的内聚度$ a^i $ 将样本$ x^i $与其最近簇中所有点之间的平均距离看作是与下一最近簇的分离度$ b^i $ 轮廓系数如下：$$s^i = \\frac{b^i - a^i}{max{b^i, a^i}}$$ 可以发现，理想的轮廓系数时1，轮廓系数代码如下： 1234567891011121314151617181920212223242526272829303132333435363738km = KMeans(n_clusters=3, init=&#x27;k-means++&#x27;, n_init=10, max_iter=300, tol=1e-04, random_state=0)y_km = km.fit_predict(X)import numpy as npfrom matplotlib import cmfrom sklearn.metrics import silhouette_samplescluster_labels = np.unique(y_km)n_clusters = cluster_labels.shape[0]silhouette_vals = silhouette_samples(X, y_km, metric=&#x27;euclidean&#x27;)y_ax_lower, y_ax_upper = 0, 0yticks = []for i, c in enumerate(cluster_labels): c_silhouette_vals = silhouette_vals[y_km == c] c_silhouette_vals.sort() y_ax_upper += len(c_silhouette_vals) color = cm.jet(float(i) / n_clusters) plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor=&#x27;none&#x27;, color=color) yticks.append((y_ax_lower + y_ax_upper) / 2.) y_ax_lower += len(c_silhouette_vals)silhouette_avg = np.mean(silhouette_vals)plt.axvline(silhouette_avg, color=&quot;red&quot;, linestyle=&quot;--&quot;)plt.yticks(yticks, cluster_labels + 1)plt.ylabel(&#x27;Cluster&#x27;)plt.xlabel(&#x27;Silhouette coefficient&#x27;)plt.show() 图像如下： img 从上图可见，轮廓系数未接近0点，此指标显示聚类效果不错。为了解聚类效果不佳的轮廓图的形状，我们使用两个中心点来初始化k-means算法： 1234567891011121314151617181920212223242526272829km = KMeans(n_clusters=2, init=&#x27;k-means++&#x27;, n_init=10, max_iter=300, tol=1e-04, random_state=0)y_km = km.fit_predict(X)plt.scatter(X[y_km==0, 0], X[y_km==0, 1], s=50, c=&#x27;lightgreen&#x27;, marker=&#x27;s&#x27;, label=&#x27;cluster 1&#x27;)plt.scatter(X[y_km==1, 0], X[y_km==1, 1], s=50, c=&#x27;orange&#x27;, marker=&#x27;o&#x27;, label=&#x27;cluster 2&#x27;)plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=250, marker=&#x27;*&#x27;, c=&#x27;red&#x27;, label=&#x27;centroids&#x27;)plt.legend()plt.grid()plt.show() 图像如下： img 接下来，我们绘制轮廓图来对聚类结果进行评估： 123456789101112131415161718192021222324252627cluster_labels = np.unique(y_km)n_clusters = cluster_labels.shape[0]silhouette_vals = silhouette_samples(X, y_km, metric=&#x27;euclidean&#x27;)y_ax_lower, y_ax_upper = 0, 0yticks = []for i, c in enumerate(cluster_labels): c_silhouette_vals = silhouette_vals[y_km == c] c_silhouette_vals.sort() y_ax_upper += len(c_silhouette_vals) color = cm.jet(float(i) / n_clusters) plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor=&#x27;none&#x27;, color=color) yticks.append((y_ax_lower + y_ax_upper) / 2.) y_ax_lower += len(c_silhouette_vals)silhouette_avg = np.mean(silhouette_vals)plt.axvline(silhouette_avg, color=&quot;red&quot;, linestyle=&quot;--&quot;)plt.yticks(yticks, cluster_labels + 1)plt.ylabel(&#x27;Cluster&#x27;)plt.xlabel(&#x27;Silhouette coefficient&#x27;)plt.show() 由结果可见，轮廓图由明显不同的长度和宽度，这说明该聚类并非最优结果： img 层次聚类本节中，我们将学习另外一种基于原型的聚类：层次聚类。层次聚类算法的优势在于：他能够使我们绘制出树状图，这有助于我们使用有意义的分类解释聚类结果。层次聚类的另外一个优势在于我们无需指定簇数量。 层次聚类有两种主要方法：凝聚层次聚类和分裂层次聚类。在凝聚层次聚类中，判定簇间距离的两个标准方法分别是单连接和全连接。单连接方法计算每一对簇中最相似两个样本的距离，并且合并距离最近的两个样本所属簇。与之相反，全连接方法是通过比较找到分布于两个簇中最不相似的样本（距离最远的样本），进而完成簇的合并。 本节中，我们主要关注基于全连接方法的凝聚层次聚类，迭代过程如下： 计算得到所有样本间的距离矩阵 将每个数据点看作是一个单独的簇 基于最不相似（距离最远）样本的距离，合并两个最接近的簇 更新相似矩阵 重复步骤2到4，直到所有样本都合并到一个簇为止 计算距离矩阵的方式如下： 12345678import pandas as pdimport numpy as npnp.random.seed(123)variables = [&#x27;X&#x27;, &#x27;Y&#x27;, &#x27;Z&#x27;]labels = [&#x27;ID_0&#x27;, &#x27;ID_1&#x27;, &#x27;ID_2&#x27;, &#x27;ID_3&#x27;, &#x27;ID_4&#x27;]X = np.random.random_sample([5, 3])*10df = pd.DataFrame(X, columns=variables, index=labels)df 得到的数据如下： 1581229979827 基于距离矩阵进行层次聚类我们基于SciPy来计算距离矩阵： 123from scipy.spatial.distance import pdist, squareformrow_dist &#x3D; pd.DataFrame(squareform(pdist(df, metric&#x3D;&#39;euclidean&#39;)), columns&#x3D;labels, index&#x3D;labels)row_dist 得到的数据如下： 1581230299131 接下来我们使用linkage函数，此函数以全连接作为距离判定标准： 1234from scipy.cluster.hierarchy import linkagerow_clusters = linkage(df.values, method=&#x27;complete&#x27;, metric=&#x27;euclidean&#x27;)pd.DataFrame(row_clusters, columns=[&#x27;row label 1&#x27;, &#x27;row label 2&#x27;, &#x27;distance&#x27;, &#x27;no. of items&#x27;], index=[&#x27;cluster %d&#x27; % (i+1) for i in range(row_clusters.shape[0])]) 得到的数据如下： 1581230747985 接下来采用树状图的形式对聚类结果进行可视化展示： 12345from scipy.cluster.hierarchy import dendrogramrow_dendr = dendrogram(row_clusters, labels=labels)plt.tight_layout()plt.ylabel(&#x27;Euclidean distance&#x27;)plt.show() 得到的图像如下： img 此树状图采用了凝聚层次聚类合并生成不同簇的过程，从图中可见，首先ID_0和ID_4合并，解下来是ID_1和ID_2合并。 树状图与热度图的关联实际应用中，层次聚类的树状图与热度图结合使用，本节中我们讨论如何将树状图附加到热度图上： 123456789101112131415fig = plt.figure(figsize=(8, 8))axd = fig.add_axes([0.09, 0.1, 0.2, 0.6])row_dendr = dendrogram(row_clusters, orientation=&#x27;left&#x27;)df_rowclust = df.ix[row_dendr[&#x27;leaves&#x27;][::-1]]axm = fig.add_axes([0.23, 0.1, 0.6, 0.6])cax = axm.matshow(df_rowclust, interpolation=&#x27;nearest&#x27;, cmap=&#x27;hot_r&#x27;)axd.set_xticks([])axd.set_yticks([])for i in axd.spines.values(): i.set_visible(False)fig.colorbar(cax)axm.set_xticklabels([&#x27;&#x27;] + list(df_rowclust.columns))axm.set_yticklabels([&#x27;&#x27;] + list(df_rowclust.index))plt.show() 得到图像可得： img 通过scikit-learn进行凝聚聚类本节使用scikit-learn进行基于凝聚的层次聚类： 12345from sklearn.cluster import AgglomerativeClusteringac = AgglomerativeClustering(n_clusters=2, affinity=&#x27;euclidean&#x27;, linkage=&#x27;complete&#x27;)labels = ac.fit_predict(X)print(&#x27;Cluster labels: %s&#x27; % labels)&gt;&gt; Cluster labels: [0 1 1 0 0] 通过对簇类标的预测结果进行分析，我们可以看出第一第四第五样本被划分至第一个簇，第二第三样本被划分到第二个簇。 使用DBSCAN划分高密度区域接下来我们介绍另外一种聚类方法：基于密度空间的聚类算法。在DBSCAN中，基于一下标准，每个样本都被赋予了一个特殊的标签： 如果一个点周边的指定半径$ \\epsilon $内，其他样本点的数量不小于指定数量（MinPts），则此样本点称为核心点（core point） 在指定半径$ \\epsilon $内，如果一个点的邻居数量小于MinPts时，但是却包含一个核心点，则此点称为边界点（border point） 除了核心点和边界点外的其他样本点称为噪声点（noise point） 完成对核心点，边界点和噪声点的标记后，DBSCAN算法可以总结为两个简单的步骤： 基于每个核心点或者一组相连的核心点形成一个单独的簇 将每个边界点划分到对应核心点所在的簇中 为了给出一个更能说明问题的例子，我们创建一个半月形的数据集，以及k-means聚类，层次聚类和DBSCAN聚类进行比较，首先得到半月形数据集： 1234from sklearn.datasets import make_moonsX, y = make_moons(n_samples=200, noise=0.05, random_state=0)plt.scatter(X[:, 0], X[:, 1])plt.show() 得到的图像如下： img 下面首先使用前面讨论过的k-means算法和基于全连接的层次聚类算法，算法如下： 1234567891011121314151617181920212223242526272829303132333435363738f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))km = KMeans(n_clusters=2, random_state=0)y_km = km.fit_predict(X)ax1.scatter(X[y_km==0,0], X[y_km==0,1], c=&#x27;lightblue&#x27;, edgecolor=&#x27;black&#x27;, marker=&#x27;o&#x27;, s=40, label=&#x27;cluster 1&#x27;)ax1.scatter(X[y_km==1,0], X[y_km==1,1], c=&#x27;red&#x27;, edgecolor=&#x27;black&#x27;, marker=&#x27;s&#x27;, s=40, label=&#x27;cluster 2&#x27;)ax1.set_title(&#x27;K-means clustering&#x27;)ac = AgglomerativeClustering(n_clusters=2, affinity=&#x27;euclidean&#x27;, linkage=&#x27;complete&#x27;)y_ac = ac.fit_predict(X)ax2.scatter(X[y_ac==0,0], X[y_ac==0,1], c=&#x27;lightblue&#x27;, edgecolor=&#x27;black&#x27;, marker=&#x27;o&#x27;, s=40, label=&#x27;cluster 1&#x27;)ax2.scatter(X[y_ac==1,0], X[y_ac==1,1], c=&#x27;red&#x27;, edgecolor=&#x27;black&#x27;, marker=&#x27;s&#x27;, s=40, label=&#x27;cluster 2&#x27;)ax2.set_title(&#x27;Agglomerative clustering&#x27;)plt.legend()plt.show() 得到的图像如下： img 可以发现，上述两个算法无法有效分开两个数组。最后我们试一下DBSCAN算法在此数据集上的效果： 123456789101112131415161718192021from sklearn.cluster import DBSCANdb = DBSCAN(eps=0.2, min_samples=5, metric=&#x27;euclidean&#x27;)y_db = db.fit_predict(X)plt.scatter(X[y_db==0,0], X[y_db==0,1], c=&#x27;lightblue&#x27;, edgecolor=&#x27;black&#x27;, marker=&#x27;o&#x27;, s=40, label=&#x27;cluster 1&#x27;)plt.scatter(X[y_db==1,0], X[y_db==1,1], c=&#x27;red&#x27;, edgecolor=&#x27;black&#x27;, marker=&#x27;s&#x27;, s=40, label=&#x27;cluster 2&#x27;)plt.legend()plt.show() 得到的图像如下： img 可以发现，DBSCAN算法可以成功地对半月形数据进行划分，这也是DBSCAN算法的优势。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"使用回归分析预测连续型目标变量","slug":"使用回归分析预测连续型目标变量","date":"2020-02-08T04:32:51.000Z","updated":"2020-12-17T10:23:43.586Z","comments":true,"path":"2020/02/08/使用回归分析预测连续型目标变量/","link":"","permalink":"http://blog.zsstrike.top/2020/02/08/%E4%BD%BF%E7%94%A8%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%E8%BF%9E%E7%BB%AD%E5%9E%8B%E7%9B%AE%E6%A0%87%E5%8F%98%E9%87%8F/","excerpt":"本章将会介绍监督学习的另外一个分支，回归分析（regression analysis）。回归模型可以用于连续型目标变量的预测分析，这使得它在探寻变量间关系，评估趋势，做出预测等领域极具吸引力。具体的例子如预测公司在未来几个月的销售情况等。","text":"本章将会介绍监督学习的另外一个分支，回归分析（regression analysis）。回归模型可以用于连续型目标变量的预测分析，这使得它在探寻变量间关系，评估趋势，做出预测等领域极具吸引力。具体的例子如预测公司在未来几个月的销售情况等。 简单线性回归模型初探简单（单变量）线性回归的目标是：通过模型来描述某一特征（解释变量x）与输出变量（目标变量y）之间的关系。当只有一个解释变量时，线性模型函数定义如下：$$y = w_0 + w_1x$$其中，$ w_0 $为函数在y轴上的截距，$ w_1 $为解释变量的系数。 基于前面定义的线性方程，线性回归可以看作是求解样本点的最佳拟合直线，如下图： 1581137289049 这条最佳拟合线称作是回归线，回归线和样本点之间的垂直连线就是偏移或残差。 多元线性回归函数定义如下：$$y = w_0x_0+w_1x_1+\\cdots+w_mx_m$$其中，$ w_0 $时$ x_0 =1 $时在y轴上的截距。 波士顿房屋数据集在本章的后续内容中，我们将会使用房屋价格（MEDV）作为目标变量，使用其他13个变量中的一个或多个值作为解释变量对其进行预测： 12345import pandas as pddf = pd.read_csv(&#x27;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data&#x27;, header=None, sep=&#x27;\\s+&#x27;)df.columns = [&#x27;CRIM&#x27;, &#x27;ZN&#x27;, &#x27;INDUS&#x27;, &#x27;CHAS&#x27;, &#x27;NOX&#x27;, &#x27;RM&#x27;, &#x27;AGE&#x27;, &#x27;DIS&#x27;, &#x27;RAD&#x27;, &#x27;TAX&#x27;, &#x27;PTRATIO&#x27;, &#x27;B&#x27;, &#x27;LSTAT&#x27;, &#x27;MEDV&#x27;]df.head() 输出如下： 1581140075569 搜索性数据分析（EDA）是机器学习模型训练前的一个重要步骤。首先，借助散点图矩阵，我们可以以可视化的方法汇总显示各不同特征两两之间的关系： 123456import matplotlib.pyplot as pltimport seaborn as snssns.set(style=&#x27;whitegrid&#x27;, context=&#x27;notebook&#x27;)cols = [&#x27;LSTAT&#x27;, &#x27;INDUS&#x27;, &#x27;NOX&#x27;, &#x27;RM&#x27;, &#x27;MEDV&#x27;]sns.pairplot(df[cols], size=2.5)plt.show() 得到的图像如下： fig 通过此散点图矩阵，我们可以快速了解数据是如何分布的，以及其中是否包含异常值。从右下角子图可以发现：MEDV看似呈正态分布，但是包含几个异常值。 为了量化特征之间的关系，我们创建一个相关系数矩阵。相关系数矩阵是一个包含皮尔逊积矩相关系数，它是用来衡量两两特征间的线性依赖关系。计算公式如下：$$r = \\frac{\\sum_{i=1}^{n}[(x^i - \\mu_x)(y^i-\\mu_y)]}{\\sqrt{\\sum_{i=1}^n(x^i-\\mu_x)^2}\\sqrt{\\sum_{i=1}^n(y^i-\\mu_y)^2}}=\\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}$$其中，$ \\mu $为样本特征的均值，$ \\sigma_{xy} $为相应的协方差，$ \\sigma_x,\\sigma_y $分别为两个特征的标准差。 通过以下代码，我们计算前5个特征间的相关系数矩阵： 1234567891011121314import numpy as npcm = np.corrcoef(df[cols].values.T)# sns.set(font_scale=1.5)hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt=&#x27;.2f&#x27;, annot_kws=&#123;&#x27;size&#x27;: 15&#125;, yticklabels=cols, xticklabels=cols)hm.set_ylim([5, 0])plt.show() 得到的图像如下： img 为了拟合线性回归模型，我们主要关注那些跟目标变量MEDV高度相关的特征。观察前面的相关系数矩阵，可以发现MEDV与变量LSTAT的相关性最大（-0.74）。另一方面，RM和MEDV间的相关性也较高（0.70）。 基于最小二乘法构建线性回归模型接下来我们需要对最优拟合做出判断，在此使用最小二乘法（Ordinary Least Square，OLS）估计回归曲线的参数，使得回归曲线到样本点垂直距离的平方和最小。 通过梯度下降计算回归参数在第二章中介绍的Adaline中使用了一个线性激励函数，同时定义了一个激励函数，可以通过梯度下降（GD），随机梯度下降（SGD）等优化算法使得代价函数最小，从而得到相应的权重。Adaline中的代价函数就是误差平方和（SSE），他等同于我们定义的OLS代价函数：$$J(w) = \\frac{1}{2}\\sum_{i=1}^n(y^i - \\hat{y^i})^2$$本质上，OLS线性回归可以理解为无单位阶跃函数的Adaline，这样我们的得到的是连续型的输出值，而不是-1或者1的类标。接下来可以看一下线性回归梯度下降代码： 123456789101112131415161718192021222324class LinearRegressionGD(object): def __init__(self, eta=0.001, n_iter=20): self.eta = eta self.n_iter = n_iter def fit(self, X, y): self.w_ = np.zeros(1 + X.shape[1]) self.cost_ = [] for i in range(self.n_iter): output = self.net_input(X) errors = (y - output) self.w_[1:] += self.eta * X.T.dot(errors) self.w_[0] +=self.eta * errors.sum() cost = (errors**2).sum() / 2.0 self.cost_.append(cost) return self def net_input(self, X): return np.dot(X, self.w_[1:] + self.w_[0]) def predict(self, X): return self.net_input(X) 接下来我们使用房屋数据集中的RM（房间数量）作为解释变量来训练模型以预测MEDV（房屋价格）。此外，为了使得梯度下降算法收敛性更佳，在此对相关变量做了标准化处理： 123456789X = df[[&#x27;RM&#x27;]].valuesy = df[&#x27;MEDV&#x27;].valuesfrom sklearn.preprocessing import StandardScalersc_x = StandardScaler()sc_y = StandardScaler()X_std = sc_x.fit_transform(X)y_std = sc_y.fit_transform(y[:, np.newaxis]).flatten()lr = LinearRegressionGD()lr.fit(X_std, y_std) 接下来看一下代价函数和迭代次数的图像： 1234plt.plot(range(1, lr.n_iter+1), lr.cost_)plt.ylabel(&#x27;SSE&#x27;)plt.xlabel(&#x27;Epoch&#x27;)plt.show() 得到的图像如下： img 接下来，绘制房间数和房屋价格的关系： 12345678def lin_regplot(X, y, model): plt.scatter(X, y, c=&#x27;b&#x27;) plt.plot(X, model.predict(X), color=&#x27;r&#x27;) return Nonelin_regplot(X_std, y_std, lr)plt.xlabel(&#x27;Average Number&#x27;)plt.ylabel(&#x27;Price&#x27;)plt.show() 得到的图像如下： img 从图中可知，随着房间数的增加，房价呈现上涨趋势。但是从图中可以看到，房间数在很多的情况下并不能很好解释房价。对于经过标准化处理的变量，它们的截距必定是0： 12print(&#x27;Slope: %.3f&#x27; % lr.w_[1])print(&#x27;Intercept: %.3f&#x27; % lr.w_[0]) 使用scikit-learn估计回归模型的系数下面，我们使用scikit-learn中的库实现回归分析： 1234567from sklearn.linear_model import LinearRegressionslr = LinearRegression()slr.fit(X, y)print(&#x27;Slope: %.3f&#x27; % slr.coef_[0])print(&#x27;Intercept: %.3f&#x27; % slr.intercept_)&gt;&gt; Slope: 9.102&gt;&gt; Intercept: -34.671 执行代码发现得到了不同的模型系数，现在绘制出图像： 1234lin_regplot(X, y, slr)plt.xlabel(&#x27;Average Number&#x27;)plt.ylabel(&#x27;Price&#x27;)plt.show() 得到的图像如下： img 从图中可以看出，总体结果与GD算法实现的模型是一致的。 使用RANSAC拟合高鲁棒性回归模型作为清除异常值的一种高鲁棒性回归方法，在此我们将学习随机抽样一致性（RANSAC）算法，使用数据的一个子集来进行回归模型的拟合。该算法流程如下： 从数据集中随机抽取样本构建内点集合类拟合模型 使用剩余数据对上一步得到的模型进行测试，并将落在预定公差范围内的样本点增至内带你集合中 使用全部内点集合数据再次进行模型的拟合 使用内点集合来估计模型的误差 如果模型性能达到了特定阈值或者迭代达到了预定次数，则算法中止，否则跳转到第1步 首先我们用RANSACRegressor对象来实现我们的线性模型： 12345678from sklearn.linear_model import RANSACRegressorfrom sklearn.linear_model import LinearRegressionransac = RANSACRegressor(LinearRegression(), max_trials=100, min_samples=50, residual_threshold=5.0, random_state=0)ransac.fit(X, y) 完成拟合后，我们接着来绘制内点和异常值图像： 1234567891011inlier_mask = ransac.inlier_mask_outlier_mask = np.logical_not(inlier_mask)line_X = np.arange(3, 10, 1)line_y_ransac = ransac.predict(line_X[:, np.newaxis])plt.scatter(X[inlier_mask], y[inlier_mask], c=&#x27;b&#x27;, marker=&#x27;o&#x27;, label=&#x27;Inliers&#x27;)plt.scatter(X[outlier_mask], y[outlier_mask], c=&#x27;r&#x27;, marker=&#x27;s&#x27;, label=&#x27;Outliers&#x27;)plt.plot(line_X, line_y_ransac, color=&#x27;g&#x27;)plt.xlabel(&#x27;Average Number&#x27;)plt.ylabel(&#x27;Price&#x27;)plt.legend()plt.show() 图像如下： img 接下来看一下模型的截距和斜率： 1234print(&#x27;Slope: %.3f&#x27; % ransac.estimator_.coef_[0])print(&#x27;Intercept: %.3f&#x27; % ransac.estimator_.intercept_)&gt;&gt; Slope: 10.735&gt;&gt; Intercept: -44.089 线性回归模型性能的评估现在我们使用数据集中的所有变量训练多元回归模型： 12345678from sklearn.model_selection import train_test_splitX = df.iloc[:, :-1].valuesy = df[&#x27;MEDV&#x27;].valuesX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)slr = LinearRegression()slr.fit(X_train, y_train)y_train_pred = slr.predict(X_train)y_test_pred = slr.predict(X_test) 使用如下代码，我们会绘制出残差图： 12345678plt.scatter(y_train_pred, y_train_pred - y_train, c&#x3D;&#39;b&#39;, marker&#x3D;&#39;o&#39;, label&#x3D;&#39;Training data&#39;)plt.scatter(y_test_pred, y_test_pred - y_test, c&#x3D;&#39;r&#39;, marker&#x3D;&#39;s&#39;, label&#x3D;&#39;Test data&#39;)plt.xlabel(&#39;Predicted values&#39;)plt.ylabel(&#39;Residuals&#39;)plt.legend()plt.hlines(y&#x3D;0, xmin&#x3D;-10, xmax&#x3D;50, lw&#x3D;2, color&#x3D;&#39;red&#39;)plt.xlim([-10, 50])plt.show() 得到的残差图如下： img 完美的预测结果其残差为0，但是在实际的应用中，这种情况不会出现。不过，对于一个好的回归模型，我们期望误差是随机分布在中心线附近的。 另外一种对模型性能进行定量评估的方法是均方误差（Mean Squared Error，MSE），计算公式如下：$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y^i - \\hat{y^i})^2$$执行如下代码： 123from sklearn.metrics import mean_squared_errorprint(&#x27;MSE train: %.3f, test: %.3f&#x27; % (mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)))&gt;&gt; MSE train: 19.958, test: 27.196 从结果而知，训练集上的MSE值为19.96，测试集上的MSE值骤升为27.20，这意味着我们的模型过拟合于训练数据。 某些情况下也可以使用决定系数来进行评估，它的计算公式如下：$$R^2 = 1 - \\frac{MSE}{Var(y)}$$可以使用如下代码来计算$ R^2 $： 123from sklearn.metrics import r2_scoreprint(&#x27;R^2 train: %.3f, test: %.3f&#x27; % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))&gt;&gt; R^2 train: 0.765, test: 0.673 回归中的正则化方法正则化是通过在模型中加入额外信息来解决过拟合问题的一种方法，引入罚项增加了模型的复杂度但却降低了模型了模型参数的影响。最常见的正则化线性回归方法就是所谓的岭回归（Ridge Regression），最小绝对收缩及算子选择（LASSO）以及弹性网络（Elastic Net）。 岭回归是基于L2罚项的模型，我们只是在最小二乘代价函数中加入了权重的平方和：$$J(w){Ridge} = \\sum{i=1}^{n}(y^i-\\hat{y^i})^2+\\lambda||w||^2_2$$其中：$$L2： \\lambda||w||^2_2=\\lambda\\sum_{j=1}^{m}w_j^2$$对于稀疏数据训练的模型，还可以使用LASSO：$$J(w){LASSO}= = \\sum{i=1}^{n}(y^i-\\hat{y^i})^2+\\lambda||w||1$$其中：$$L1： \\lambda||w||1 = \\lambda\\sum{j=1}^{m}|w_j|$$弹性网络如下：$$J(w)_{ElasticNet} = \\sum{i=1}^{n}(y^i-\\hat{y^i})^2+\\lambda_1\\sum_{j=1}^{m}w_j^2+\\lambda_2\\sum_{j=1}^{m}|w_j|$$scikit-learn中岭回归模型的初始化方法如下： 12from sklearn.linear_model import Ridgeridge = Ridge(alpha=1.0) 正则化强度通过alpha参数来调节，类似于参数$ \\lambda $。 LASSO对象的初始化如下： 12from sklearn.linear_model import Lassolasso = Lasso(alpha=1.0) 最后，scikit-learn下面的ElasticNet允许我们调整L1与L2的比率： 12from sklearn.linear_model import ElasticNetlasso = ElasticNet(alpha=1.0, l1_ratio=0.5) 线性回归模型的曲线化—-多项式回归对于不符合线性假设的问题，一种常用的解释方法就是：$$y = w_0+w_1x+w_2x^2+\\cdots+w_dx^d$$接下来我们讨论一下如何使用scikit-learn中的PolynomialFeatures转化类在只含有一个解释变量的简单回归问题中加入二次项。步骤如下： 增加一个二次多项式： 12345678from sklearn.preprocessing import PolynomialFeaturesX = np.array([ 258.0, 270.0, 294.0, 320.0, 342.0, 368.0, 396.0, 446.0, 480.0, 586.0])[:, np.newaxis]y = np.array([ 236.4, 234.4, 252.8, 298.6, 314.2, 342.2, 360.8, 368.0, 391.2, 390.8])lr = LinearRegression()pr = LinearRegression()quadratic = PolynomialFeatures(degree=2)X_quad = quadratic.fit_transform(X) 拟合一个用于对比的简单线性回归模型： 123lr.fit(X, y)X_fit = np.arange(250, 600, 10)[:, np.newaxis]y_lin_fit = lr.predict(X_fit) 使用经过转换后的特征针对多项式回归拟合一个多元线性回归模型： 1234567pr.fit(X_quad, y)y_quad_fit = pr.predict(quadratic.fit_transform(X_fit))plt.scatter(X, y, label=&#x27;training points&#x27;)plt.plot(X_fit, y_lin_fit, label=&#x27;linear fit&#x27;, linestyle=&#x27;--&#x27;)plt.plot(X_fit, y_quad_fit, label=&#x27;quadratic fit&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show() 得到的图像如下： img 从图像可以看出，和线性拟合相比，多项式拟合可以更好地捕捉到解释变量和响应变量之间的关系。 12345678910y_lin_pred = lr.predict(X)y_quad_pred = pr.predict(X_quad)print(&#x27;Training MSE linear: %.3f, quadratic: %.3f&#x27; % ( mean_squared_error(y, y_lin_pred), mean_squared_error(y, y_quad_pred)))print(&#x27;Training R^2 linear: %.3f, quadratic: %.3f&#x27; % ( r2_score(y, y_lin_pred), r2_score(y, y_quad_pred)))&gt;&gt; Training MSE linear: 569.780, quadratic: 61.330&gt;&gt; Training R^2 linear: 0.832, quadratic: 0.982 执行上述代码后，MSE的值由线性拟合的570下降到了61。同时和线性拟合结果相比，二次模型的判定系数结果更好，说明二次拟合的效果更好。 房屋数据中的非线性关系建模接下来，我们将会使用二次核三次多项式对房屋价格核LSTAT之间的关系进行建模，并且和线性拟合进行对比： 1234567891011121314151617181920212223242526272829303132333435X = df[[&#x27;LSTAT&#x27;]].valuesy = df[&#x27;MEDV&#x27;].valuesregr = LinearRegression()# create polynomial featuresquadratic = PolynomialFeatures(degree=2)cubic = PolynomialFeatures(degree=3)X_quad = quadratic.fit_transform(X)X_cubic = cubic.fit_transform(X)# linear fitX_fit = np.arange(X.min(), X.max(), 1)[:, np.newaxis]regr = regr.fit(X, y)y_lin_fit = regr.predict(X_fit)linear_r2 = r2_score(y, regr.predict(X))# quadratic fitregr = regr.fit(X_quad, y)y_quad_fit = regr.predict(quadratic.fit_transform(X_fit))quadratic_r2 = r2_score(y, regr.predict(X_quad))# cubic fitregr = regr.fit(X_cubic, y)y_cubic_fit = regr.predict(cubic.fit_transform(X_fit))cubic_r2 = r2_score(y, regr.predict(X_cubic))# plot resultsplt.scatter(X, y, label=&#x27;training points&#x27;, color=&#x27;lightgray&#x27;)plt.plot(X_fit, y_lin_fit, label=&#x27;linear(d=1), R^2 = %.2f&#x27; % linear_r2, color=&#x27;b&#x27;, lw=2, linestyle=&#x27;:&#x27;)plt.plot(X_fit, y_quad_fit, label=&#x27;quadratic(d=2), R^2 = %.2f&#x27; % quadratic_r2, color=&#x27;g&#x27;, lw=2, linestyle=&#x27;-&#x27;)plt.plot(X_fit, y_cubic_fit, label=&#x27;cubic(d=3), R^2 = %.2f&#x27; % cubic_r2, color=&#x27;r&#x27;, lw=2, linestyle=&#x27;--&#x27;)plt.xlabel(&#x27;LSTAT&#x27;)plt.ylabel(&#x27;Price&#x27;)plt.legend()plt.show() 图像如下： img 从图像而知，相较于线性拟合和二次拟合，三次拟合更好地捕获了房屋价格与LSTAT之间的关系。不过，加入越来越多的多项式特征会增加模型复杂度，容易造成过拟合。 此外，多项式特征并非总是非线性关系建模的最佳选择。例如，我们仅就MEDV-LSTAT的散点图来说，我们可以将LSTAT特征变量的对数值以及MEDV的平方根映射到一个特征空间，并用线性回归进行拟合： 1234567891011121314151617# transform featuresX_log = np.log(X)y_sqrt = np.sqrt(y)# fit featuresX_fit = np.arange(X_log.min()-1, X_log.max()+1, 1)[:, np.newaxis]regr = regr.fit(X_log, y_sqrt)y_lin_fit = regr.predict(X_fit)linear_r2 = r2_score(y_sqrt, regr.predict(X_log))# plot resultsplt.scatter(X_log, y_sqrt, label=&#x27;training points&#x27;,color=&#x27;lightgray&#x27;)plt.plot(X_fit, y_lin_fit, label=&#x27;linear(d=1), R^2=%.2f&#x27; % linear_r2, color=&#x27;blue&#x27;, lw=2)plt.xlabel(&#x27;LSTAT&#x27;)plt.ylabel(&#x27;Price&#x27;)plt.legend()plt.show() 得到的图像如下： img 从$ R^2 $的值可以看出，这种拟合形式优于前面的任何一种多项式回归。 使用随机森林处理非线性关系本节中，我们将会学习随机森林回归，他从概念上异于本章中介绍的其他回归模型。随机森林是多颗决策树的集合，它可以被理解成分段线性函数的集成。 决策树回归 决策树算法的一个优点是我们无需对数据进行特征转换。在scikit-learn中对其进行建模： 12345678910from sklearn.tree import DecisionTreeRegressorX = df[[&#x27;LSTAT&#x27;]].valuesy = df[&#x27;MEDV&#x27;].valuestree = DecisionTreeRegressor(max_depth=3)tree.fit(X, y)sort_idx = X.flatten().argsort()lin_regplot(X[sort_idx], y[sort_idx], tree)plt.xlabel(&#x27;LSTAT&#x27;)plt.ylabel(&#x27;Price&#x27;)plt.show() 图像如下： img 在此例中，深度为3的树看起来是比较合适的。 随机森林回归 随机森林算法是组合多颗决策树的一种集成技术。随机森林的一个优势是：它对数据集中的异常值不敏感，且无需过多的参数调优。接下来使用scikit-learn中的库来拟合一个随机森林回归模型： 123456789101112X = df.iloc[:, :-1].valuesy = df[&#x27;MEDV&#x27;].valuesX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)from sklearn.ensemble import RandomForestRegressorforest = RandomForestRegressor(n_estimators=1000, criterion=&#x27;mse&#x27;, random_state=1, n_jobs=-1)forest.fit(X_train, y_train)y_train_pred = forest.predict(X_train)y_test_pred = forest.predict(X_test)print(&#x27;MSE train: %.3f, test: %.3f&#x27; % (mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)))print(&#x27;R^2 train: %.3f, test: %.3f&#x27; % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))&gt;&gt; MSE train: 1.641, test: 11.056&gt;&gt; R^2 train: 0.979, test: 0.878 遗憾的是，我们发现随机森林对于训练数据有些过拟合，接下来看一下预测的残差图： 12345678plt.scatter(y_train_pred, y_train_pred - y_train, c=&#x27;black&#x27;, marker=&#x27;o&#x27;, s=35, alpha=0.5, label=&#x27;Training data&#x27;)plt.scatter(y_test_pred, y_test_pred - y_test, c=&#x27;lightgreen&#x27;, marker=&#x27;s&#x27;, s=35, alpha=0.7, label=&#x27;Test data&#x27;)plt.xlabel(&#x27;Predicted values&#x27;)plt.ylabel(&#x27;Residuals&#x27;)plt.legend()plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color=&#x27;red&#x27;)plt.xlim([-10, 50])plt.show() 图像如下： img 可以发现，随机森林的残差图相比线性拟合产生的残差图有了很大的改进。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"在Web中嵌入机器学习模型","slug":"在Web中嵌入机器学习模型","date":"2020-02-08T01:39:29.000Z","updated":"2020-12-17T10:23:43.598Z","comments":true,"path":"2020/02/08/在Web中嵌入机器学习模型/","link":"","permalink":"http://blog.zsstrike.top/2020/02/08/%E5%9C%A8Web%E4%B8%AD%E5%B5%8C%E5%85%A5%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/","excerpt":"本章中，我们将学习如何将机器学习模型嵌入到Web应用中，不仅仅是分类，还包括从实时数据中学习。","text":"本章中，我们将学习如何将机器学习模型嵌入到Web应用中，不仅仅是分类，还包括从实时数据中学习。 序列化通过scikit-laern拟合的模型正如我们上一章所述，训练机器模型会带来很高的计算成本。当然，我们不希望每次进行预测分析都需要训练模型。模型持久化的一个方法是使用Python内嵌的pickle模块，它使得我们可以在Python对象与字节码之间进行转换（序列化和反序列化），这样我们就可以将分类器当前的状态保存下来。当需要对新的数据进行分类时，可以直接加载已经保存的分类器，而不必再次用训练数据对模型进行训练： 123456789101112import pickleimport osdest = os.path.join(os.getcwd(), &#x27;movieclassifier&#x27;, &#x27;pkl_objects&#x27;)if not os.path.exists(dest): os.makedirs(dest)pickle.dump(stop, open(os.path.join(dest, &#x27;stopwords.pkl&#x27;), &#x27;wb&#x27;), protocol=4)pickle.dump(clf, open(os.path.join(dest, &#x27;classifier.pkl&#x27;), &#x27;wb&#x27;), protocol=4) 由于无需拟合HashingVectorizer，也就不必对其进行持久化操作。相反，我们创建一个新的脚本文件，通过此脚本可以将向量数据导入到当前Python会话中，下面代码以vectorizer.py作为文件名，保存在movieclassifier目录下： 1234567891011121314151617from sklearn.feature_extraction.text import HashingVectorizerimport reimport osimport picklecur_dir = os.path.dirname(__file__)stop = pickle.load(open(os.path.join(cur_dir, &#x27;pkl_objects&#x27;, &#x27;stopwords.pkl&#x27;), &#x27;rb&#x27;))def tokenizer(text): text = re.sub(&#x27;&lt;[^&gt;]*&gt;&#x27;, &#x27;&#x27;, text) text = re.sub(&#x27;[\\W]+&#x27;, &#x27; &#x27;, text.lower()) tokenized = [w for w in text.split() if w not in stop] return tokenizedvect = HashingVectorizer(decode_error=&#x27;ignore&#x27;, n_features=2**21, preprocessor=None, tokenizer=tokenizer) 接下来定位到movieclassifer目录，就可以导入vectorizer及对分类器进行持久化处理： 12345import pickleimport reimport osfrom vectorizer import vectclf = pickle.load(open(os.path.join(&#x27;pkl_objects&#x27;, &#x27;classifier.pkl&#x27;), &#x27;rb&#x27;)) 在成功加载vectorizer以及反序列化分类器后，我们现在使用这些对象对文档样本进行预处理，并且对其进行预测： 1234567import numpy as nplabel = &#123;0: &#x27;negative&#x27;, 1: &#x27;postive&#x27;&#125;example = [&#x27;I love this movie&#x27;]X = vect.transform(example)print(&#x27;Prediction: %s\\nProbability: %.3f%%&#x27; % (label[clf.predict(X)[0]], np.max(clf.predict_proba(X))*100))&gt;&gt; Prediction: postive&gt;&gt; Probability: 81.483% 使用SQLite数据库存储数据本节中，我们将创建一个简单的SQLite数据库以收集Web应用的用户对于预测结果的反馈。SQLite是一个进程内的库，实现了自给自足的、无服务器的、零配置的、事务性的 SQL 数据库引擎。它是一个零配置的数据库，这意味着与其他数据库一样，我们不需要在系统中配置。就像其他数据库，SQLite 引擎不是一个独立的进程，可以按应用程序需求进行静态或动态连接。SQLite 可以直接访问其存储文件。 通过如下代码，我们将在movieclassifier所在目录创建一个新的SQLite数据库，并且向其中插入两条电影评论的示例数据： 12345678910111213141516import sqlite3import osconn = sqlite3.connect(&#x27;reviews.sqlite&#x27;)c = conn.cursor()c.execute(&quot;CREATE TABLE review_db&quot;\\ &quot;(review TEXT, sentiment INTEGER, date TEXT)&quot;)example1 = &#x27;I love this movie&#x27;c.execute(&quot;INSERT INTO review_db&quot;\\ &quot;(review, sentiment, date) VALUES&quot;\\ &quot;(?, ?, DATETIME(&#x27;now&#x27;))&quot;, (example1, 1))example2 = &#x27;I dislike this movie&#x27;c.execute(&quot;INSERT INTO review_db&quot;\\ &quot;(review, sentiment, date) VALUES&quot;\\ &quot;(?, ?, DATETIME(&#x27;now&#x27;))&quot;, (example2, 0))conn.commit()conn.close() 使用Flask开发Web应用上一节中完成了用于电影评论分类的代码，现在来讨论使用Flask框架开发Web应用的基础知识。 第一个Flask Web应用首先，按照如下目录结构创建Web应用的框架： 12341st_flask_app_1&#x2F; -app.py -templates&#x2F; -first_app.html app.py文件中包含了运行Flask Web应用程序而需要在Python解释器中执行的入口代码。templates目录下面是Flask用到的静态HTML文件。首先，看一下app.py的内容： 12345678910from flask import Flask, render_templateapp = Flask(__name__)@app.route(&#x27;/&#x27;)def index(): return render_template(&#x27;first_app.html&#x27;)if __name__ == &#x27;__main__&#x27;: app.run() 其中需要注意的是路由注解（@app.route(‘/‘)）指定触发index函数的URL路径。接下里通过终端窗口执行下列命令启动Web应用： 12python3 app.py&gt;&gt; Running on http://127.0.0.1:5000/ 接下来打开对应的网站，如果一切正常，将会看到如下内容网页： “Hi, this is my first Flask Web app!”。 表单验证本节中，我们使用HTML表单升级Flask Web应用，以及学习如何使用WTForms库收集数据。 新的应用程序所需的目标结构看起来如下： 123456781st_flask_app_1&#x2F; -app.py -static -style.css -templates&#x2F; -_formhelpers.html -first_app.html -hello.html 以下为修改后的app.py文件内容： 1234567891011121314151617181920212223from flask import Flask, render_template, requestfrom wtforms import Form, TextAreaFiled, validatorsapp = Flask(__name__)class HelloForm(Form): sayhello = TextAreaFiled(&#x27;&#x27;, [validators.DateRequired()])@app.route(&#x27;/&#x27;)def index(): form = HelloForm(request.form) return render_template(&#x27;first_app.html&#x27;, form=form)@app.route(&#x27;/hello&#x27;, method=[&#x27;POST&#x27;])def hello(): form = HelloForm(request.form) if request.method == &#x27;POST&#x27; and form.validate(): name = request.form[&#x27;sayhello&#x27;] return render_template(&#x27;hello.html&#x27;, name=name) return render_template(&#x27;first_app.html&#x27;, form=form)if __name__ == &#x27;__main__&#x27;: app.run(debug=True) 现在通过Jinjia2模板引擎，在_formhelper.html文件中实现一个通用宏，后续它会被导入到first_app.html文件中用来渲染文本： 123456789101112&#123;% macro render_field(field) %&#125; &lt;dt&gt;&#123;&#123; field.label &#125;&#125; &lt;dd&gt;&#123;&#123; field(**kwargs)|safe &#125;&#125; &#123;% if field.errors %&#125; &lt;ul class&#x3D;errors&gt; &#123;% for error in field.errors %&#125; &lt;li&gt;&#123;&#123; error &#125;&#125;&lt;&#x2F;li&gt; &#123;% endfor %&#125; &lt;&#x2F;ul&gt; &#123;% endif %&#125; &lt;&#x2F;dd&gt;&#123;% endmacro %&#125; 接下来，我们创建一个style.css文件，用于控制样式： 123body &#123; font-size: 2em;&#125; 下面是修改后的first_app.html文件内容： 1234567891011121314151617&lt;!doctype html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;First app&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;&#123; url_for(&#x27;static&#x27;, filename=&#x27;style.css&#x27;) &#125;&#125;&quot;&gt; &lt;/head&gt; &lt;body&gt; &#123;% from &quot;_formhelpers.html&quot; import render_field %&#125; &lt;div&gt;What&#x27;s your name?&lt;/div&gt; &lt;form method=post action=&quot;/hello&quot;&gt; &lt;dl&gt; &#123;&#123; render_field(form.sayhello) &#125;&#125; &lt;/dl&gt; &lt;input type=submit value=&#x27;Say Hello&#x27; name=&#x27;submit_btn&#x27;&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 最后我们创建一个hello.html的文件： 12345678910&lt;!doctype html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;First app&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;&#123; url_for(&#x27;static&#x27;, filename=&#x27;style.css&#x27;) &#125;&#125;&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt;Hello &#123;&#123; name &#125;&#125;&lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 接下来通过如下代码来运行我们的Web应用： 1python3 app.py 将电影分类器嵌入Web应用下面更进一步，将电影分类器嵌入到Web应用中。 首先，看一下此电影评论分类应用的目录结构，如下图： 1581131990058 在本章前面的小节中，我们已经创建了vectorizer.py文件，reviews.sqlite以及pkl_objects对象。 由于app.py文件较长，我们分两步来分析。首先导入所需的Python模块和对象，并且通过反序列化恢复我们的分类模型： 12345678910111213141516171819202122232425262728293031323334from flask import Flask, render_template, requestfrom wtforms import Form, TextAreaField, validatorsimport pickleimport sqlite3import osimport numpy as np# import HashingVectorizer from local dirfrom vectorizer import vectapp = Flask(__name__)######## Preparing the Classifiercur_dir = os.path.dirname(__file__)clf = pickle.load(open(os.path.join(cur_dir, &#x27;pkl_objects/classifier.pkl&#x27;), &#x27;rb&#x27;))db = os.path.join(cur_dir, &#x27;reviews.sqlite&#x27;)def classify(document): label = &#123;0: &#x27;negative&#x27;, 1: &#x27;positive&#x27;&#125; X = vect.transform([document]) y = clf.predict(X)[0] proba = np.max(clf.predict_proba(X)) return label[y], probadef train(document, y): X = vect.transform([document]) clf.partial_fit(X, [y]) def sqlite_entry(path, document, y): conn = sqlite3.connect(path) c = conn.cursor() c.execute(&quot;INSERT INTO review_db (review, sentiment, date)&quot;\\ &quot; VALUES (?, ?, DATETIME(&#x27;now&#x27;))&quot;, (document, y)) conn.commit() conn.close() app.py的第二部分如下： 12345678910111213141516171819202122232425262728293031323334353637app = Flask(__name__)class ReviewForm(Form): moviereview = TextAreaField(&#x27;&#x27;, [validators.DataRequired(), validators.length(min=15)])@app.route(&#x27;/&#x27;)def index(): form = ReviewForm(request.form) return render_template(&#x27;reviewform.html&#x27;, form=form)@app.route(&#x27;/results&#x27;, methods=[&#x27;POST&#x27;])def results(): form = ReviewForm(request.form) if request.method == &#x27;POST&#x27; and form.validate(): review = request.form[&#x27;moviereview&#x27;] y, proba = classify(review) return render_template(&#x27;results.html&#x27;, content=review, prediction=y, probability=round(proba*100, 2)) return render_template(&#x27;reviewform.html&#x27;, form=form)@app.route(&#x27;/thanks&#x27;, methods=[&#x27;POST&#x27;])def feedback(): feedback = request.form[&#x27;feedback_button&#x27;] review = request.form[&#x27;review&#x27;] prediction = request.form[&#x27;prediction&#x27;] inv_label = &#123;&#x27;negative&#x27;: 0, &#x27;positive&#x27;: 1&#125; y = inv_label[prediction] if feedback == &#x27;Incorrect&#x27;: y = int(not(y)) train(review, y) sqlite_entry(db, review, y) return render_template(&#x27;thanks.html&#x27;)if __name__ == &#x27;__main__&#x27;: app.run(debug=True) 接下来，看一下reviewform.html模板： 123456789101112131415161718&lt;!doctype html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;Movie Classification&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;Please enter your movie review:&lt;/h2&gt; &#123;% from &quot;_formhelpers.html&quot; import render_field %&#125; &lt;form method=post action=&quot;/results&quot;&gt; &lt;dl&gt; &#123;&#123; render_field(form.moviereview, cols=&#x27;30&#x27;, rows=&#x27;10&#x27;) &#125;&#125; &lt;/dl&gt; &lt;div&gt; &lt;input type=submit value=&#x27;Submit review&#x27; name=&#x27;submit_btn&#x27;&gt; &lt;/div&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 下一个模板是result.html，看上去很有趣： 123456789101112131415161718192021222324252627&lt;!doctype html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;Movie Classification&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;&#123; url_for(&#x27;static&#x27;, filename=&#x27;style.css&#x27;) &#125;&#125;&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;h3&gt;Your movie review:&lt;/h3&gt; &lt;div&gt;&#123;&#123; content &#125;&#125;&lt;/div&gt; &lt;h3&gt;Prediction:&lt;/h3&gt; &lt;div&gt;This movie review is &lt;strong&gt;&#123;&#123; prediction &#125;&#125;&lt;/strong&gt; (probability: &#123;&#123; probability &#125;&#125;%).&lt;/div&gt; &lt;div id=&#x27;button&#x27;&gt; &lt;form action=&quot;/thanks&quot; method=&quot;post&quot;&gt; &lt;input type=submit value=&#x27;Correct&#x27; name=&#x27;feedback_button&#x27;&gt; &lt;input type=submit value=&#x27;Incorrect&#x27; name=&#x27;feedback_button&#x27;&gt; &lt;input type=hidden value=&#x27;&#123;&#123; prediction &#125;&#125;&#x27; name=&#x27;prediction&#x27;&gt; &lt;input type=hidden value=&#x27;&#123;&#123; content &#125;&#125;&#x27; name=&#x27;review&#x27;&gt; &lt;/form&gt; &lt;/div&gt; &lt;div id=&#x27;button&#x27;&gt; &lt;form action=&quot;/&quot;&gt; &lt;input type=submit value=&#x27;Submit another review&#x27;&gt; &lt;/form&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 此外，style.css文件如下： 123456body&#123; width:600px;&#125;#button&#123; padding-top: 20px;&#125; 同样，thanks.html的内容如下： 1234567891011121314&lt;!doctype html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;Movie Classification&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h3&gt;Thank you for your feedback!&lt;/h3&gt; &lt;div id=&#x27;button&#x27;&gt; &lt;form action=&quot;/&quot;&gt; &lt;input type=submit value=&#x27;Submit another review&#x27;&gt; &lt;/form&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 同样，最后我们启动Web应用： 1python3 app.py 接下来，我们就可以访问网站了。 在公共服务器上部署Web应用测试完Web应用后，我们可以将其托管到PythonAnywhere服务器上。托管到PythonAnywhere网站后，我们可以通过访问&lt;username&gt;.pythonanywhere.com。 当收到用户的反馈后，模型会自动即时更新，但是如果服务器崩溃或者重启，clfd对象的更新就会重置。使得更新能够持久化保存的一个方法就是：模型一旦被更新就立即序列化新的clf对象。但是随着用户的增多，此方案的效率会逐渐底下。另外一种解决方案就是使用SQLite数据库保存的反馈信息更新预测模型。为了更新clf对象，我们创建一个update.py脚本文件： 12345678910111213141516171819202122232425262728293031323334353637383940import pickleimport sqlite3import numpy as npimport os# import HashingVectorizer from local dirfrom vectorizer import vectdef update_model(db_path, model, batch_size=10000): conn = sqlite3.connect(db_path) c = conn.cursor() c.execute(&#x27;SELECT * from review_db&#x27;) results = c.fetchmany(batch_size) while results: data = np.array(results) X = data[:, 0] y = data[:, 1].astype(int) classes = np.array([0, 1]) X_train = vect.transform(X) clf.partial_fit(X_train, y, classes=classes) results = c.fetchmany(batch_size) conn.close() return Nonecur_dir = os.path.dirname(__file__)clf = pickle.load(open(os.path.join(cur_dir, &#x27;pkl_objects&#x27;, &#x27;classifier.pkl&#x27;), &#x27;rb&#x27;))db = os.path.join(cur_dir, &#x27;reviews.sqlite&#x27;)update_model(db_path=db, model=clf, batch_size=10000)# Uncomment the following lines if you are sure that# you want to update your classifier.pkl file# permanently.# pickle.dump(clf, open(os.path.join(cur_dir,# &#x27;pkl_objects&#x27;, &#x27;classifier.pkl&#x27;), &#x27;wb&#x27;)# , protocol=4) 创建好update.py的脚本中，我们需要在app.py开头增加一行导入update.py脚本中update_model函数的代码： 12# import update function from local_dirfrom update import update_model 然后在应用程序的主脚本中调用update_model函数： 1234...if __name__ == &#x27;__main__&#x27;: update_model(filepath=db, model=clf, batch_size=10000)...","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"使用机器学习进行情感分析","slug":"使用机器学习进行情感分析","date":"2020-02-07T07:01:11.000Z","updated":"2020-12-17T10:23:43.597Z","comments":true,"path":"2020/02/07/使用机器学习进行情感分析/","link":"","permalink":"http://blog.zsstrike.top/2020/02/07/%E4%BD%BF%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%BF%9B%E8%A1%8C%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/","excerpt":"本章我们将深入研究自然语言处理（natural language processing，NLP）领域的一个分支—-情感分析（sentiment analysis），还将学习如何使用机器学习算法基于文档的情感倾向对文档进行分类。","text":"本章我们将深入研究自然语言处理（natural language processing，NLP）领域的一个分支—-情感分析（sentiment analysis），还将学习如何使用机器学习算法基于文档的情感倾向对文档进行分类。 获取IMDB电影评论数据集情感分析，又是也称作是观点挖掘，是NLP领域一个非常流行的分支，它分析的是文档的情感倾向。本章中，我们将要使用的是互联网电影数据库中的大量电影评论数据。可以访问http://ai.stanford.edu/~amaas/data/sentiment/来下载电影评论。 在下载完成后对文档进行解压，接下来我们着手将从压缩文件中得到的各文本文档组合为一个CSV文件，在下面的代码中，我们把电影的评论读取到pandas的DataFrame对象中。同时使用PyPrid（Python Progress Indicator）包来预测剩余处理时间： 12345678910111213141516import pyprindimport pandas as pdimport ospbar = pyprind.ProgBar(50000)labels = &#123;&#x27;pos&#x27;: 1, &#x27;neg&#x27;: 0&#125;df = pd.DataFrame()for s in (&#x27;test&#x27;, &#x27;train&#x27;): for l in (&#x27;pos&#x27;, &#x27;neg&#x27;): path = &#x27;./aclImdb/%s/%s&#x27; % (s, l) for file in os.listdir(path): with open(os.path.join(path, file), &#x27;r&#x27;) as infile: txt = infile.read() df = df.append([[txt, labels[l]]], ignore_index=True) pbar.update()df.columns = [&#x27;review&#x27;, &#x27;sentiment&#x27;] 由于集成处理过后数据集中的对应类标是经过排序的，我们现在使用np.random子模块下的permutation函数对DataFrame对象进行重排，并且将其存储为CSV文件： 12345import numpy as npnp.random.seed(0)df = df.reindex(np.random.permutation(df.index))df.to_csv(&#x27;./movie_data.csv&#x27;, index=False) 现在读取前三个样本的摘要： 12df = pd.read_csv(&#x27;./movie_data.csv&#x27;)df.head(3) 1581062367489 词袋模型简介本节中，我们介绍词袋模型，它将文本以数值特征向量的形式来表示。词袋模型的理念很简单，可描述如下： 我们在整个文档上为每个词汇创建了唯一的标记，如单词 我们为每个文档构建一个特征向量，其中包含每个单词在此文档中出现的次数 下面讲解创建简单词袋模型的过程。 将单词转换为特征向量我们可以使用scikit-learn中的CountVector类来根据每个文档中的单词数量构建词袋模型： 123456789import numpy as npfrom sklearn.feature_extraction.text import CountVectorizercount = CountVectorizer()docs = np.array([&#x27;The sun is shining&#x27;, &#x27;The weather is sweet&#x27;, &#x27;The sun is shining and the weather is sweet&#x27;])bag = count.fit_transform(docs)print(count.vocabulary_)&gt;&gt; &#123;&#x27;the&#x27;: 5, &#x27;sun&#x27;: 3, &#x27;is&#x27;: 1, &#x27;shining&#x27;: 2, &#x27;weather&#x27;: 6, &#x27;sweet&#x27;: 4, &#x27;and&#x27;: 0&#125; 由上述命令的运行结果可见，词汇以Python字典的格式存储，将单个单词映射为一个整数索引。接下来看一下之前创建的特征向量： 1234print(bag.toarray())&gt;&gt; [[0 1 1 1 0 1 0]&gt;&gt; [0 1 0 0 1 1 1]&gt;&gt; [1 2 1 1 1 2 1]] 出现在特征向量中的值也称作是原始词频：$ tf(t, d):=词汇t在文档d中出现的次数 $。 通过词频–逆文档频率计算单词关联度当我们分析文档数据时，经常遇到的问题就是：一个单词出现在两种类型的多个文档中，这种频繁出现的单词通常不包含有用或具备辨识度的信息。本节中，我们将会学习词频–逆文档频率：$$tf-idf(t, d) = tf(t, d) \\times idf(t, d)$$其中，逆文档频率计算公式如下：$$idf(t, d) = log\\frac{n_d}{1+df(d, t)}$$这里的$ n_d $问文档的总数，$ df(d, f) $为词汇t在文档d中的数量。分母中的1是为了防止分母为0；取对数是为了出现频率较低的词汇不会被赋予过大的权重。 scikit-learn中还实现了TfidfTransformer转换器： 12345678from sklearn.feature_extraction.text import TfidfTransformer​tfidf = TfidfTransformer()np.set_printoptions(precision=2)print(tfidf.fit_transform(count.fit_transform(docs)).toarray())&gt;&gt; [[0. 0.43 0.56 0.56 0. 0.43 0. ]&gt;&gt; [0. 0.43 0. 0. 0.56 0.43 0.56]&gt;&gt; [0.4 0.48 0.31 0.31 0.31 0.48 0.31]] 可以发现，is在第三个文档中具有较高的词频，但是在将特征向量转换为$ tf-idf $后，单词is在第三个文档中只得到了一个相对较小的$ tf-idf $。 scikit-learn中计算$ tf-idf $之前都会对原始词频进行归一化处理。 清洗文本数据在构建词袋模型之前，最重要的一步就是去除所有不需要的字符对文本数据进行清洗。我们先展示一下经过重排后数据集中第一个文档的最后50个字符： 12df.loc[0, &#x27;review&#x27;][-50:]&gt;&gt; &#x27;is seven.&lt;br /&gt;&lt;br /&gt;Title (Brazil): Not Available&#x27; 接下来，我们将会去除标点符号和HTML标签： 12345import redef preprocessor(text): text = re.sub(&#x27;&lt;[^&gt;]*&gt;&#x27;, &#x27;&#x27;, text) text = re.sub(&#x27;[\\W]+&#x27;, &#x27; &#x27;, text.lower()) return text 接下来我们看一下该函数是否能正常工作： 12preprocessor(df.loc[0, &#x27;review&#x27;][-50:])&gt;&gt; &#x27;is seven title brazil not available&#x27; 最后，我们在下一节中将会反复使用在此经过清洗的文本数据，现在通过preprocessor函数清洗所有的电影评论： 1df[&#x27;review&#x27;] = df[&#x27;review&#x27;].apply(preprocessor) 标记文档准备好电影评论数据集后，我们需要将文本语料拆分为单独的元素。标记（tokenize）文档的一个常用方法是通过文档的空白字符将其拆分为单独的单词： 1234def tokenizer(text): return text.split()tokenizer(&#x27;runner likes running and thus he run&#x27;)&gt;&gt; [&#x27;runner&#x27;, &#x27;likes&#x27;, &#x27;running&#x27;, &#x27;and&#x27;, &#x27;thus&#x27;, &#x27;he&#x27;, &#x27;run&#x27;] 在对文本标记的过程中，另外一种有用的技术就是词干提取（word stemming），这是一个提取单词原型的过程，这样，我们就能将一个单词映射到对应的词干上。Python的自然语言工具包（NLPK）实现了Porter Stemming算法： 123456from nltk.stem import PorterStemmerporter = PorterStemmer()def tokenizer_porter(text): return [porter.stem(word) for word in text.split()]tokenizer_porter(&#x27;runner likes running and thus he run&#x27;)&gt;&gt; [&#x27;runner&#x27;, &#x27;like&#x27;, &#x27;run&#x27;, &#x27;and&#x27;, &#x27;thu&#x27;, &#x27;he&#x27;, &#x27;run&#x27;] 可以发现，running被修改为run，但是thus被修改为不存在的单词thu。在实际应用中中，这种结果造成的影响不大。 另外，还有一种有用的技术：停用词移除（stop-word removal）。停用词在英文中太常见了，它们包含很少的有用信息，因此可以将他们删除： 123456import nltkfrom nltk.corpus import stopwordsnltk.download(&#x27;stopwords&#x27;)stop = stopwords.words(&#x27;english&#x27;)[w for w in tokenizer_porter(&#x27;a runner likes running and runs a lot&#x27;)[-10:] if w not in stop]&gt;&gt; [&#x27;runner&#x27;, &#x27;like&#x27;, &#x27;run&#x27;, &#x27;and&#x27;, &#x27;thu&#x27;, &#x27;he&#x27;, &#x27;run&#x27;] 训练用于文档分类的逻辑斯蒂回归模型本节中，我们将会使用逻辑斯蒂回归模型将电影评论分为正面评价和负面评价。首先，我们将文本对象划分为测试数据和训练数据： 1234X_train = df.loc[:25000, &#x27;review&#x27;].valuesy_train = df.loc[:25000, &#x27;sentiment&#x27;].valuesX_test = df.loc[25000:, &#x27;review&#x27;].valuesy_test = df.loc[25000:, &#x27;sentiment&#x27;].values 接着我们使用Grid Search CV对象，并且使用5折分层交叉验证找到最佳参数： 123456789101112131415161718192021from sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import Pipelinefrom sklearn.linear_model import LogisticRegressionfrom sklearn.feature_extraction.text import TfidfVectorizertfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)param_grid = [&#123;&#x27;vect__ngram_range&#x27;: [(1, 1)], &#x27;vect__stop_words&#x27;: [stop, None], &#x27;vect__tokenizer&#x27;: [tokenizer, tokenizer_porter], &#x27;clf__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;], &#x27;clf__C&#x27;: [1.0, 10.0, 100.0]&#125;, &#123;&#x27;vect__ngram_range&#x27;: [(1, 1)], &#x27;vect__stop_words&#x27;: [stop, None], &#x27;vect__tokenizer&#x27;: [tokenizer, tokenizer_porter], &#x27;vect__use_idf&#x27;: [False], &#x27;vect__norm&#x27;: [None], &#x27;clf__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;], &#x27;clf__C&#x27;: [1.0, 10.0, 100.0]&#125;]lr_tfidf = Pipeline([(&#x27;vect&#x27;, tfidf), (&#x27;clf&#x27;, LogisticRegression(random_state=0))])gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring=&#x27;accuracy&#x27;, cv=5, verbose=1, n_jobs=-1)gs_lr_tfidf.fit(X_train, y_train) 在网格搜索结束后，我们可以输出最佳的参数集： 12print(&#x27;Best params: %s&#x27; % gs_lr_tfidf.best_params_)&gt;&gt; Best params: &#123;&#x27;clf__C&#x27;: 10.0, &#x27;clf__penalty&#x27;: &#x27;l2&#x27;, &#x27;vect__ngram_range&#x27;: (1, 1), &#x27;vect__stop_words&#x27;: None, &#x27;vect__tokenizer&#x27;: &lt;function tokenizer at 0x000002517B0C5F78&gt;&#125; 使用网格搜索得到的最佳模型，我们分别输出5折交叉验证的准确率得分，以及在测试数据集上的分类准确率： 12345print(&#x27;CV acc: %s&#x27; % gs_lr_tfidf.best_score_)&gt;&gt; CV acc: 0.8974041038358466clf = gs_lr_tfidf.best_estimator_print(&#x27;Test acc: %s&#x27; % clf.score(X_test, y_test))&gt;&gt; Test acc: 0.89844 结果表明，我们的机器学习模型针对电影评论是正面评论还是负面评论的分类准确率为90%。 使用大数据之在线算法与外存学习在上一节中，使用网格搜索最佳参数的算法计算成本很高。回顾一下第2章中的随机梯度下降（stochastic gradient descent， SGD）概念，此优化算法每次使用一个样本来更新模型的权重信息。在本节中，我们将使用scikit-learn中SGDClassifier的partial_fit函数来读取本地存储设备，并且使用小型子批次（minibatches）文档来训练一个逻辑斯蒂回归模型。 首先，我们定义一个tokenizer函数来清理movie_data.csv文件中未经处理的文本数据： 123456789import numpy as npimport refrom nltk.corpus import stopwordsstop = stopwords.words(&#x27;english&#x27;)def tokenizer(text): text = re.sub(&#x27;&lt;[^&gt;]*&gt;&#x27;, &#x27;&#x27;, text) text = re.sub(&#x27;[\\W]+&#x27;, &#x27; &#x27;, text.lower()) tokenized = [w for w in text.split() if w not in stop] return tokenized 接下来我们定义一个生成器函数：stream_docs，它每次读取且返回一个文档的内容： 123456def stream_docs(path): with open(path, &#x27;r&#x27;) as scv: next(csv) for line in csv: text, label = line[:-3], int(line[-2]) yield text, label 定义一个get_minibatch函数，它以stream_doc函数得到的文档数据流作为输入，并且通过size返回指定数量的文档内容： 12345678910def get_minibatch(doc_stream, size): docs, y = [], [] try: for _ in range(size): text, label = next(doc_stream) docs.append(text) y.append(label) except StopIteration: return None, None return docs, y 不幸的是，由于需要将所有的词汇加载到内存中，我们无法通过CountVectorizer来使用外存学习方法。另外，TfidfVectorizer需要将所有训练数据集中的特征向量加载到内存以计算逆文档频率。不过，scikit-learn提供了另外一个处理文本信息的向量处理器：HashingVectorizer。HashingVectorizer是独立数据的： 12345678from sklearn.feature_extraction.text import HashingVectorizerfrom sklearn.linear_model import SGDClassifiervect = HashingVectorizer(decode_error=&#x27;ignore&#x27;, n_features=2**21, preprocessor=None, tokenizer=tokenizer)clf = SGDClassifier(loss=&#x27;log&#x27;, random_state=1, n_iter=1)doc_stream = stream_docs(path=&#x27;./movie_data.csv&#x27;) 接下来我们可以通过下述代码使用外存学习： 12345678910import pyprindpbar = pyprind.ProgBar(45)classes = np.array([0, 1])for _ in range(45): X_train, y_train = get_minibatch(doc_stream, size=1000) if not X_train: break X_train = vect.transform(X_train) clf.partial_fit(X_train, y_train, classes=classes) pbar.update() 完成增量学习后，我们将使用剩余的5000个文档来评估模型的性能： 1234X_test, y_test = get_minibatch(doc_stream, size=5000)X_test = vect.transform(X_test)print(&#x27;Acc: %.3f&#x27; % clf.score(X_test, y_test))&gt;&gt; Acc: 0.868 可以看到，模型的准确率约为87%，略微低于我们上一节我们使用网格搜索进行超参调优得到的模型。不过外存学习的存储效率高，只用了不到一分钟的实践就完成了。最后，我们可以通过剩下的5000个文档进行升级： 1clf = clf.partial_fit(X_test, y_test)","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"集成学习之组合不同的模型","slug":"集成学习之组合不同的模型","date":"2020-02-06T07:18:53.000Z","updated":"2020-12-17T10:23:43.747Z","comments":true,"path":"2020/02/06/集成学习之组合不同的模型/","link":"","permalink":"http://blog.zsstrike.top/2020/02/06/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BB%84%E5%90%88%E4%B8%8D%E5%90%8C%E7%9A%84%E6%A8%A1%E5%9E%8B/","excerpt":"本章中，我们将会学习如何构建一组分类器的集合，使得整体分类效果优于其中任意一个单独的分类器。","text":"本章中，我们将会学习如何构建一组分类器的集合，使得整体分类效果优于其中任意一个单独的分类器。 集成学习集成方法（ensemble method）的目标是：将不同的分类器组合成一个元分类器，与包含于其中的单个分类器相比，元分类器具有更好的泛化性能。 本章中介绍的几种流行的集成方法，它们都使用了多数投票（majority voting）。多数投票原则是将大多数分类器预测的结果作为最终的预测指标。基于训练集，我们首先训练m个不同的成员分类器（$ C_1, \\cdots, C_m $），接着我们将新的未知数据$ x $输入，然后对所有分类器$ C_j $的预测类标进行汇总，选择出得票率最高的类标$ \\hat{y} $：$$\\hat{y} = mode{C_1(x), \\cdots, C_m(x)}$$ mode函数：众数函数，返回出现次数最多的值。 另外，由统计学知识得到，当成员分类器出错率低于$ 50% $时，集成分类器的出错率要低于单个分类器。 实现一个简单的多数投票分类器集成算法允许我们使用单独的权重对不同分类算法进行组合，可以将加权多数投票记为：$$\\hat{y} = argmax_i\\sum_{j=1}^{m}w_j\\chi_A(C_j(x)=i)$$其中，$ w_j $是$ C_j $分类器的权重。 argmax是一种函数，是对函数求参数(集合)的函数。当我们有另一个函数$ y=f(x) $时，若有结果$ x_0= argmax(f(x)) $，则表示当函数$ argmax(f(x)) $取$ x=x_0 $的时候，得到f(x)取值范围的最大值；若有多个点使得f(x)取得相同的最大值，那么$ argmax(f(x)) $的结果就是一个点集。 为了使用Python代码实现加权多数投票，我们可以使用NumPy中的argmax和bincount函数： 123import numpy as npnp.argmax(np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6]))&gt;&gt; 1 在实际应用中，我们可以将原来的类标转换为预测类表的概率，这样修正公式如下：$$\\hat{y} = argmax_i\\sum_{j=1}^mw_jp_{ij}$$其中，$ p_{ij} $是第j个分类器预测样本类标为i的概率。 为实现基于类别预测概率的加权多数投票，我们可以使用如下代码： 123456ex = np.array([[0.9, 0.1], [0.8, 0.2], [0.4, 0.6]])p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6])print(p, np.argmax(p))&gt;&gt; [0.58 0.42] 0 综上，我们可以实现MajorityVoteClassifier: 12345678910111213141516171819202122232425from sklearn.base import BaseEstimatorfrom sklearn.base import ClassifierMixinfrom sklearn.preprocessing import LabelEncoderfrom sklearn.externals import sixfrom sklearn.base import clonefrom sklearn.pipeline import _name_estimatorsimport numpy as npimport operatorclass MajorityVoteClassifier(BaseEstimator, ClassifierMixin): def __init__(self, classifiers, vote=&#x27;classlabel&#x27;, weights=None): self.classifiers = classifiers self.named_classifiers = &#123;key: value for key, value in _name_estimators(classifiers)&#125; self.vote = vote self.weights = weights def fit(self, X, y): self.labelenc_ = LabelEncoder() self.labelenc_.fit(y) self.classes_ = self.labelenc_.classes_ self.classifiers_ = [] for clf in self.classifiers: fitted_clf = clone(clf).fit(X, self.labelenc_.transform(y)) self.classifiers_.append(fitted_clf) return self 在这里，我们使用了两个基类BaseEstimator和ClassifierMixIn获取某些基本方法，包括设定分类器参数的set_params和返回参数的get_params方法，以及用于计算预测准确度的score方法。此外，导入six包是为了使得MajorityVoteClassifier与Python 2.7兼容。 接下来，我们加入predict方法： 1234567891011121314151617181920212223242526def predict(self, X): if self.vote == &#x27;probability&#x27;: maj_vote = np.argmax(self.predict_proba(X), axis=1) else: predictions = np.asarray([clf.predict(X) for clf in self.classifiers_]).T maj_vote = np.apply_along_axis(lambda x: np.argmax(np.bincount(x, weights=self.weights)), axis=1, arr=predictions) maj_vote = self.labelenc_.inverse_transform(maj_vote) return maj_votedef predict_proba(self, X): probas = np.asarray([clf.predict_proba(X) for clf in self.classifiers_]) avg_proba = np.average(probas, axis=0, weights=self.weights) return avg_probadef get_params(self, deep=True): if not deep: return super(MajorityVoteClassifier, self).get_params(deep=False) else: out = self.named_classifiers.copy() for name, step in six.iteritems(self.named_classifiers): for key, value in six.iteritems(step.get_params(deep=True)): out[&#x27;%s__%s&#x27; % (name, key)] = value return out 接下来我们可以将上述算法用于实战了。我们导入鸢尾花数据集，并且只是用其中的两个特征：萼片宽度和花瓣长度。同时我们只区分两个类别的样本：Iris-Versicolor和Iris-Virginica，并且绘制ROC AUC曲线： 123456789from sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.preprocessing import LabelEncoderiris = datasets.load_iris()X, y = iris.data[50:, [1, 2]], iris.target[50:]le = LabelEncoder()y = le.fit_transform(y) 下面划分数据集为测试数据集和训练数据集： 1X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1) 接下来将数据集训练三种不同类型的分类器：逻辑斯蒂回归分类器，决策树分类器和k-近邻分类器各一个： 12345678910111213141516171819from sklearn.model_selection import cross_val_scorefrom sklearn.linear_model import LogisticRegressionfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.pipeline import Pipelineimport numpy as pyclf1 = LogisticRegression(penalty=&#x27;l2&#x27;, C=0.01, random_state=0)clf2 = DecisionTreeClassifier(max_depth=1, criterion=&#x27;entropy&#x27;, random_state=0)clf3 = KNeighborsClassifier(n_neighbors=1, p=2, metric=&#x27;minkowski&#x27;)pipe1 = Pipeline([[&#x27;sc&#x27;, StandardScaler()], [&#x27;clf&#x27;, clf1]])pipe3 = Pipeline([[&#x27;sc&#x27;, StandardScaler()], [&#x27;clf&#x27;, clf3]])clf_labels = [&#x27;LR&#x27;, &#x27;DT&#x27;, &#x27;KNN&#x27;]for clf, label in zip([pipe1, clf2, pipe3], clf_labels): scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring=&#x27;roc_auc&#x27;) print(&#x27;ROC AUC: %.3f +/- %.3f [%s]&#x27; % (scores.mean(), scores.std(), label))&gt;&gt; ROC AUC: 0.917 +/- 0.201 [LR]&gt;&gt; ROC AUC: 0.917 +/- 0.154 [DT]&gt;&gt; ROC AUC: 0.933 +/- 0.104 [KNN] 在此，为什么将逻辑斯蒂回归和k-近邻分类器的训练作为流水线的一部分？不同于决策树，逻辑斯蒂回归和k-近邻算法对数据缩放不敏感，需要对其进行数据标准化处理。 接下来我们基于多数投票原则，在其中组合成员分类器： 12345678910mv_clf = MajorityVoteClassifier(classifiers=[pipe1, clf2, pipe3])clf_labels += [&#x27;MV&#x27;]all_clf = (pipe1, clf2, pipe3, mv_clf)for clf, label in zip(all_clf, clf_labels): scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring=&#x27;roc_auc&#x27;) print(&#x27;ROC AUC: %.3f +/- %.3f [%s]&#x27; % (scores.mean(), scores.std(), label))&gt;&gt; ROC AUC: 0.917 +/- 0.201 [LR]&gt;&gt; ROC AUC: 0.917 +/- 0.154 [DT]&gt;&gt; ROC AUC: 0.933 +/- 0.104 [KNN]&gt;&gt; ROC AUC: 0.967 +/- 0.100 [MV] 从上述输出来看，以10折交叉验证作为评估标准，MajorityVotingClassifier的性能与单个成员分类器相比有着质的提高。 评估与调优集成分类器接下来，我们将在测试数据上计算多数投票的ROC曲线，以验证其在未知数据上的泛化性能： 123456789101112131415161718from sklearn.metrics import roc_curvefrom sklearn.metrics import aucfrom matplotlib import pyplot as pltcolors = [&#x27;black&#x27;, &#x27;orange&#x27;, &#x27;blue&#x27;, &#x27;green&#x27;]linestyles = [&#x27;:&#x27;, &#x27;--&#x27;, &#x27;-.&#x27;, &#x27;-&#x27;]for clf, label, clr, ls in zip(all_clf, clf_labels, colors, linestyles): y_pred = clf.fit(X_train, y_train).predict_proba(X_test)[:, 1] fpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_pred) roc_auc = auc(x=fpr, y=tpr) plt.plot(fpr, tpr, color=clr, linestyle=ls, label=&#x27;%s (auc = %.3f)&#x27; % (label, roc_auc))plt.legend()plt.xlim([-0.1, 1.1])plt.ylim([-0.1, 1.1])plt.grid()plt.xlabel(&#x27;False Positive Rate&#x27;)plt.ylabel(&#x27;True Postive Rate&#x27;)plt.show() 得到的图像如下： img 由ROC结果我们可以得到，继承分类器在测试集上表现优秀（ROC AUC = 0.95），而KNN分类器对于训练数据有些过拟合。 在学习集成分类的成员分类器调优之前，我们调用一下get_param方法： 1mv_clf.get_params() 输出如下： 12345678910111213141516171819&#123;&#x27;pipeline-1&#x27;: Pipeline(memory=None, steps=[(&#x27;sc&#x27;, StandardScaler(copy=True, with_mean=True, with_std=True)), [&#x27;clf&#x27;, LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#x27;warn&#x27;, n_jobs=None, penalty=&#x27;l2&#x27;, random_state=0, solver=&#x27;warn&#x27;, tol=0.0001, verbose=0, warm_start=False)]], verbose=False),... &#x27;pipeline-2__clf__leaf_size&#x27;: 30, &#x27;pipeline-2__clf__metric&#x27;: &#x27;minkowski&#x27;, &#x27;pipeline-2__clf__metric_params&#x27;: None, &#x27;pipeline-2__clf__n_jobs&#x27;: None, &#x27;pipeline-2__clf__n_neighbors&#x27;: 1, &#x27;pipeline-2__clf__p&#x27;: 2, &#x27;pipeline-2__clf__weights&#x27;: &#x27;uniform&#x27;&#125; 接下来我们先通过网格搜索来调整逻辑斯蒂回归分类器的正则化系数倒数C以及决策深度： 12345678from sklearn.model_selection import GridSearchCVparams = &#123;&#x27;decisiontreeclassifier__max_depth&#x27;: [1, 2], &#x27;pipeline-1__clf__C&#x27;: [0.001, 0.1, 100.0]&#125;grid = GridSearchCV(estimator=mv_clf, param_grid=params, cv=10, scoring=&#x27;roc_auc&#x27;)grid.fit(X_train, y_train)res = grid.cv_results_for params, mean_score, std_score in zip(res[&#x27;params&#x27;], res[&#x27;mean_test_score&#x27;], res[&#x27;std_test_score&#x27;]): print(&#x27;%.3f +/- %.3f %r&#x27; % (mean_score, std_score, params)) 得到的结果如下： 1234560.967 +/- 0.100 &#123;&#x27;decisiontreeclassifier__max_depth&#x27;: 1, &#x27;pipeline-1__clf__C&#x27;: 0.001&#125;0.967 +/- 0.100 &#123;&#x27;decisiontreeclassifier__max_depth&#x27;: 1, &#x27;pipeline-1__clf__C&#x27;: 0.1&#125;1.000 +/- 0.000 &#123;&#x27;decisiontreeclassifier__max_depth&#x27;: 1, &#x27;pipeline-1__clf__C&#x27;: 100.0&#125;0.967 +/- 0.100 &#123;&#x27;decisiontreeclassifier__max_depth&#x27;: 2, &#x27;pipeline-1__clf__C&#x27;: 0.001&#125;0.967 +/- 0.100 &#123;&#x27;decisiontreeclassifier__max_depth&#x27;: 2, &#x27;pipeline-1__clf__C&#x27;: 0.1&#125;1.000 +/- 0.000 &#123;&#x27;decisiontreeclassifier__max_depth&#x27;: 2, &#x27;pipeline-1__clf__C&#x27;: 100.0&#125; 最优的参数和准确度如下： 123print(&#x27;Best params: %s\\nAcc: %.3f&#x27; % (grid.best_score_, grid.best_score_))&gt;&gt; Best params: 1.0&gt;&gt; Acc: 1.000 可以发现，当选择正则化强度较小时，我们能得到最佳的交叉验证结果，而决策树的深度似乎没有什么影响。注意在模型评估时，不止一次使用测试集并非一个好的做法。接下来将学习另外一种集成方法：bagging。 在本节中我们实现的多数投票方法有时也成为堆叠（stocking）。 bagging–通过bootstrap样本构建集成分类器bagging是一种与上节实现的MajorityVoteClassifier关系紧密的集成学习技术，但是不同的是这个算法没有使用相同的训练数据集拟合集成分类器中的单个成员分类器。由于原始数据集使用了bootstrap抽样（有放回的随机抽样），这也是bagging被称为bootstrap aggregating的原因。 接下来为了检验bagging的实际效果，我们用葡萄酒数据集构建一个更复杂的分类问题，在此我们只考虑葡萄酒中的类别2和类别3，且只选择Alcohol和Hue这两个特征： 123456789101112import pandas as pddf_wine = pd.read_csv(&#x27;./wine.data&#x27;, header=None)df_wine.columns = [&#x27;Class label&#x27;, &#x27;Alcohol&#x27;, &#x27;Malic acid&#x27;, &#x27;Ash&#x27;, &#x27;Alcalinity&#x27;, &#x27;Magnesium&#x27;, &#x27;Total phenols&#x27;, &#x27;Flavanoids&#x27;, &#x27;Nonflavanoids&#x27;, &#x27;Proanthocyanins&#x27;, &#x27;Color intensity&#x27;, &#x27;Hue&#x27;, &#x27;Diluted&#x27;, &#x27;Proline&#x27;]df_wine = df_wine[df_wine[&#x27;Class label&#x27;] != 1]y = df_wine[&#x27;Class label&#x27;].valuesX = df_wine[[&#x27;Alcohol&#x27;, &#x27;Hue&#x27;]].values 接下来，对类标进行编码，同时将数据集划分为测试集和训练集： 123456from sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import train_test_splitle = LabelEncoder()y = le.fit_transform(y)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1) scikit-learn中已经实现了Bagging Classifier相关算法，我们可以从ensemble子模块中导入使用： 1234567891011from sklearn.ensemble import BaggingClassifiertree = DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=None)bag = BaggingClassifier(base_estimator=tree, n_estimators=500, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, n_jobs=-1, random_state=1) 接下来我们将计算训练数据集和测试数据集上的预测准确率： 123456789from sklearn.metrics import accuracy_scoretree = tree.fit(X_train, y_train)y_train_pred = tree.predict(X_train)y_test_pred = tree.predict(X_test)tree_train = accuracy_score(y_train, y_train_pred)tree_test = accuracy_score(y_test, y_test_pred)print(tree_train, tree_test)&gt;&gt; 1.0 0.8333333333333334 基于上述代码执行的结果可见，未经剪枝的决策树显现出过拟合的现象。接下来看一下bag的拟合效果： 1234567bag = bag.fit(X_train, y_train)y_train_pred = bag.predict(X_train)y_test_pred = bag.predict(X_test)bag_train = accuracy_score(y_train, y_train_pred)bag_test = accuracy_score(y_test, y_test_pred)print(tree_train, tree_test)&gt;&gt; 1.0 0.8958333333333334 从上图可见bagging分类器在测试数据上的泛化性能稍有胜出。接下来看一下它们的决策区域： 12345678910111213141516x_min = X_train[:, 0].min() - 1x_max = X_train[:, 0].max() + 1y_min = X_train[:, 1].min() - 1y_max = X_train[:, 1].max() + 1xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))f, axarr = plt.subplots(nrows=1, ncols=2, sharex=&#x27;col&#x27;, sharey=&#x27;row&#x27;, figsize=(8, 3))for idx, clf, tt in zip([0, 1], [tree, bag], [&#x27;DT&#x27;, &#x27;Bagging&#x27;]): clf.fit(X_train, y_train) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) axarr[idx].contourf(xx, yy, Z, alpha=0.3) axarr[idx].scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c=&#x27;b&#x27;, marker=&#x27;^&#x27;) axarr[idx].scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c=&#x27;r&#x27;, marker=&#x27;o&#x27;) axarr[idx].set_title(tt)axarr[0].set_ylabel(&#x27;Alcohol&#x27;)plt.show() 可以得到图像如下： img 从结果可见，和深度为3的决策树相比，bagging集成分类器的决策边界显得更加平滑。 bagging算法是降低模型方差的一种有效方法，但是它在降低模型偏差方面的作用不大。 通过自适应boosting提高弱学习机的性能在本节中，重点讨论boosting算法的一个常用例子：Adaboost（Adaptive Boosting）。 在boosting中，集成分类器包含多个非常简单的成原分类器，这些成原分类器的性能仅仅好于随即猜测，常被称为弱学习机。原始的boosting过程如下： 从训练集D中以无放回抽样方式随机抽取一个训练子集$ d_1 $，用于弱学习机$ C_1 $的训练 从D中无放回抽样抽取一个训练子集$ d_2 $，并且将$ C_1 $中误分类样本的50%加入到训练集中，训练得到弱学习机$ C_2 $ 从训练集中抽取$ C_1 $和$ C_2 $分类结果不一致的样本生成训练样本$ d_3 $，以此训练第三个弱学习机$ C_3 $ 通过多数投票组合三个弱学习机$ C_1,C_2,C_3 $ 和bagging模型相比，boosting可以同时降低偏差和方差。在实践中，boosting算法对训练数据有过拟合的倾向。 Adaboost和原始的boosting算法不同，它使用整个训练集来训练弱学习机，其中训练样本在每次迭代中都会重新被赋予一个权重，在上一弱学习机错误的基础上进行学习从而构建一个更加强大的分类器。 1581052267970 如上图，从图1开始，所有的样本都被赋予相同的权重，基于次训练集，我们得到了一个分类器（决策曲线是图中虚线）；在下一轮中，我们为前面误分类的样本赋予更高的权重，此外我们降低被正确分类的样本的权重，如子图2所示，弱学习机错误划分了圆形类的三个样本，它们将在子图3中被赋予更大的权重…重复以上过程，最终组合三个学习机得到新的决策区域如图4。 下面是AdaBoost算法的基本步骤： 以等值的方式为权重向量$ w $赋值，其中$ \\sum_iw_i=1 $ 在m轮boosting操作中，对第j轮做如下操作 训练一个加权的弱学习机：$ C_j = train(X, y, w) $ 预测样本类标：$ \\hat{y} = =predict(C_j, X) $ 计算权重错误率：$ \\epsilon = w \\cdot (\\hat{y} == y)$ 计算相关系数：$ a_j = 0.5log\\frac{1-\\epsilon} {\\epsilon}$ 更新权重：$ w = w \\times exp(-a_j\\times \\hat{y} \\times y) $ 归一化权重：$ w:=w/\\sum_i{w_i} $ 完成最终预测：$ \\hat{y} = (\\sum_{j=1}^{m}(a_j\\times predict(C_j, X)) &gt; 0) $ 下面通过scikit-learn来训练一个AdaBoost集成分类器，我们仍然使用上一节中的葡萄酒数据集： 12345678910from sklearn.ensemble import AdaBoostClassifiertree = DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=1)ada = AdaBoostClassifier(base_estimator=tree, n_estimators=500, learning_rate=0.1, random_state=0)tree = tree.fit(X_train, y_train)y_train_pred = tree.predict(X_train)y_test_pred = tree.predict(X_test)tree_train = accuracy_score(y_train, y_train_pred)tree_test = accuracy_score(y_test, y_test_pred)print(tree_train, tree_test)&gt;&gt; 0.8450704225352113 0.8541666666666666 和上一节中未剪枝决策树相比，单层决策树对于训练数据过拟合的成都更加严重一点，接下来看一下AdaBoost分类器的性能： 1234567ada = ada.fit(X_train, y_train)y_train_pred = ada.predict(X_train)y_test_pred = ada.predict(X_test)ada_train = accuracy_score(y_train, y_train_pred)ada_test = accuracy_score(y_test, y_test_pred)print(ada_train, ada_test)&gt;&gt; 1.0 0.875 可以发现，A大Boost模型准确预测了所有的训练集类标，与单层决策树相比，它在测试集上的表现良好，不过，在代码中也可以看到，我们在降低模型偏差的同时使得方差额外的增加。 最后看一下决策区域的形状： 12345678910111213141516x_min = X_train[:, 0].min() - 1x_max = X_train[:, 0].max() + 1y_min = X_train[:, 1].min() - 1y_max = X_train[:, 1].max() + 1xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))f, axarr = plt.subplots(nrows=1, ncols=2, sharex=&#x27;col&#x27;, sharey=&#x27;row&#x27;, figsize=(8, 3))for idx, clf, tt in zip([0, 1], [tree, ada], [&#x27;DT&#x27;, &#x27;AdaBoost&#x27;]): clf.fit(X_train, y_train) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) axarr[idx].contourf(xx, yy, Z, alpha=0.5) axarr[idx].scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c=&#x27;b&#x27;, marker=&#x27;^&#x27;) axarr[idx].scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c=&#x27;r&#x27;, marker=&#x27;o&#x27;) axarr[idx].set_title(tt)axarr[0].set_ylabel(&#x27;Alcohol&#x27;)plt.show() 得到的图像如下： img 从上图可知，AdaBoost的决策区域比单层决策树的决策区域复杂得多。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"模型评估与参数调优实战","slug":"模型评估与参数调优实战","date":"2020-02-05T06:28:00.000Z","updated":"2020-12-17T10:23:43.626Z","comments":true,"path":"2020/02/05/模型评估与参数调优实战/","link":"","permalink":"http://blog.zsstrike.top/2020/02/05/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/","excerpt":"本章中，我们将使用代码进行实践，通过对算法进行调优来构建性能良好的机器学习模型，并对模型的性能进行评估。","text":"本章中，我们将使用代码进行实践，通过对算法进行调优来构建性能良好的机器学习模型，并对模型的性能进行评估。 基于流水线的工作流本节学习scikit-learn中的Pipeline类，它使得我们可以拟合出包含任意多个处理步骤的模型，并将模型用于新数据的预测。 加载威斯康辛乳腺癌数据集首先获取乳腺癌数据集： 12import pandas as pddf = pd.read_csv(&#x27;http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data&#x27;, header=None) 该数据集划分为32列，前两列是样本唯一ID和对样本的诊断结果（M代表恶性，B代表良性），后面的几列是包含了30个从细胞核照片中提取的特征。接下来，将数据集的30个特征赋值给数组对象X，同时转换诊断结果为数字： 1234567from sklearn.preprocessing import LabelEncoderX = df.loc[:, 2:].valuesy = df.loc[:, 1].valuesle = LabelEncoder()y = le.fit_transform(y)le.transform([&#x27;M&#x27;, &#x27;B&#x27;])&gt;&gt; array([1, 0], dtype=int64) 此时良性肿瘤和恶性肿瘤分别被标记为类0和类1。接下来将数据集划分为训练数据集和测试数据集： 12from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) 在流水线中集成数据转换及评估操作接下来可以直接使用Pipeline将上述步骤串联起来： 12345678910from sklearn.preprocessing import StandardScalerfrom sklearn.decomposition import PCAfrom sklearn.linear_model import LogisticRegressionfrom sklearn.pipeline import Pipelinepipe_lr = Pipeline([(&#x27;scl&#x27;, StandardScaler()), (&#x27;pca&#x27;, PCA(n_components=2)), (&#x27;clf&#x27;, LogisticRegression(random_state=1))])pipe_lr.fit(X_train, y_train)print(pipe_lr.score(X_test, y_test))&gt;&gt; 0.9473684210526315 Pipeline对象使用元组的序列作为输入，其中每个元组的第一个值为字符串，它可以是任意的标识符，我们通过它来访问流水线中的元素，而元组的第二个值为scikit-learn中的以恶转换器或者是评估器。 使用k折交叉验证评估模型性能本节中，我们学习两种有用的交叉验证技术：holdout交叉验证和k折交叉验证。借助于这两种方法，我们可以得到模型泛化误差的可靠估计，即模型在新数据上的性能表现。 holdout方法使用holdout进行模型选择更好的方法是将数据分为三个部分：训练数据集，验证数据集和测试数据集。训练数据集用于不同模型的拟合，模型在验证数据集上的性能表现作为模型选择的标准。使用模型训练和模型选择阶段不曾使用的数据作为测试数据集的优势在于：评估模型应用于新数据上能够获得较小偏差。 holdout方法的一个缺点是：模型性能的评估对训练数据集划分为训练及验证子集的方法是敏感的，评价的结果会随着样本的不同而发生变化。下一节介绍鲁棒性更好的性能评价技术：k折交叉验证，我们将在k个训练数据子集上重复holdout方法k次。 k折交叉验证在k折交叉验证中，我们不重复地随机将训练数据集划分为k个，其中$ k-1 $个用于模型的训练，剩余的1个用于测试。重复此过程k次，我们就得到了k个模型及对模型性能的评价。 由于k折交叉验证使用了无重复抽样技术，该方法的优势在于每个样本点只有一次被划入训练数据集或者测试数据集的机会，与holdout方法相比，这将会使得模型性能的评估具有较小的方差。 留一（LOO）交叉验证：在LOO中，我们将数据子集划分的数量等同于样本数（k = n），这样每次只有一个样本用于测试。 分层k折交叉验证相对于k折交叉验证做了稍许改进，它可以得到更低的偏差或方差。接下来通过scikit-learn中的StratifiedKFold迭代器来演示： 1234567891011import numpy as npfrom sklearn.model_selection import StratifiedKFoldkfold = StratifiedKFold(n_splits=10, random_state=1)scores = []k = 0for train, test in kfold.split(X_train, y_train): pipe_lr.fit(X_train[train], y_train[train]) score = pipe_lr.score(X_train[test], y_train[test]) scores.append(score) k += 1 print(&#x27;Fold: %s, Class dist.: %s, Acc: %.3f&#x27; % (k, np.bincount(y_train[train]), score)) 得到的结果如下： 12345678910Fold: 1, Class dist.: [256 153], Acc: 0.891Fold: 2, Class dist.: [256 153], Acc: 0.978Fold: 3, Class dist.: [256 153], Acc: 0.978Fold: 4, Class dist.: [256 153], Acc: 0.913Fold: 5, Class dist.: [256 153], Acc: 0.935Fold: 6, Class dist.: [257 153], Acc: 0.978Fold: 7, Class dist.: [257 153], Acc: 0.933Fold: 8, Class dist.: [257 153], Acc: 0.956Fold: 9, Class dist.: [257 153], Acc: 0.978Fold: 10, Class dist.: [257 153], Acc: 0.956 尽管之前的代码清楚介绍了k折交叉验证的工作方式，scikit-learn同样实现了k折交叉验证评分的计算，这时的我们可以更加高效地使用分层k折交叉验证对模型进行评估： 123456from sklearn.model_selection import cross_val_scorescores = cross_val_score(estimator=pipe_lr, X=X_train, y=y_train, cv=10, n_jobs=1)print(scores)&gt;&gt; [0.89130435 0.97826087 0.97826087 0.91304348 0.93478261 0.97777778 0.93333333 0.95555556 0.97777778 0.95555556]print(&#x27;CV Acc: %.3f +/- %.3f&#x27; % (np.mean(scores), np.std(scores)))&gt;&gt; CV Acc: 0.950 +/- 0.029 通过学习及验证曲线来调试算法在本节中，我们将会学习两个有助于提高学习算法性能的简单但功能强大的判定工具：学习曲线（learning curve）与验证曲线（validation curve）。 使用学习曲线判定偏差和方差问题通过将模型的训练及准确性验证看作是训练数据集大小的函数，并且绘制其图像，可以很容易地看出来模型面临高方差还是高偏差。 高偏差模型的训练准确率和交叉验证准确率都很低，这表明此模型未能很好地拟合数据。而高方差模型训练准确率和交叉验证准确率之间相差很大。 接下来，使用scikit-learn中的学习曲线函数评估模型： 123456789101112131415161718192021222324import matplotlib.pyplot as pltfrom sklearn.model_selection import learning_curvepipe_lr = Pipeline([(&#x27;scl&#x27;, StandardScaler()), (&#x27;clf&#x27;, LogisticRegression(penalty=&#x27;l2&#x27;, random_state=0))])train_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=10, n_jobs=1)train_mean = np.mean(train_scores, axis=1)train_std = np.std(train_scores, axis=1)test_mean = np.mean(test_scores, axis=1)test_std = np.std(test_scores, axis=1)plt.plot(train_sizes, train_mean, color=&#x27;b&#x27;, marker=&#x27;o&#x27;, label=&#x27;training acc.&#x27;)plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color=&#x27;b&#x27;)plt.plot(train_sizes, test_mean, color=&#x27;r&#x27;, marker=&#x27;s&#x27;, label=&#x27;validation acc&#x27;)plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color=&#x27;r&#x27;)plt.grid()plt.xlabel(&#x27;Number of training samples&#x27;)plt.ylabel(&#x27;Accuracy&#x27;)plt.legend()plt.ylim([0.8, 1.0])plt.show() 可以得到如下图像： img 从图像可知，模型在测试数据集上表现良好。 通过验证曲线来判定过拟合和欠拟合验证曲线和学习曲线类似，不过绘制的不是样本大小和训练准确率，测试准确率之间的关系，而是准确率与模型参数之间的关系，例如逻辑斯蒂回归模型中的正则化参数倒数C。下面使用scikit-learn来绘制验证曲线： 1234567891011121314151617181920212223from sklearn.model_selection import validation_curveparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]train_scores, test_scores = validation_curve(estimator=pipe_lr, X=X_train, y=y_train, param_name=&#x27;clf__C&#x27;, param_range=param_range, cv=10)train_mean = np.mean(train_scores, axis=1)train_std = np.std(train_scores, axis=1)test_mean = np.mean(test_scores, axis=1)test_std = np.std(test_scores, axis=1)plt.plot(param_range, train_mean, color=&#x27;b&#x27;, marker=&#x27;o&#x27;, label=&#x27;training acc.&#x27;)plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.5, color=&#x27;b&#x27;)plt.plot(param_range, test_mean, color=&#x27;r&#x27;, marker=&#x27;s&#x27;, label=&#x27;validation acc.&#x27;)plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha=0.5, color=&#x27;b&#x27;)plt.grid()plt.xscale(&#x27;log&#x27;)plt.legend()plt.xlabel(&#x27;Parameter C&#x27;)plt.ylabel(&#x27;Acc.&#x27;)plt.ylim([0.8, 1.0])plt.show() 得到的图像如下： img 在本例中，需要验证的是参数C，即定义在scikit-learn流水线中的逻辑斯蒂回归分类器的正则化参数，我们将其记为clf__C，并且通过param_range参数来设定其值的范围。 从上图可以看到，如果加大正则化强度（较小的C值），会导致模型的欠拟合；如果增加C的值，模型又会趋向于过拟合，在本例中，最优点在$ C=0.1 $附近。 使用网格搜索调优机器学习模型机器学习中，有两类参数：通过训练数据学习得到的参数，如逻辑斯蒂回归中的回归系数；以及学习算法中需要单独进行优化的参数。后者是调优参数，也称为超参，对模型来说，就如逻辑斯蒂回归中的正则化系数，或者决策树中的深度参数。 接下来，我们学习一种更加强大的超参数优化技巧：网格搜索（grid search），它通过寻找最优的超参数值的组合以进一步提高模型的性能。 使用网络搜索调优参数网格搜索法很简单，它通过对我们指定的不同超参列表进行暴力穷举法，来计算评估每个组合对模型性能的影响，以获得参数的最优组合： 1234567891011from sklearn.model_selection import GridSearchCVfrom sklearn.svm import SVCpipe_svc = Pipeline([(&#x27;scl&#x27;, StandardScaler()), (&#x27;clf&#x27;, SVC(random_state=1))])param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]param_grid = [&#123;&#x27;clf__C&#x27;: param_range, &#x27;clf__kernel&#x27;: [&#x27;linear&#x27;]&#125;, &#123;&#x27;clf__C&#x27;: param_range, &#x27;clf__gamma&#x27;: param_range, &#x27;clf__kernel&#x27;: [&#x27;rbf&#x27;]&#125;]gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring=&#x27;accuracy&#x27;, cv=10, n_jobs=-1)gs.fit(X_train, y_train)print(gs.best_score_, gs.best_params_)&gt;&gt; 0.978021978021978 &#123;&#x27;clf__C&#x27;: 0.1, &#x27;clf__kernel&#x27;: &#x27;linear&#x27;&#125; 在本例中，线性SVM模型可得到的最优k折交叉验证准确率为$ 97.8% $。 最后，我们使用独立的测试数据集，通过GridSearchCV对象的best_estimator_属性对最优模型进行评估： 1234clf = gs.best_estimator_clf.fit(X_train, y_train)print(clf.score(X_test, y_test))&gt;&gt; 0.9649122807017544 虽然网格搜索时寻找最优参数集合的一种功能强大的方法，但是他的计算成本时很高的，此时可以尝试使用另外一种方法：随即搜索（randomized search）。该方法在scikit-learn中的RandomizedSearchCV类已经实现。 通过嵌套交叉验证选择算法上一节的方法由于在同一个算法中找到最优超参，而本节中介绍的方法用于在不同的机器学习算法中找到最优的机器算法，它就是嵌套交叉验证。 在嵌套交叉验证中，我们将数据划分为训练块和测试块；而在用于模型选择的内部循环中，我们则基于这些训练块使用k折交叉验证。在完成模型的选择后，测试块用于模型性能的评估。 借助于scikit-learn，我们可以通过如下方法使用嵌套交叉验证： 1234gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring=&#x27;accuracy&#x27;, cv=10, n_jobs=-1)scores = cross_val_score(gs, X, y, scoring=&#x27;accuracy&#x27;, cv=5)print(&#x27;CV acc: %.3f +/- %.3f&#x27; % (np.mean(scores), np.std(scores)))&gt;&gt; CV acc: 0.972 +/- 0.012 代码返回的交叉验证准确率平均值对模型超参调优的预期值给出了很好的估计，且使用该值优化过的模型呢能够预测未知数据。例如，我们可以使用嵌套交叉验证方法比较SVM模型与决策树分类器；为了简单起见，我们只调优数的深度参数： 12345678from sklearn.tree import DecisionTreeClassifiergs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0), param_grid=[&#123;&#x27;max_depth&#x27;: [1, 2, 3, 4, 5, 6, 7, None]&#125;], scoring=&#x27;accuracy&#x27;, cv=5)scores = cross_val_score(gs, X_train, y_train, scoring=&#x27;accuracy&#x27;, cv=5)print(&#x27;CV acc: %.3f +/- %.3f&#x27; % (np.mean(scores), np.std(scores)))&gt;&gt; CV acc: 0.908 +/- 0.045 从两个算法的输出看，嵌套交叉验证对SVM的评价高于决策树。由此可见，SVM是用于对测数据集未知数据进行分类的一个更好的选择。 了解不同的性能评价指标前面的几个章节中，我们使用的都是模型准确性来对模型进行评估，接下来学习其他几个性能指标：准确率（precision），召回率（recall），F1分数（F1-score）。 读取混淆矩阵首先，了解一下混淆矩阵： 1580967439551 虽然这些指标可以人工比较的到结果，但是scikit-learn提供了一个confusion_matrix函数： 1234567from sklearn.metrics import confusion_matrixpipe_svc.fit(X_train, y_train)y_pred = pipe_svc.predict(X_test)conmat = confusion_matrix(y_true=y_test, y_pred=y_pred)print(conmat)&gt;&gt; [[71 1]&gt;&gt; [ 2 40]] 在执行上述代码后，我们可以得到混淆矩阵。在本例中，假定类别1是正类，模型正确预测了71个属于类别0的样本（真负），以及40个属于类别1的样本（真正）。 优化分类模型的准确率和召回率预测准确率（ACC）和预测误差率（ERR）都提供了样本分类的相关信息。他们的计算方法如下：$$ERR = \\frac{FP + FN}{FP + FN + TP + TN}\\ACC = \\frac{TP + TN}{FP + FN + TP + TN}$$对于 类别数量不均衡的分类问题爱来说，真正率（TPR）和假正率（FPR）是非常有用的指标：$$FPR = \\frac{FP}{N} = {FP}{FP + TN}\\TPR = \\frac{TP}{P} = {TP}{TP + FN}$$准确率（PRE）和召回率（REC）是和真正率，真负率相关的性能指标，实际上，召回率和真正率含义相同：$$PRE = \\frac{TP}{TP+FP}\\REC = TPR = \\frac{TP}{FN + TP}$$在实践中，常常用准确率和召回率的结合，称为F1分数：$$F1 = 2\\frac{PRE \\times REC}{PRE + REC}$$所有的这些评分指标在scikit-learn中已经实现，他们使用方法如下： 12345678from sklearn.metrics import precision_scorefrom sklearn.metrics import recall_score, f1_scoreprint(&#x27;Pre.: %.3f&#x27; % precision_score(y_true=y_test, y_pred=y_pred))print(&#x27;Rec.: %.3f&#x27; % recall_score(y_true=y_test, y_pred=y_pred))print(&#x27;F1.: %.3f&#x27; % f1_score(y_true=y_test, y_pred=y_pred))&gt;&gt; Pre.: 0.976&gt;&gt; Rec.: 0.952&gt;&gt; F1.: 0.964 请记住scikit-learn中将正类类标标识为1。如果我们想要指定一个不同的类标，可以通过make_scorer来构建我们自己的评分，这样我们可以将其应用于GridSearchCV： 123from sklearn.metrics import make_scorer, f1_scorescorer = make_scorer(f1_score, pos_label=0)gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring=scorer, cv=10) ROC曲线受试者工作特征曲线（receiver operator characteristic，ROC）是基于假正率和真正率等性能指标进行分类模型选择的有用工具，假正率和真正率通过移动分类器的阈值实现。基于ROC曲线，我们可以计算所谓的ROC线下区域（AUC），用来刻画分类模型的性能。 在scikit-learn中ROC AUC得分可以通过roc_auc_score函数计算得到： 12345from sklearn.metrics import roc_auc_score, accuracy_scoreprint(&quot;ACC: %.3f&quot; % (accuracy_score(y_true=y_test, y_pred=y_pred)))print(&#x27;ROC AUC: %.3f&#x27; % (roc_auc_score(y_true=y_test, y_score=y_pred)))&gt;&gt; ACC: 0.974&gt;&gt; ROC AUC: 0.969 通过ROC AUC得到的分类器性能可以让我们进一步洞悉分类器在类别不均衡样本集合上的性能。 多类别分类的评价标准本节中讨论的评分标准都是基于二分类系统的。不同，scikit-learn实现了macro（宏）和micro（微）均值方法，计算公式如下：$$PRE_{micro} = \\frac{\\sum_i^kTP_i}{\\sum_i^kTP_i+FP_i}\\PRE_{macro} = \\frac{\\sum_i^kPRE_i}{k}$$","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"通过降维压缩数据","slug":"通过降维压缩数据","date":"2020-02-04T09:28:09.000Z","updated":"2020-12-17T10:23:43.742Z","comments":true,"path":"2020/02/04/通过降维压缩数据/","link":"","permalink":"http://blog.zsstrike.top/2020/02/04/%E9%80%9A%E8%BF%87%E9%99%8D%E7%BB%B4%E5%8E%8B%E7%BC%A9%E6%95%B0%E6%8D%AE/","excerpt":"在本章中，我们将会学习到三种特征提取的方法，它们都可以将原始数据集变换到一个维度更低的新的特征子空间。","text":"在本章中，我们将会学习到三种特征提取的方法，它们都可以将原始数据集变换到一个维度更低的新的特征子空间。 无监督数据降维技术之主成分分析主成分分析（PCA）是一种广泛应用于不同领域的无监督线性数据转换技术，突出作用是降维。PCA的目标是在高维数据中找到最大方差的方向，并且将数据映射到一个维度不大于原始数据的新的子空间上。 如果使用PCA技术，我们需要构建一个$ d * k $维的转换矩阵$ W $，从而将原来的d维特征向量转换为k维特征向量（k&lt;d）。PCA算法的步骤如下： 对原始d维数据做标准化处理 构造样本的协方差矩阵 计算协方差矩阵的特征值和相应的特征向量 选择前k个最大特征对应的特征向量（k为新的特征空间维度） 通过前k个特征向量构建映射矩阵$ W $ 将原始的d维特征$ x $通过$ W $转换为新的k维特征$ x’ $ 总体方差和贡献方差这一小节完成PCA的前四个步骤。 首先，使用前面用到的葡萄酒数据集： 12import pandas as pddf_wine = pd.read_csv(&#x27;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&#x27;, header=None) 接着，将数据集划分为训练集和测试集，同时使用StandardScaler来将其标准化： 12345678from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].valuesX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)sc = StandardScaler()X_train_std = sc.fit_transform(X_train)X_test_std = sc.fit_transform(X_test) 接下来构造协方差矩阵，同时求解协方差矩阵的特征值和特征向量： 1234567import numpy as npcov_mat = np.cov(X_train_std.T)eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)print(eigen_vals)&gt;&gt; [4.8923083 2.46635032 1.42809973 1.01233462 0.84906459 0.60181514&gt;&gt; 0.52251546 0.08414846 0.33051429 0.29595018 0.16831254 0.21432212&gt;&gt; 0.2399553 ] 通过使用np.linalg.elg函数，可以得到一个包含有13个特征值的向量（eigen_vals）和一个13 * 13的特征矩阵（eigen_vecs），其中，特征向量以列的方式存在于特征矩阵中。 由于我们需要将数据压缩到一个新的特征子空间上实现降维，我们只需要选择那些包含最多信息的特征向量组成的子集。在此衡量函数是特征值$ \\lambda_j $的方差贡献率：$$\\frac{\\lambda_j}{\\sum_{i=1}^{d}j}$$接下来看一下不同特征值对应的方差贡献率： 1234tot = sum(eigen_vals)var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]print(var_exp)&gt;&gt; [0.3732964772349068, 0.18818926106599568, 0.10896790724757796, 0.07724389477124863, 0.0647859460182618, 0.045920138114781475, 0.03986935597634714, 0.025219142607261574, 0.022581806817679666, 0.01830924471952691, 0.016353362655051454, 0.01284270583749274, 0.006420756933868311] 可以知道，第一主成分占方差总和的$ 40% $左右。 特征转换接下来继续执行PCA方法的最后三个步骤。 首先，按照特征值的降序排列特征对： 12eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]eigen_pairs.sort(reverse=True) 接下来，我们只选择两个对应的最大的特征向量： 12345678910111213141516w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis]))print(w)&gt;&gt; [[ 0.14669811 0.50417079]&gt;&gt; [-0.24224554 0.24216889]&gt;&gt; [-0.02993442 0.28698484]&gt;&gt; [-0.25519002 -0.06468718]&gt;&gt; [ 0.12079772 0.22995385]&gt;&gt; [ 0.38934455 0.09363991]&gt;&gt; [ 0.42326486 0.01088622]&gt;&gt; [-0.30634956 0.01870216]&gt;&gt; [ 0.30572219 0.03040352]&gt;&gt; [-0.09869191 0.54527081]&gt;&gt; [ 0.30032535 -0.27924322]&gt;&gt; [ 0.36821154 -0.174365 ]&gt;&gt; [ 0.29259713 0.36315461]] 从而我们现在得到了一个13*2的映射矩阵$ W $。接下来转换原始的数据集： 1X_train_pca = X_train_std.dot(w) 最后，新的数据集被保存在124*2的矩阵中，接下来对其进行可视化： 123456789import matplotlib.pyplot as pltcolors = [&#x27;r&#x27;, &#x27;b&#x27;, &#x27;g&#x27;]markers = [&#x27;s&#x27;, &#x27;x&#x27;, &#x27;o&#x27;]for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(X_train_pca[y_train==l, 0], X_train_pca[y_train==l, 1], c=c, label=l, marker=m)plt.xlabel(&#x27;PC 1&#x27;)plt.ylabel(&#x27;PC 2&#x27;)plt.legend()plt.show() 得到的图像如下： img 从上图可以很直观的看到，线性分类器能够对其有很好的划分。 使用scikit-learn进行主成分分析我们先使用PCA对葡萄酒数据做预处理，然后再使用逻辑斯蒂回归模型对转换后的数据进行分类，最后绘制出散点图： 123456789101112131415from sklearn.decomposition import PCApca = PCA(n_components=2)X_train_pca = pca.fit_transform(X_train_std)X_test_pca = pca.transform(X_test_std)import matplotlib.pyplot as pltcolors = [&#x27;r&#x27;, &#x27;b&#x27;, &#x27;g&#x27;]markers = [&#x27;s&#x27;, &#x27;x&#x27;, &#x27;o&#x27;]for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(X_train_pca[y_train==l, 0], X_train_pca[y_train==l, 1], c=c, label=l, marker=m)plt.xlabel(&#x27;PC 1&#x27;)plt.ylabel(&#x27;PC 2&#x27;)plt.legend()plt.show() 得到的图像如下： img 比较该图和上一节中的图像，可以发现上图实际上就是我们自己完成的PCA图沿着PC1轴翻转的结果。出现此差异的原因在于特征分析方法：特征向量为正或者为负。 接下来使用逻辑斯蒂回归模型进行训练，并且得到训练结果： 12345678from sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import accuracy_scorelr = LogisticRegression()lr.fit(X_train_pca, y_train)y_pred = lr.predict(X_test_pca)accuracy_score(y_test, y_pred)&gt;&gt; 0.9814814814814815 可以发现的逻辑斯蒂回归模型的拟合率很优良。 通过线性判别分析压缩无监督数据线性判别分析（LDA）是一种可作为特征抽取的技术，它可以提高数据分析过程中的计算效率，同时，对于不适用于正则化的模型，它可以降低因维度灾难带来的过拟合。 LDA方法的步骤如下： 对d为数据集进行标准化处理 对于每一类别，计算d维的均值向量 构造类间的散布矩阵$ S_{B} $以及类内的散布举证$ S_{W} $ 计算矩阵$ s_{W}^{-1}S_{B} $的特征值及对应的特征向量 选取前k个特征值对应的特征向量，构造一个d*k维的转换矩阵$ W $ 使用转换矩阵$ W $将样本映射到新的特征子空间中 计算散布矩阵葡萄酒数据我们已经经过标准化处理，接下来求解均值向量$ m_i $： 1234567891011np.set_printoptions(precision=4)mean_vecs = []for label in range(1, 4): mean_vecs.append(np.mean(X_train_std[y_train==label], axis=0))print(mean_vecs)&gt;&gt; [array([ 0.9259, -0.3091, 0.2592, -0.7989, 0.3039, 0.9608, 1.0515,&gt;&gt; -0.6306, 0.5354, 0.2209, 0.4855, 0.798 , 1.2017]),&gt;&gt; array([-0.8727, -0.3854, -0.4437, 0.2481, -0.2409, -0.1059, 0.0187,&gt;&gt; -0.0164, 0.1095, -0.8796, 0.4392, 0.2776, -0.7016]),&gt;&gt; array([ 0.1637, 0.8929, 0.3249, 0.5658, -0.01 , -0.9499, -1.228 ,&gt;&gt; 0.7436, -0.7652, 0.979 , -1.1698, -1.3007, -0.3912])] 通过均值向量，我们计算一下类内散布矩阵$ S_W $:$$S_W = \\sum_{i=1}^cS_i$$这可以通过累加各类别i的散步矩阵$ S_i $来计算：$$S_i = \\sum_{x \\in D_i}^{c}(x-m_i)(x-m_i)^T$$ 12345678910d = 13S_W = np.zeros((d, d))for label, mv in zip(range(1, 4), mean_vecs): class_scater = np.zeros((d, d)) for row in X[y==label]: row, mv = row.reshape(d, 1), mv.reshape(d, 1) class_scater += (row - mv).dot((row - mv).T) S_W += class_scaterS_W.shape&gt;&gt; (13, 13) 此前，我们假定对散步矩阵计算时，曾假设训练集的类标是均匀分布的，但是，通过以下程序，我们发现其不遵守这个假设： 12np.bincount(y_train)&gt;&gt; array([ 0, 40, 49, 35], dtype=int64) 因此，在我们通过累加方式计算散布矩阵$ S_{W} $前，需要对各类别的散步矩阵$ S_i $做缩放处理。但采用此种方式时，此时散布矩阵和协方差矩阵计算方式相同。协方差矩阵可以看作是归一化的散布矩阵：$$\\frac{1}{N_i}S_{W} = \\frac{1}{N_i}\\sum_{x \\in D_i}^c(x-m_i)(x-m_i)^T$$ 1234567d = 13S_W = np.zeros((d, d))for label, mv in zip(range(1, 4), mean_vecs): class_scater = np.cov(X_train_std[y_train==label].T) S_W += class_scaterS_W.shape&gt;&gt; (13, 13) 接下来计算类间散布矩阵$ S_B $:$$S_B = \\sum_{i=1}^cN_i(m_i-m)(m_i-m)^T$$其中，m是全局均值，他在计算时用到了所有类别中的全部样本： 12345678910mean_overall = np.mean(X_train_std, axis=0)d = 13S_B = np.zeros((d, d))for i, mean_vec in enumerate(mean_vecs): n = X[y==i+1, :].shape[0] mean_vec = mean_vec.reshape(d, 1) mean_overall = mean_overall.reshape(d, 1) S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)S_B.shape&gt;&gt; (13, 13) 在新特征子空间上选取线性判别算法LDA余下的步骤和PCA的步骤相似： 12345eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]eigen_pairs = sorted(eigen_pairs, key=lambda k: k[0], reverse=True)for eigen_pair in eigen_pairs: print(eigen_pair[0]) 得到的结果如下： 12345678910111213643.0153843460517225.086981854162568.002675183788468e-145.757534614184537e-143.5105079604736804e-143.4638958368304884e-142.587811510007498e-142.587811510007498e-142.4449817310582036e-141.6532199129716054e-148.331225171347768e-152.3238388797036527e-156.522430076120113e-16 从上述输出来看，我们只得到了两个非零特征值（实际得到的3-13个特征值并未严格为0，这是由numpy的浮点数运算导致的），说明只有前面两个特征值对应的特征几乎包含了葡萄酒训练数据集中的全部有用信息。 接下来构造转换矩阵： 123456789101112131415w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real, eigen_pairs[1][1][:, np.newaxis].real))w&gt;&gt; array([[-0.0707, 0.3778],&gt;&gt; [ 0.0359, 0.2223],&gt;&gt; [-0.0263, 0.3813],&gt;&gt; [ 0.1875, -0.2955],&gt;&gt; [-0.0033, -0.0143],&gt;&gt; [ 0.2328, -0.0151],&gt;&gt; [-0.7719, -0.2149],&gt;&gt; [-0.0803, -0.0726],&gt;&gt; [ 0.0896, -0.1767],&gt;&gt; [ 0.1815, 0.2909],&gt;&gt; [-0.0631, -0.2376],&gt;&gt; [-0.3794, -0.0867],&gt;&gt; [-0.3355, 0.586 ]]) 将样本映射到新的特征空间通过上一节中构建的转换矩阵$ W $，我们来对原始数据进行转换： 123456789X_train_lda = X_train_std.dot(w)colors = [&#x27;r&#x27;, &#x27;b&#x27;, &#x27;g&#x27;]markers = [&#x27;s&#x27;, &#x27;x&#x27;, &#x27;o&#x27;]for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(X_train_lda[y_train==l, 0], X_train_lda[y_train==l, 1], c=c, label=l, marker=m)plt.xlabel(&#x27;LD 1&#x27;)plt.ylabel(&#x27;LD 2&#x27;)plt.legend()plt.show() 得到的图像如下： img 通过图像可知，三个葡萄酒类在新的特征子空间上是线性可分的。 使用scikit-laern进行LDA分析接下来，看一下scikit-laern中对LDA类的实现： 1234567891011from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDAlda = LDA(n_components=2)X_train_lda = lda.fit_transform(X_train_std, y_train)colors = [&#x27;r&#x27;, &#x27;b&#x27;, &#x27;g&#x27;]markers = [&#x27;s&#x27;, &#x27;x&#x27;, &#x27;o&#x27;]for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(X_train_lda[y_train==l, 0], X_train_lda[y_train==l, 1], c=c, label=l, marker=m)plt.xlabel(&#x27;LD 1&#x27;)plt.ylabel(&#x27;LD 2&#x27;)plt.legend()plt.show() 得到的图像如下： img 此时看一下逻辑斯蒂回归模型的预测准确度： 123456lr = LogisticRegression()lr = lr.fit(X_train_lda, y_train)X_test_lda = lda.fit_transform(X_test_std, y_test)y_pred = lr.predict(X_test_lda)accuracy_score(y_pred, y_test)&gt;&gt; 1.0 可以看到，逻辑斯蒂回归模型在测试数据集上对样本分类可谓完美。 使用核主成分分析进行非线性映射许多机器学习算法都假定输入数据是线性可分的，但是在现实世界中，大多数的数据是线性不可分的，针对此类问题，使用PCA或者LDA等降维技术，将其转化为线性问题并不是最好的方法。在本节中，我们将了解一下利用核技巧的PCA，或者称其为核PCA，这和第三章中我们介绍的核支持向量机的概念有一定的联系。使用核PCA，我们将学习如何将非线性可分的数据转换到一个适合对其进行线性分类的新的低维子空间中。 核函数通过核PCA，我们能够得到已经映射到各成分的样本，而不像标准PCA那样去构建一个转换矩阵。简单地说，可以将核函数理解为：通过两个向量点积来度量向量间相似度的函数。最常用的核函数有： 多项式核：$$k(x^i, x^j) = (x^{iT}x^j + \\theta)^p$$其中，阈值$ \\theta $和幂的值$ p $需要自行定义。 双曲正切（sigmoid）核：$$k(x^i, x^j) = thah(\\eta x^{iT}x^j+\\theta)$$ 径向基核函数（RBF）或者称为高斯核函数：$$k(x^i, x^j) = exp\\left(-\\frac{||x^i-x^j||^2}{2\\sigma^2}\\right)= exp(-\\gamma||x^i - x^j||^2)$$ 基于RBF核的PCA可以通过如下三个步骤实现： 为了计算核矩阵$ k $，我们需要做如下计算：$$k(x^i,x^j) = = exp(-\\gamma||x^i - x^j||^2)$$我们需要计算任意两个样本对之间的值：$$K = \\begin{bmatrix}k(x^1,x^1) &amp; k(x^1, x^2) &amp; \\cdots &amp; k(x^1, x^n)\\k(x^2,x^1) &amp; k(x^2, x^2) &amp; \\cdots &amp; k(x^2, x^n)\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\k(x^n,x^1) &amp; k(x^n, x^2) &amp; \\cdots &amp; k(x^n, x^n)\\\\end{bmatrix}$$ 通过如下公式，使得核矩阵$ k $更为聚集：$$K’ = K-l_nK-Kl_n+l_nKl_n$$其中， $ l_n $是一个n*n的矩阵，其所有的值都是$ \\frac{1}{n} $。 将聚集后的核矩阵的特征值按照降序排列，选择前k个特征值对应的特征向量。和标准PCA不同，这里的特征向量不是主成分轴，而是将样本映射到这些轴上。 使用Python实现主成分分析接下来，借助SciPy和NumPy的函数，我们手动实现一个核PCA： 123456789101112131415from scipy.spatial.distance import pdist, squareformfrom scipy import expfrom scipy.linalg import eighimport numpy as npdef rbf_kernel_pca(X, gamma, n_components): sq_dists = pdist(X, &#x27;sqeuclidean&#x27;) mat_sq_dists = squareform(sq_dists) K = exp(-gamma * mat_sq_dists) N = K.shape[0] one_n = np.ones((N, N)) / N K = K - one_n.dot(K) - k.dot(one_n) + one_n.dot(K).dot(one_n) eigvals, eigvecs = eigh(K) X_pc = np.column_stack((eigvecs[:, -i] for i in range(1, n_components + 1))) return X_pc 接下来查看几个实例。 实例一：分离半月形数据 首先创建一个包含100个样本点的二维数据集，以两个半月形状表示： 12345from sklearn.datasets import make_moonsX, y = make_moons(n_samples=100, random_state=123)plt.scatter(X[y==0, 0], X[y==0, 1], color=&#x27;r&#x27;, marker=&#x27;^&#x27;, alpha=0.5)plt.scatter(X[y==1, 0], X[y==1, 1], color=&#x27;b&#x27;, marker=&#x27;o&#x27;, alpha=0.5)plt.show() 得到的图像如下： img 显然，这两个半月形不是线性可分的，我们的目标是通过核PCA将这两个半月形数据展开，使得数据集成为适用于某一线性分类器的输入数据。 首先，我们看一下经过标准PCA处理的数据集的图像： 123456from sklearn.decomposition import PCAscikit_pca = PCA(n_components=2)X_spca = scikit_pca.fit_transform(X)plt.scatter(X_spca[y==0, 0], X_spca[y==0, 1], color=&#x27;r&#x27;, marker=&#x27;^&#x27;, alpha=0.5)plt.scatter(X_spca[y==1, 0], X_spca[y==1, 1], color=&#x27;b&#x27;, marker=&#x27;o&#x27;, alpha=0.5)plt.show() 得到的图像如下： img 可以发现，经过标准化PCA处理后，线性分类器未必能很好地发挥作用。 接下来尝试一下核PCA函数： 1234X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color=&#x27;r&#x27;, marker=&#x27;^&#x27;, alpha=0.5)plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color=&#x27;b&#x27;, marker=&#x27;o&#x27;, alpha=0.5)plt.show() 得到的图像如下： img 可以看到，此时两个类别是线性可分的。 示例二：分离同心圆 接下俩看一下非线性相关的另外一个例子：同心圆： 12345from sklearn.datasets import make_circlesX, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)plt.scatter(X[y==0, 0], X[y==0, 1], color=&#x27;r&#x27;, marker=&#x27;^&#x27;, alpha=0.5)plt.scatter(X[y==1, 0], X[y==1, 1], color=&#x27;b&#x27;, marker=&#x27;o&#x27;, alpha=0.5)plt.show() 得到的图像如下： img 接下来使用核PCA，观察数据集分布： 1234X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color=&#x27;r&#x27;, marker=&#x27;^&#x27;, alpha=0.5)plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color=&#x27;b&#x27;, marker=&#x27;o&#x27;, alpha=0.5)plt.show() 得到的图像如下： img 可以发现，此时两个类别的数据是线性可分的。 映射新的数据点在标准PCA方法中，我们通过转换矩阵和输入样本之间的点积来对数据进行映射。但是在核PCA中，该如何转换型的数据点呢？实际上，如果我们希望将新的样本$ x’ $映射到此主成分轴，需要进行如下计算：$$\\phi(x’)^Tv$$幸运的是，我们可以使用核技巧，这样就无需精确计算映射$ \\phi(x’)^Tv $。通过以下公式计算：$$\\phi(x’)^Tv = \\sum_ia^ik(x’, x^i)^T$$其中，核矩阵K的特征向量$ a $和特征值$ \\lambda $关系如下：$$Ka = \\lambda a$$通过如下程序实现映射： 1234def project_x(x_new, X, gamma, alphas, lambdas): pair_dist = np.array([np.sum(x_new - row)**2 for row in X]) k = np.exp(-gamma * pair_dist) return k.dot(alphas / lambdas) 其中，alphas是前k个特征向量，lambdas是前k个对应的特征值： 12alphas = np.column_stack((eigvecs[:, i] for i in range(1, n_components+1)))lambdas = [eigvals[-i] for i in range(1, n_components+1)] 将上述两条语句加到rbf_kernel_pca函数末端并且返回他们的值即可。 scikit-learn中的核主成分分析使用scikit-learn中的API实现核PCA如下： 1234567from sklearn.decomposition import KernelPCAX, y = make_moons(n_samples=100, random_state=123)scikit_kpca = KernelPCA(n_components=2, kernel=&#x27;rbf&#x27;, gamma=15)X_skpca = scikit_kpca.fit_transform(X)plt.scatter(X_skpca[y==0, 0], X_skpca[y==0, 1], color=&#x27;r&#x27;, marker=&#x27;^&#x27;, alpha=0.5)plt.scatter(X_skpca[y==1, 0], X_skpca[y==1, 1], color=&#x27;b&#x27;, marker=&#x27;o&#x27;, alpha=0.5)plt.show() 得到的图像如下： img 从上图来看，scikit-learn中KernelPCA得到的结果核我们手动实现的结果相一致。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"数据预处理","slug":"数据预处理","date":"2020-01-31T07:54:15.000Z","updated":"2020-12-17T10:23:43.617Z","comments":true,"path":"2020/01/31/数据预处理/","link":"","permalink":"http://blog.zsstrike.top/2020/01/31/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/","excerpt":"在本节中，我们将会学习主要的数据预处理技术，使用这些技术可以高效地构建好的机器学习模型。","text":"在本节中，我们将会学习主要的数据预处理技术，使用这些技术可以高效地构建好的机器学习模型。 缺失数据的处理在采集数据的时候，可能有的数据会有缺失的情况。通常我们见到的缺失值是数据表中的空值，或者是类似于NaN的占位符。 首先构造一个含有缺失值的CSV文件： 1234567891011121314import pandas as pdfrom io import StringIOcsv_data = &quot;&quot;&quot;A, B, C, D1.0, 2.0, 3.0, 4.05.0, 6.0, , 8.010.0, 11.0, 12.0, &quot;&quot;&quot;df = pd.read_csv(StringIO(csv_data))print(df) &gt;&gt; A B C D&gt;&gt; 0 1.0 2.0 3.0 4.0&gt;&gt; 1 5.0 6.0 NaN 8.0&gt;&gt; 2 10.0 11.0 12.0 NaN 上述代码中，我们通过read_csv将CSV格式的数据读取到pandas库的DataFrame中，可以看到有两个缺失值。 对于大的DataFrame来说，我们可以使用内置的isnull方法来判断某单元中是否含有缺失值： 123456df.isnull().sum()&gt;&gt; A 0&gt;&gt; B 0&gt;&gt; C 1&gt;&gt; D 1&gt;&gt; dtype: int64 通过这个方式我们可以得到每列中缺失值的数量。 将存在缺失值的特征或样本删除这是最简单的数据处理方式：将含有缺失值的特征（列）或者样本（行）从数据中删除。 可通过 dropna方法来删除包含缺失值的行： 123df.dropna()&gt;&gt; A B C D&gt;&gt; 0 1.0 2.0 3.0 4.0 类似地，我们可以通过将axis参数设置为1，以删除包含缺失值的列： 12345df.dropna(axis=1)&gt;&gt; A B &gt;&gt; 0 1.0 2.0 &gt;&gt; 1 5.0 6.0 &gt;&gt; 2 10.0 11.0 同样地， dropna方法还有其他的参数，以应对各种缺失值的情况： 12345678# only drop rows where all columns are NaNdf.dropna(how=&#x27;any&#x27;)# drop rows that hava not at least 4 non-NaN valuedf.dropna(thresh=4)# only drop rows where NaN in specific columns(here is &#x27;C&#x27;)df.dropna(subset=[&#x27;C&#x27;]) 删除数据是一种简单的方法，但是如果删除过多的样本，会导致分析结果可靠性不高。接下来学习另外一种最常用的处理缺失数据的方法：插值技术。 缺失数据填充所谓插值技术是指通过数据集中的其他训练样本的数据来估计缺失值，最常用的插值技术是均值插值（meaneinputation），即使用相应的特征均值来替换缺失值。我们可以使用scikit-learn中的Impute类来实现此方法： 12345678910from sklearn.preprocessing import Imputerimr = Imputer(missing_values=&#x27;NaN&#x27;, strategy=&#x27;mean&#x27;, axis=0)imr = imr.fit(df)imputed_data = imr.transform(df.values)imputed_data&gt;&gt; array([[ 1. , 2. , 3. , 4. ], [ 5. , 6. , 7.5, 8. ], [10. , 11. , 12. , 6. ]]) 首先计算各个特征列的均值，然后将均值插入到NaN处。参数axis用来控制按列计算均值还是按行计算均值，参数strategy还有median和most_frequent可选值。 理解scikit-learn预估器的API上一节中，我们使用的Imputer类来填充我们数据集中的缺失值，这个类属于scikit-learn中的转换器类，主要用于数据的转换。这些类中常用的两个方法是fit和transform。其中，fit方法用于对数据集中的参数进行识别并且构建相应的数据补齐模型，而transform方法则使用刚创建的数据补齐模型对数据集中的缺失值进行补齐。 在前面的章节中，我们用到了分类器，它们在scikit-learn中属于预估器类别，其API的设计与转换器非常相似。预估器包含一个predict方法，同时也包含一个transform方法。 处理类别数据目前我们只学习了处理数值型数据的方法，但是在真实的数据集中，常常会出现类别数据。类别数据可以进一步划分为标称特征和有序特征。有序特征可以理解为类别的值是可以排序的，如T恤的尺寸；相反，标称数据不具备排序的特征，如T恤的颜色。 首先构造一个数据集： 12345678910111213import pandas as pddf = pd.DataFrame([ [&#x27;green&#x27;, &#x27;M&#x27;, 10.1, &#x27;class1&#x27;], [&#x27;red&#x27;, &#x27;L&#x27;, 13.5, &#x27;class2&#x27;], [&#x27;blue&#x27;, &#x27;XL&#x27;, 15.3, &#x27;class1&#x27;]])df.columns = [&#x27;color&#x27;, &#x27;size&#x27;, &#x27;price&#x27;, &#x27;classlabel&#x27;]df&gt;&gt; color size price classlabel&gt;&gt; 0 green M 10.1 class1&gt;&gt; 1 red L 13.5 class2&gt;&gt; 2 blue XL 15.3 class1 我们构造的数据包括一个标称特征（颜色），一个有序特征（大小）以及一个数据特征（价格）。类标存储在最后一类。 有序特征的映射对于有序特征，scikit-learn中没有实现相应的自动转换方法，因此，我们需要手动构造相应的映射。假设尺寸之间的关系是：XL = L + 1 = M + 2. 1234567size_mapping = &#123;&#x27;XL&#x27;: 3, &#x27;L&#x27;: 2, &#x27;M&#x27;: 1&#125;df[&#x27;size&#x27;] = df[&#x27;size&#x27;].map(size_mapping)df&gt;&gt; color size price classlabel&gt;&gt; 0 green 1 10.1 class1&gt;&gt; 1 red 2 13.5 class2&gt;&gt; 2 blue 3 15.3 class1 如果在后续的过程中需要将整数值还原为有序字符串，可以简单定义一个逆映射字典inv_size_mapping = &#123;v : k for k, v in size_mapping.items()&#125;，然后再使用pandas提供的map方法即可。 类标的编码许多机器学习库中要求类标以整数值的方式进行编码。需要注意的一点是，类标不是有序的，因此，我们只需要简单的以枚举的方式从0开始设定类标： 1234import numpy as npclass_mapping = &#123;label: idx for idx, label in enumerate(np.unique(df[&#x27;classlabel&#x27;]))&#125;class_mapping&gt;&gt; &#123;&#x27;class1&#x27;: 0, &#x27;class2&#x27;: 1&#125; 接下来映射一下就行： 123456df[&#x27;classlabel&#x27;] = df[&#x27;classlabel&#x27;].map(class_mapping)df&gt;&gt; color size price classlabel&gt;&gt; 0 green 1 10.1 0&gt;&gt; 1 red 2 13.5 1&gt;&gt; 2 blue 3 15.3 0 同样可以构造一个逆映射来将类表还原为字符串。 此外，可以使用scikit-learn中的LabelEncoder类可以更加方便完成对类标的编码工作： 123456from sklearn.preprocessing import LabelEncoderclass_le = LabelEncoder()y = class_le.fit_transform(df[&#x27;classlabel&#x27;].values)y&gt;&gt; array([0, 1, 0], dtype=int64) 同样可以使用inverse_transform方法将类标转换为原始的字符串。 标称特征的独热编码常见的思路如下，使用LabelEncoder类将字符串转换为整数： 1234567X = df[[&#x27;color&#x27;, &#x27;size&#x27;, &#x27;price&#x27;]].valuescolor_le = LabelEncoder()X[:, 0] = color_le.fit_transform(X[:, 0])X&gt;&gt; array([[1, 1, 10.1], [2, 2, 13.5], [0, 3, 15.3]], dtype=object) 这样的数据处理是常见的错误处理方式，因为学习算法将会假定green大于blue，red大于green，这显然是不合理的。 标称特征不能像有序特征一样简单赋予一个整数值，最常用的转换方法是独热编码技术。这个方法的思想就是创建一个新的虚拟特征，虚拟特征的每一列各代表标称数据的一个值。在此，我们将color特征转换为三个新的特征：blue，green和red。此时可以通过二进制值来标识样本的颜色。 1234567from sklearn.preprocessing import OneHotEncoderohe = OneHotEncoder(categorical_features=[0])ohe.fit_transform(X).toarray()&gt;&gt; array([[ 0. , 1. , 0. , 1. , 10.1], [ 0. , 0. , 1. , 2. , 13.5], [ 1. , 0. , 0. , 3. , 15.3]]) 另外，我们可以通过pandas中的get_dummies方法，更加方便地实现虚拟特征。 12345pd.get_dummies(df[[&#x27;price&#x27;, &#x27;color&#x27;, &#x27;size&#x27;]])&gt;&gt; price size color_blue color_green color_red&gt;&gt;0 10.1 1 0 1 0 &gt;&gt;1 13.5 2 0 0 1&gt;&gt;2 15.3 3 1 0 0 将数据集划分为训练数据集和测试数据集接下来，我们将会使用葡萄酒数据集，可以通过UCI机器学习样本数据库来获得。通过pandas库，我们可以在线获取数据集： 1234df_wine = pd.read_csv(&#x27;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&#x27;, header=None)df_wine.columns = [&#x27;Class label&#x27;, &#x27;Alcohol&#x27;, &#x27;Malic acid&#x27;, &#x27;Ash&#x27;, &#x27;Alcalinity of ash&#x27;, &#x27;Magnesium&#x27;, &#x27;Total phenols&#x27;, &#x27;Flavanoids&#x27;, &#x27;Nonflavanoid phenols&#x27;, &#x27;Proanthocyanins&#x27;, &#x27;Color intensity&#x27;, &#x27;Hue&#x27;, &#x27;diluted wines&#x27;, &#x27;Proline&#x27;]df_wine.head() 得到数据集如下： 1580709518611 葡萄酒样本库通过13个不同的特征，对178个葡萄酒样本划分为类标为1，2，3的三个不同的类别，想要将这些样本划分为训练数据集和测试数据集，可以使用scikit-learn中的train_test_split函数： 1234from sklearn.model_selection import train_test_splitX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].valuesX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) 这样，我们就得到了$ 30% $的测试样本和$ 70% $的训练样本。 将特征的值缩放到相同的区间 特征缩放(peature scaling)是数据预处理中至关重要的一步，除了决策树和随机森林不需要特征缩放，其他的机器学习算法几乎都需要这个处理使得算法准确度提高。 特征缩放有两个常用的方法：归一化和标准化。归一化指的是将特征的值缩放到区间$ [0,1] $上，可以使用min-max缩放来实现：$$x_{norm}^i = \\frac{x^i - x_{min}}{x^i - x_{max}}$$在scikit-learn中，已经实现了min-max缩放： 12345from sklearn.preprocessing import MinMaxScalermms = MinMaxScaler()X_train_norm = mms.fit_transform(X_train)X_test_norm = mms.fit_transform(X_test) 而标准化的过程可以使用如下方程：$$x_{std}^i = \\frac{x^i - \\mu_x}{\\sigma_x}$$其中，$ \\mu_x $和$ \\sigma_x $分别表示某个特征列的均值和样本。同样地，可以使用scikit-learn中的方法实现标准化： 12345from sklearn.preprocessing import StandardScalerstdsc = StandardScaler()X_train_std = stdsc.fit_transform(X_train)X_test_std = stdsc.fit_transform(X_test) 选择有意义的特征如果一个模型在训练数据集上面的表现比在测试数据集上面好很多，那么很可能产生了过拟合。在本节中，我们将会学习使用正则化和特征选择降维这两种常用的减少过拟合问题的方法。 使用L1正则化满足数据稀疏化在第三章节中，权重向量的L2范数如下：$$L2：||w||2^2=\\sum{j=1}^{m}w_j^2$$而降低模型复杂度的L1正则化公式：$$L1：||w||1 = \\sum{j=1}^m|w_j|$$对于scikit-learn来说，已经支持了 L1的正则化模型，可以将penalty参数设置为’l1’来进行简单的数据稀处理： 123from sklearn.linear_model import LogisticRegressionLogisticRegression(penalty=&#x27;l1&#x27;) 我们将L1正则化用于标准化处理的葡萄酒数据，经过L1正则化的逻辑斯蒂回归模型可以产生如下稀疏化结果： 123456lr = LogisticRegression(penalty=&#x27;l1&#x27;, C=0.1)lr.fit(X_train_std, y_train)print(&#x27;Training accuracy: &#x27;, lr.score(X_train_std, y_train))print(&#x27;Test accuracy: &#x27;, lr.score(X_test_std, y_test))&gt;&gt; Training accuracy: 0.9838709677419355&gt;&gt; Test accuracy: 0.9814814814814815 训练和测试的精确度显示此模型未出现过拟合，通过如下代码可以获得截距项： 12lr.intercept_&gt;&gt; array([-0.38381104, -0.1580416 , -0.70043119]) 由于我们lr对象默认使用了一对多(One vs Rest, OvR)的方法，因此，第一项截距是类别1相对于类别2和类别3的匹配结果。同样，我们可以查看系数矩阵： 12345678910lr.coef_&gt;&gt; array([[ 0.2801916 , 0. , 0. , -0.02793952, 0. ,&gt;&gt; 0. , 0.71018709, 0. , 0. , 0. ,&gt;&gt; 0. , 0. , 1.2362193 ],&gt;&gt; [-0.64408995, -0.06876656, -0.05722202, 0. , 0. ,&gt;&gt; 0. , 0. , 0. , 0. , -0.92643033,&gt;&gt; 0.06037655, 0. , -0.37111071],&gt;&gt; [ 0. , 0.06151885, 0. , 0. , 0. ,&gt;&gt; 0. , -0.6360538 , 0. , 0. , 0.49810762,&gt;&gt; -0.35817768, -0.57128442, 0. ]]) 可以发现，权重向量是稀疏的，这意味着只有少数几个特征被考虑进来，符合L1的作用效果。 最后，对L1正则化来说，在强的正则化参数(C&lt;0.1)的作用下，罚项使得所有的特征权重趋于0。 在前面已经介绍过，$ \\lambda $是正则化参数，而C是正则化参数的倒数。 序列特征选择算法另外一种降低模型复杂度从而解决过拟合问题的方法是通过特征选择进行降维，该方法对未经正则化处理的模型特别有效。降维技术主要分为两个大类：特征选择和特征提取。通过特征选择，可以选择原始特征的一个子集；而在特征提取中，通过对现有的特征信息进行推演，构造出一个新的特征子空间。 在本节中，我们着眼于一些经 序列特征选择算法是一种贪婪搜索方法，用于将原始的d维特征空间压缩到一个k维空间中，其中$ k &lt; d $。一个经典的序列特征选择算法是序列后向选择算法（SBS），其目的是在分类行性能衰弱最小的约束小，降低原始数据的维度，提高计算效率。 SBS算法的理念很简单：SBS依次从特征集合中删除某些特征，直到新的特征子空间包含指定数量的特征。为了确定每一步需要删除的特征，为此我们需要定义一个最小化的标准衡量函数J。该函数的计算准则是：比较判定分类器在删除某个特征前后的差异，每次删除的特征，就是那些能够使得标准衡量函数值尽可能大的特征，或者说，每一步特征被删除后，所引起的模型性能损失最小。 基于上述对SBS的定义，总结出以下四个步骤： 设$ k = d $进行算法初始化，其中 d 是特征空间$ X_d $的维度。 定义$ x^- $为满足标准$ x^- = argmaxJ(X_k - x) $最大化的特征，其中$ x \\in X_k $。 将特征$ x^- $从特征集中删除：$ X_{k-1} = X_k - x^- , k = k -1$。 如果k的值等于目标特征数量，算法终止，否则跳转到第2步。 遗憾的是，scikit-learn并没有实现SBS算法，我们可以手动实现它： 123456789101112131415161718192021222324252627282930313233343536373839404142434445from sklearn.base import clonefrom itertools import combinationsimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scoreclass SBS(): def __init__(self, estimator, k_features, scoring=accuracy_score, test_size=0.25, random_state=1): self.scoring = scoring self.estimator = clone(estimator) self.k_features = k_features self.test_size = test_size slef.random_state = random_state def fit(self, X, y): X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state) dim = X_train.shape[1] self.indices_ = tuple(range(dim)) self.subsets_ = [self.indices_] score = self._calc_score(X_train, y_train, X_test, y_test, self.indices_) self.scores_ = [score] while dim &gt; self.k_features: scores = [] subsets = [] for p in combinations(self.indices_, r=dim-1): score = self._calc_score(X_train, y_train, X_test, y_test, p) scores.append(score) subsets.append(p) best = np.argmax(scores) self.indices_ = subsets[best] self.subsets_.append(self.indices_) dim -= 1 self.scores_.append(scores[best]) self.k_score_ = self.scores_[-1] return self def transform(self, X): return X[:, self.indices_] def _calc_score(self, X_train, y_train, X_test, y_test, indices): self.estimator.fit(X_train[:, indices], y_train) y_pred = self.estimator.predict(X_test[:, indices]) score = self.scoring(y_test, y_pred) return score 我们使用k_features来指定需要返回的特征数量，并且最终特征子集的列标被赋值给self.indices_。注意，在fit方法中，我们没有在fit方法中明确地计算评价标准，只是简单的删除了那些没有包含在最优特征子集中的特征。 接下来我们看一下SBS应用于KNN分类器的效果： 12345678910111213from sklearn.neighbors import KNeighborsClassifierimport matplotlib.pyplot as pltknn = KNeighborsClassifier(n_neighbors=2)sbs = SBS(knn, k_features=1)sbs.fit(X_train_std, y_train)k_feat = [len(k) for k in sbs.subsets_]plt.plot(k_feat, sbs.scores_, marker=&#x27;o&#x27;)plt.ylim([0.7, 1.1])plt.ylabel(&#x27;Accuracy&#x27;)plt.xlabel(&#x27;Number of features&#x27;)plt.grid()plt.show() 得到的图像如下： img 可以发现，当k = {5, 6, 7, 8, 9, 10}时，算法可以达到百分百的准确率。 接下来看一下是哪五个特征在验证数据集上有如此良好的表现： 123k5 = list(sbs.subsets_[8])print(df_wine.columns[1:][k5])&gt;&gt; Index([&#x27;Alcohol&#x27;, &#x27;Malic acid&#x27;, &#x27;Alcalinity of ash&#x27;, &#x27;Hue&#x27;, &#x27;Proline&#x27;], dtype=&#x27;object&#x27;) 通过随机森林判定特征的重要性接下来使用随机森林来从数据集中选择相关特征，下面的代码根据葡萄酒数据集特征重要程度对这13个特征给出重要性等级。但是注意：无需对基于树的模型做标准化或者归一化处理。代码如下： 12345678from sklearn.ensemble import RandomForestClassifierfeat_labels = df_wine.columns[1:]forest = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)forest.fit(X_train, y_train)importances = forest.feature_importances_indices = np.argsort(importances)[::-1]for f in range(X_train.shape[1]): print(&quot;%2d) %-*s %f&quot; % (f + 1, 30, feat_labels[f], importances[indices[f]])) 得到的输出数据如下： 12345678910111213 1) Alcohol 0.182483 2) Malic acid 0.158610 3) Ash 0.150948 4) Alcalinity of ash 0.131987 5) Magnesium 0.106589 6) Total phenols 0.078243 7) Flavanoids 0.060718 8) Nonflavanoid phenols 0.032033 9) Proanthocyanins 0.02540010) Color intensity 0.02235111) Hue 0.02207812) diluted wines 0.01464513) Proline 0.013916 从上述输出我们可以得到最具有判别效果的特征是‘Alcohol’。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"使用scikit-learn实现分类算法","slug":"使用scikit-learn实现分类算法","date":"2020-01-30T05:17:10.000Z","updated":"2020-12-17T10:23:43.573Z","comments":true,"path":"2020/01/30/使用scikit-learn实现分类算法/","link":"","permalink":"http://blog.zsstrike.top/2020/01/30/%E4%BD%BF%E7%94%A8scikit-learn%E5%AE%9E%E7%8E%B0%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/","excerpt":"在本节中，我们将会介绍常用分类算法的概念，以及如何使用 scikit-learn 机器学习库和选择机器学习算法时需要注意的问题。","text":"在本节中，我们将会介绍常用分类算法的概念，以及如何使用 scikit-learn 机器学习库和选择机器学习算法时需要注意的问题。 分类算法的选择机器学习算法涉及到的五个步骤可以描述如下： 特征的选择 确定性能评价标准 选择分类器及其优化算法 对模型性能的评估 算法的调优 在本节中集中学习不同分类算法的概念，并再次回顾特征选择，预处理及性能评价指标等内容。 初涉 scikit-learn 的使用首先，使用 scikit-learn 来实现一个感知器模型，这个模型和前面讲的感知器模型类似。仍旧使用鸢尾花数据集中的两个特征。 提取150朵鸢尾花的花瓣长度和宽度两个特征的值，并且由此构建矩阵$ X $，同时将对应的类标赋值给$ y $： 123456from sklearn import datasetsimport numpy as npiris = datasets.load_iris()X = iris.data[:, [2, 3]]y = iris.target 为了评估训练得到的模型在位置数据上的表现，我们进一步将数据集划分为训练数据集和测试数据集： 1234from sklearn.model_selection import train_test_split# sklearn.cross_validation 已经废弃，改用sklearn.model_selectionX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) 由此，我们得到45个测试样本和105个训练样本。为了优化性能，还需要对数据进行特征缩放： 123456from sklearn.preprocessing import StandardScalersc = StandardScaler()sc.fit(X_train)X_train_std = sc.transform(X_train)X_test_std = sc.transform(X_test) 通过调用 sc.fit可以计算出X_train的每个特征的样本均值$ \\mu $和标准差$ \\sigma $。通过调用transform方法，可以使用已经计算出来的$ \\mu $和$ \\sigma $来对训练集数据做标准化处理。在特征缩放后，我们就可以训练感知器模型了： 1234from sklearn.linear_model import Perceptronppn = Perceptron(max_iter=40, eta0=0.1, random_state=0)ppn.fit(X_train_std, y_train) 训练好后，就可以进行预测了： 123y_pred = ppn.predict(X_test_std)print(&#x27;Misclassified samples: %d&#x27; % (y_test != y_pred).sum())&gt;&gt; Misclassified samples: 5 最终可以看到有5个预测错误，从而准确率是$ 89% $。同样地，scikit-learn还实现了许多不同 的性能矩阵，可以通过如下代码计算准确率： 1234from sklearn.metrics import accuracy_scoreprint(&#x27;Accuracy: %.2f&#x27; % accuracy_score(y_test, y_pred))&gt;&gt; Accuracy: 0.89 最后，我们可以在第二章中实现的plot_decision_regions函数来绘制刚刚训练过的决策区域，并且观察不同分类的效果，代码如下： 12345678910111213141516171819202122232425262728from matplotlib.colors import ListedColormapimport matplotlib.pyplot as pltdef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # setup marker generator and color map markers = (&#x27;s&#x27;, &#x27;x&#x27;, &#x27;o&#x27;, &#x27;^&#x27;, &#x27;v&#x27;) colors = (&#x27;red&#x27;, &#x27;blue&#x27;, &#x27;lightgreen&#x27;, &#x27;gray&#x27;, &#x27;cyan&#x27;) cmap = ListedColormap(colors[:len(np.unique(y))]) # plot the decision surface x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot all samples X_test, y_test = X[test_idx, :], y[test_idx] for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl) if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], c=&#x27;orange&#x27;, alpha=1, linewidth=1, marker=&#x27;+&#x27;, s=55, label=&#x27;test set&#x27;) 接下来，就可以回值决策区域图了： 1234567X_combined_std = np.vstack((X_train_std, X_test_std))y_combined = np.hstack((y_train, y_test))plot_decision_regions(X=X_combined_std, y=y_combined, classifier=ppn, test_idx=range(105, 150))plt.xlabel(&#x27;petal length&#x27;)plt.ylabel(&#x27;petal width&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show() 绘图如下： img 从图中我们发现无法通过一个线性的决策边界完美划分三类样本。对于无法完美线性可分的数据集，感知器算法将会永远无法收敛，这也是实践中一般不使用感知器算法的原因。 逻辑斯蒂回归中的类别概率初识逻辑斯蒂回归模型逻辑斯蒂回归模型和Adaline模型类似，不同的是在Adaline中，我们使用$ \\phi(z)=z $作为激励函数，而在逻辑斯蒂回归中使用的是sigmoid函数作为激励模型：$$sigmoid(z) = \\phi(z) = \\frac{1}{1+e^{-z}}$$它的函数图像如下： img 在给定特征$ x $和权重$ w $的情况下，sigmoid函数的输出值给出了特定样本$ x $属于类别1的概率$ \\phi(z) = P(y=1|x;w) $。预测到的概率可以通过一个量化器进行二元输出：$$\\hat{y}=\\begin{cases}1 &amp; \\phi(z) \\ge 0.5\\0 &amp; others\\end{cases}$$对应的，逻辑斯蒂回归模型图如下： See the source image 逻辑斯蒂回归模型的代价函数在构建逻辑斯蒂回归模型时，需要先定义一个最大似然函数，公式如下:$$L(w) = \\prod_{i=1}^{n}P(y^i|x^i;w)=(\\phi(z^i))^{y^i}(1-\\phi(z))^{1-y^i}$$然后取对数并且改写一下，得到如下：$$J(w) = \\sum_i^n-y^ilog(\\phi(z^i)) - (1-y^i)log(1-\\phi(z^i))$$我们可以对单个样本实例进行成本分析：$$J(\\phi(z),y;w) = \\begin{cases}-log(\\phi(z)) &amp; y=1\\-log(1-\\phi(z)) &amp; y=0\\end{cases}$$ 可以看到，如果正确将样本划分到类别1和0中，代价都将会趋于0，但是如果错误分类，代价将会区域无穷，这也就意味着错误预测带来的代价将会越来越大。 使用scikit-learn训练逻辑斯蒂回归模型接下来，我们使用逻辑斯蒂回归模型来训练鸢尾花数据集： 123456789from sklearn.linear_model import LogisticRegressionlr = LogisticRegression(C=1000.0, random_state=0)lr.fit(X_train_std, y_train)plot_decision_regions(X_combined_std, y_combined, classifier=lr, test_idx=range(105, 150))plt.xlabel(&#x27;length&#x27;)plt.ylabel(&#x27;width&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show() 得到的决策区域图如下： img 此外，可以通过predict_proba来预测样本属于某个类别的概率： 12lr.predict_proba(X_test_std[0:1, :])&gt;&gt; array([[1.78177322e-11, 6.12453348e-02, 9.38754665e-01]]) 此结果表示模型预测此样本属于类标1的概率是$ 6.1% $，属于类标2的概率是$ 93.9% $。 通过正则化解决过拟合问题过拟合是机器学习中常见的问题，过拟合具有高方差，这可能是使用了较多的参数，使得模型过于复杂。同样地，模型也会面临着欠拟合问题，欠拟合具有高偏差，这意味着模型过于简单，使得我们在预测时性能不佳。 img 偏差-方差权衡（bias-variance tradeoff）就是通过正则化来调整模型的复杂度。正则化时解决共线性（特征间高度相关）的一个很有用的方法，最常用的正则化形式是L2正则化，可以写作：$$\\frac{\\lambda}{2}||w||^2=\\frac{\\lambda}{2}\\sum_{j=1}^m w_j^2$$其中，$ \\lambda $是正则化系数。 特征缩放之所以很重要，其中一个原因是正则化。为了使得正则化起作用，需要确保所有特征的衡量标准保持统一。 使用正则化方法时，我们只需要在逻辑斯蒂回归的代价函数中加入正则化项，以降低系数带来的副作用：$$J(w) = \\left(\\sum_i^n-y^ilog(\\phi(z^i)) - (1-y^i)log(1-\\phi(z^i))\\right)+\\frac{\\lambda}{2}||w||^2$$前面用到的scikit-learn中的LogisticRegression类，其中的参数C时正则化系数的倒数：$$C = \\frac{1}{\\lambda}$$ 使用支持向量机最大化分类间隔另外一种性能强大且广泛应用的学习算法时支持向量机（SVM），它可以看作是对感知器的扩展。在SVM中，我们的目标是最大化分类间隔。在此处间隔指的是两个分离的决策边界间的距离，而最靠近决策边界的训练样本称作是支持向量： 支持向量机（SVM）——原理篇 对分类间隔最大化的直观认识我们将平面分为正平面和负平面，对于正平面来说：$$w_0+w^TX_{pos}=1$$对于负平面：$$w_0+w^TX_{neg}=-1$$对以上两式，相减得$$w^T(X_{pos}-X_{neg})=2$$定义$ ||w|| = \\sqrt{\\sum_{j=1}^{m}w_j^2} $，于是可得到如下等式：$$\\frac{w^T(X_{pos}-X_{neg})}{||w||}=\\frac{2}{||w||}$$上述等式的左侧可以解释为正负平面间的距离，也就是我们要最大化的距离。在样本正确分类的前提下，最大化分类间隔就是$ \\frac{2}{||w||} $最大化，这也是SVM的目标函数，记作：$$w_0+w^Tx^i \\ge 1, if\\ y^i=1\\w_0+w^Tx^i \\lt -1, if\\ y^i=-1$$这两个方程可以解释为：所有的负样本都落在负超平面一侧，所有的正样本都落在正超平面一侧划分的区域中。实践中，使用二次规划方法很容易求出$ \\frac{||w||}{2} $的最小值。 使用松弛变量解决非线性可分问题引入松弛变量$ \\xi $的目的是：放松线性约束条件，以保证在适当的惩罚项样本下，对错误分类的情况进行优化时能够收敛。 取值为正的松弛变量可以简单的加到线性约束条件中：$$w^Tx^i \\ge 1, if\\ y^i=1 - \\xi^i\\w^Tx^i \\lt -1, if\\ y^i=-1 + \\xi^i$$由此，新的优化目标为$$\\frac{||w||}{2}+C(\\sum_i\\xi^i)$$通过变量C，我们可以控制对错误分类的惩罚程度，进而在偏差和方差之间取得平衡。 使用scikit-learn实现SVM接下来，我们使用SVM模型来对鸢尾花数据集中的样本进行分类： 123456789from sklearn.svm import SVCsvm = SVC(kernel=&#x27;linear&#x27;, C=1.0, random_state=0)svm.fit(X_train_std, y_train)plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105, 150))plt.xlabel(&#x27;length&#x27;)plt.ylabel(&#x27;width&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show() 得到图像如下： img 在实际的分类任务中，线性逻辑斯蒂回归和支持向量机往往得到相似的结果。但是逻辑斯蒂回归比SVM更容易处理离群点，而SVM更关注接近决策边界的点。 在有些数据集很大的时候，可以使用scikit-learn提供的SGDClassifier类供用户选择，这个流泪还提供了partial-fit方法支持在线学习。SGDClassifier类的概念类似于随机梯度算法。 我们可以使用以下方式分别构建基于随机梯度下降的感知器，逻辑斯蒂回归以及支持向量机模型。 12345from sklearn.linear_model import SGDClassifierppn = SGDClassifier(loss=&#x27;perceptron&#x27;)lr = SGDClassifier(loss=&#x27;log&#x27;)svm = SGDClassifier(loss=&#x27;hinge&#x27;) 使用核SVM解决非线性问题SVM受欢迎的一个原因是：通过“核技巧”可以很容易解决非线性可分问题。 首先来了解非线性可分问题到底是什么。通过NumPy的logicol_xor来创建数据集，其中100个样本属于类别1，另外的100个样本属于类别-1： 123456789np.random.seed(0)X_xor = np.random.randn(200, 2)y_xor = np.logical_xor(X_xor[:, 0] &gt; 0, X_xor[:, 1] &gt; 0)y_xor = np.where(y_xor, 1, -1)plt.scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c=&#x27;b&#x27;, marker=&#x27;x&#x27;, label=&#x27;1&#x27;)plt.scatter(X_xor[y_xor==-1, 0], X_xor[y_xor==-1, 1], c=&#x27;r&#x27;, marker=&#x27;s&#x27;, label=&#x27;-1&#x27;)plt.ylim(-3.0)plt.legend()plt.show() 执行以上代码后，我们得到了一个“异或”数据集，二维分布如下： img 显然，使用前面提到的线性逻辑斯蒂回归或者是线性SVM模型，都无法将样本正确划分为正类别和负类别。 核方法的基本思想是：通过映射函数$ \\phi(\\cdot) $将样本的原始特征映射到一个使样本线性可分的更高维的空间中，然后找到分界面后作反变换$ \\phi^{-1}(\\cdot) $可得到最初的划分平面。 但是这种映射会带来非常大的计算成本，这个时候我们就可以使用核技巧的方法。在实践中，我们所需要做的就是将点积$ x^{iT}x^j $映射为$ \\phi((x^i)^T \\phi(x^j) $，为此定义$$k(x^i, x^j) = \\phi((x^i)^T \\phi(x^j)$$一个最常使用的核函数就是径向基核函数（RBF kernel）：$$k(x^i, x^j) = exp(-\\frac{||x^i-x^j||^2}{2\\sigma^2}) = exp(-\\gamma ||x^i-x^j||^2)$$其中，$ \\gamma = \\frac{1}{2\\sigma^2} $是待优化的自由参数。接下来，就使用scikit-learn来训练一个核SVM使之可以对“异或”数据集进行分类： 12345svm = SVC(kernel=&#x27;rbf&#x27;, random_state=0, gamma=0.10, C=10.0)svm.fit(X_xor, y_xor)plot_decision_regions(X_xor, y_xor, classifier=svm)plt.legend()plt.show() 得到的决策边界如下： img 正如图像所示，核SVM较好地完成了对“异或”数据的划分。 在此，我们将$ \\gamma $参数设置为了0.1，可以将其理解为高斯球面的截止参数（cut-off parameter）。为了更好的理解$ \\gamma $参数，我们将基于RBF的SVM应用于鸢尾花数据集： 1234567svm = SVC(kernel=&#x27;rbf&#x27;, random_state=0, gamma=0.20, C=10.0)svm.fit(X_train_std, y_train)plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105, 150))plt.xlabel(&#x27;length&#x27;)plt.ylabel(&#x27;width&#x27;)plt.legend()plt.show() 得到的图像如下： img 现在改变$ \\gamma = 5.0 $，得到的图像如下： img 通过图像可以看出，使用一个较大的$ \\gamma $值，会使得类别0核类别1的决策边界变得紧凑了许多。 虽然模型对训练数据的你和很好，但是类似的分类器对未知数据会有较大的泛化误差。 决策树基于训练集的特征，决策树模型通过一系列的问题来推断样本的类标。 img 从树根开始，基于可获得最大信息增益（Information Gain, IG）的特征来对数据进行划分，通过迭代处理，在每个节点重复此过程，直到叶子节点。 最大化信息增益—-获知尽可能准确的结果就目前来说，大多数的库中实现的树算法都是二叉决策树。二叉决策树中常用的三个不纯度衡量标准或者划分标准分别是：基尼系数（Gini index, $ I_G $），熵（entropy， $ I_H $）以及误分类率（classification error， $ I_E $）。 非空类别熵的定义是$$I_H(t) = -\\sum_{i=1}^c p(i|t)\\log_2p(i|t)$$其中，$ p(i|t) $为在特定节点t中，属于类别i的样本占特定样本t中样本总数的比例。如果某一个节点中所有样本都属于同一个类别，那么它的熵是0，当样本以相同的比例分属于不同的类时，熵的值最大。 直观地说，基尼系数可以理解为降低误分类可能性的标准：$$I_G(t) = \\sum_{i=1}^{c}p(i|t)(1-p(i|t)) = 1 - \\sum_{i=1}^c p(i|t)^2$$和熵类似，当所有样本时等比例分布时，基尼系数的值最大。 误分类率的定义如下：$$I_E = 1 - max{p(i|t)}$$这是对于剪枝方法很有用的准则，但不建议用于决策树的构建过程，因为它对节点中各类别样本数量的变动不敏感。 构建决策树通过使用scikit-learn来构建一颗二叉决策树，需要注意的是，决策树的深度不是越大越好，深度过大的决策树，很容易产生过拟合的现象。在此，构建一棵深度是3的决策树： 1234567891011from sklearn.tree import DecisionTreeClassifiertree = DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=3, random_state=0)tree.fit(X_train, y_train)X_combined = np.vstack((X_train, X_test))y_combined = np.hstack((y_train, y_test))plot_decision_regions(X_combined, y_combined, classifier=tree, test_idx=range(105, 150))plt.xlabel(&#x27;length&#x27;)plt.ylabel(&#x27;width&#x27;)plt.legend()plt.show() 得到的决策边界如下： img 通过随机森林将弱分类器集成为强分类器直观上，随机森林可以看作是多颗决策树的集成。随机森林算法可以概括为一下几个步骤： 使用bootstrap抽样方法随机选择 n 个样本用于训练（从训练集中随机可重复选择n个样本） 使用第 1 步选定的样本构建一棵决策树，节点划分如下： 不重复选择d个特征 根据目标函数的要求，使用选定的特征对节点进行划分 重复上述过程1~2000次 汇总每棵树的类标进行多数投票 使用scikit-learn来实现随机森林： 123456789from sklearn.ensemble import RandomForestClassifierforest = RandomForestClassifier(criterion=&#x27;entropy&#x27;, n_estimators=10, random_state=1, n_jobs=2)forest.fit(X_train, y_train)plot_decision_regions(X_combined, y_combined, classifier=forest, test_idx=range(105, 150))plt.xlabel(&#x27;length&#x27;)plt.ylabel(&#x27;width&#x27;)plt.legend()plt.show() 得到的决策区域如下： img 上述代码中，我们以熵作为不纯度衡量标准，且使用了10棵决策树进行随机森林的训练，同时我们还规定算法中使用的处理器内核数量为2。 惰性学习算法之k-近临算法k-近临算法（k-nearest neighbor classifier，KNN）是惰性学习算法的典型例子。KNN算法本身是简单的，可以归纳为以下几步： 选择近临数量k和距离衡量方法 找到待分类样本的k个最近邻居 根据最近临的类标进行多数投票 下图说明了当k=3时，范围内红色三角形多，这个待分类点属于红色三角形；当K = 5 时，范围内蓝色正方形多，这个待分类点属于蓝色正方形。 img KNN算法可以快速适应新的训练数据，不过它的缺点也是显而易见的，在最坏情况下，计算复杂度随着样本的增多而线性增长。 接下来使用scikit-learn实现KNN模型，在此，我们选择欧几里得距离作为度量标准： 123456789from sklearn.neighbors import KNeighborsClassifierknn = KNeighborsClassifier(n_neighbors=5, p=2, metric=&#x27;minkowski&#x27;)knn.fit(X_train_std, y_train)plot_decision_regions(X_combined_std, y_combined, classifier=knn, test_idx=range(105, 150))plt.xlabel(&#x27;length&#x27;)plt.ylabel(&#x27;width&#x27;)plt.legend()plt.show() 得到的决策区域如下： img 在代码中用到的“闵可夫斯基（minkowski）”距离是对欧几里得距离和曼哈顿距离的一种泛化，可写作：$$d(x^i, x^j) = \\sqrt[p]{\\sum_k|x^i_kx^j_k|^p}$$如果将参数p设置为2，那么就是欧几里得距离，设置为1，则为曼哈顿距离。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"机器学习分类算法","slug":"机器学习分类算法","date":"2020-01-26T08:53:55.000Z","updated":"2020-12-17T10:23:43.619Z","comments":true,"path":"2020/01/26/机器学习分类算法/","link":"","permalink":"http://blog.zsstrike.top/2020/01/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/","excerpt":"介绍最早以算法方式描述的分类机器学习算法：感知器和自适应线性神经元。同时我们也会使用Python来实现一个感知器。","text":"介绍最早以算法方式描述的分类机器学习算法：感知器和自适应线性神经元。同时我们也会使用Python来实现一个感知器。 早期机器学习概述罗森布拉特基于MCP模型提出了第一个感知器学习法则。在这个感知器规则中，他提出了一个自学习算法，此算法通过优化得到权重系数，此系数和输入值的乘机决定了神经元是否被激活。在监督学习中，类似算法可以用于预测样本所属的类别。 我们将类别问题看作是二值分类问题，为了简单起见，分为1(正类别)和**-1（负类别）。同时定义一个激励函数$ \\phi(z) $，这个函数将会以特定的输入值x和相应的权值向量w**的线性组合作为输入，也就是说：$ z=w_1x_1 + \\cdots + w_nx_n $。此时定义法治函数为阶跃函数：$$\\begin{equation}\\phi(z)=\\begin{cases} 1, &amp; z \\ge \\theta \\ -1, &amp; z \\lt \\theta\\end{cases}\\end{equation}$$其中，我们称$\\theta$是阈值。 为了简单起见，可以将阈值移动到等式的左边，同时初始权重是$w_0=-\\theta$同时$x_0=1$，这样激励函数就变为$$\\begin{equation}\\phi(z)=\\begin{cases} 1, &amp; z \\ge 0 \\ -1, &amp; z \\lt 0\\end{cases}\\end{equation}$$同时感知器最初的规则很简单，可以归纳为如下几步： 将权重初始化为零或者一个极小的随机数。 迭代所有训练样本$x^i$,执行如下操作： 计算输出值$\\hat{y}$。 更新权值。 每次对权重向量$w$的更新方式为：$$w_j:=w_j+\\Delta{w_j}$$而对于$\\Delta{w_j}$,可以通过感知器的学习规划计算获得：$$\\Delta{w_j}=\\eta(y^i-\\hat{y^i})x_j^i$$其中，$\\eta$是学习速率，一个在0到1之间的常数，$y^i$是第i个样本的真实样标，$\\hat{y^i}$是相应的预测值。对于一个二维数据，可以得到如下更新公式：$$\\Delta{w_0} = \\eta(y^i-\\hat{output^i})\\\\Delta{w_1} = \\eta(y^i-\\hat{output^i})x_1^i\\\\cdots$$需要注意的是，感知器收敛的前提是两个类别必须是线性可分的，并且学习速率足够小。 感应器的模型如下： img 使用Python实现感知器学习算法使用面向对象的方法实现感知器的接口，同时按照Python开发惯例，对于那些并非在初始化对象时创建但是又被对象中其他方法调用的属性，可以在后面加上一个下划线，如self.w_。 Python算法如下： 12345678910111213141516171819202122232425import numpy as npclass Perceptron(object): def __init__(self, eta=0.01, n_iter=10): self.eta = eta self.n_iter = n_iter def fit(self, X, y): self.w_ = np.zeros(1 + X.shape[1]) self.errors_ = [] for _ in range(self.n_iter): errors = 0 for xi, target in zip(X, y): update = self.eta * (target - self.predict(xi)) self.w_[1:] += update * xi self.w_[0] += update errors += int(update != 0.0) self.errors_.append(errors) return self def net_input(self, X): return np.dot(X, self.w[1:]) + self.w_[0] def predict(self, X): return np.where(self.net_input(X) &gt;= 0.0, 1, -1) 接下来就可以使用相关的数据集来训练我们的感知器模型。 首先我们从pandas库直接从UCI机器学习库中将鸢尾花数据集转化为DataFrame对象并且加载到内存中，并且使用tail方法显示数据确保数据加载正确。 1234import pandas as pddf = pd.read_csv(&#x27;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&#x27;, header=None)df.tail() 0 1 2 3 4 145 6.7 3.0 5.2 2.3 Iris-virginica 146 6.3 2.5 5.0 1.9 Iris-virginica 147 6.5 3.0 5.2 2.0 Iris-virginica 148 6.2 3.4 5.4 2.3 Iris-virginica 149 5.9 3.0 5.1 1.8 Iris-virginica 鸢尾花数据集时机器学习中一个经典的实例，它包含了Setosa，Versicolor和Virginica三个品种总共150个鸢尾花的测量数据，每个品种的数量是50个。每个数据项包括序号，萼片长度，萼片宽度，花瓣长度，花瓣宽度，类标。 接下来提取前100个类标，其中分别包含50个山鸢尾类标和50个变色鸢尾类标，并且将变色鸢尾表示为1，山鸢尾表示为-1。同时提取萼片长度和花瓣长度作为输入变量$X$。 首先可视化$X$： 123456789101112import matplotlib.pyplot as pltimport numpy as npy = df.iloc[0:100, 4].valuesy = np.where(y == &#x27;Iris-setosa&#x27;, -1, 1)X = df.iloc[0:100, [0, 2]].valuesplt.scatter(X[:50, 0], X[:50, 1], color=&#x27;red&#x27;, marker=&#x27;o&#x27;, label=&#x27;setosa&#x27;)plt.scatter(X[50:100, 0], X[50:100, 1], color=&#x27;blue&#x27;, marker=&#x27;x&#x27;, label=&#x27;versicolor&#x27;)plt.xlabel(&#x27;petal length&#x27;)plt.ylabel(&#x27;sepal length&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show() 可视化的图形如下： img 现在，我们可以使用提取出的数据来训练我们的感知器了。同时我们还会绘制每次迭代的错误分类数量的折线图，以检验算法是否收敛并且找到决策边界。 123456ppn = Perceptron(eta=0.1, n_iter=10)ppn.fit(X, y)plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=&#x27;o&#x27;)plt.xlabel(&#x27;Epochs&#x27;)plt.ylabel(&#x27;Number of misclassifications&#x27;)plt.show() 得到的每次迭代的错误数量折线图如下： img 如上图所示，我们的迭代器在第六次的时候就已经收敛，下面通过一个简单的函数实现二维数据决策边界的可视化。 1234567891011121314151617181920212223from matplotlib.colors import ListedColormapdef plot_decision_regions(X, y, classifier, resolution=0.02): # setup marker generator and color map markers = (&#x27;s&#x27;, &#x27;x&#x27;, &#x27;o&#x27;, &#x27;^&#x27;, &#x27;v&#x27;) colors = (&#x27;red&#x27;, &#x27;blue&#x27;, &#x27;lightgreen&#x27;, &#x27;gray&#x27;, &#x27;cyan&#x27;) cmap = ListedColormap(colors[:len(np.unique(y))]) # plt the decision surface x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot class sample for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl) 接着调用该函数就可以画出图像： 12345plot_decision_regions(X, y, classifier=ppn)plt.xlabel(&#x27;sepal length&#x27;)plt.ylabel(&#x27;petal length&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show() img 通过图中可看出来，感知器能够对训练集中的所有数据进行正确的分类。 自适应线性神经元和学习的收敛性在感知器算法出现之后，又有人提出了Adaline算法，这个算法可以看作是对之前算法的改进。 基于Adaline规则的权重更新是通过一个连续的线性激励函数来完成的，而不像感知器中使用单位阶跃函数，这是二者主要的区别。在Adaline是算法中，激励函数是简单的恒等函数，即$ \\phi(w^Tx)=w^Tx $。线性激励函数在更新权重的同时，我们使用量化器对类标进行预测，量化器和前面提到的单位阶跃函数类似，如下图所示： img 对比前面的感知器算法模型而可以得到差别：使用线性激励函数的连续型输出值，而不是二类别分类类标来计算模型的误差以及更新权重。 通过梯度下降最小化代价函数在Adaline中，我们可以定义代价函数$J$为通过模型得到的输出值和实际值之间的误差平方和：$$J(w) = \\frac{1}{2}\\sum_i(y^i-\\phi(z^i))^2$$这里，系数1/2是为了方便的角度，是我们容易求梯度。这个代价函数是一个凸函数，这样我们可以通过简单高效的梯度下降优化算法得到权重，并且保证对训练数据进行分类时代价最小。 如下图所示，我们将梯度学习的原理形象地描述为下山： img 这样，权重更新公式如下：$$w:=w+\\Delta{w}$$对应的权重增量$ \\Delta{w} $定义为下：$$\\Delta{w}=-\\eta\\Delta{J(w)}$$化简得到：$$\\Delta{w_j} = \\eta\\sum_i(y^i-\\phi(z^i))x^i_j$$这样权重的更行是根据训练集中所有数据完成的，而不是每次一个样本渐进更新权重，这也是该方法被称为批量下降的原因。 使用Python实现自适应线性神经元我们将会根据感知器模型的代码来复写我们的Adaline模型，如下： 1234567891011121314151617181920212223242526class AdalineGD(object): def __init__(self, eta=0.01, n_iter=50): self.eta = eta self.n_iter = n_iter def fit(self, X, y): self.w_ = np.zeros(1 + X.shape[1]) self.cost_ = [] for i in range(self.n_iter): output = self.net_input(X) errors = (y - output) self.w_[1:] += self.eta * X.T.dot(errors) self.w_[0] += self.eta * errors.sum() cost = (errors**2).sum()/2.0 self.cost_.append(cost) return self def net_input(self, X): return np.dot(X, self.w_[1:]) + self.w_[0] def activation(self, X): return self.net_input(X) def predict(self, X): return np.where(self.activation(X) &gt;= 0.0, 1, -1) 感知器通过self.eta * errors.sum()来更新第0个位置的权重，通过self.eta * X.T.dot(errors)来更新第1到m个位置的权重，同时，我们设置一个列表self.cost_用于追踪本轮训练的误差值。 实践中，我们常常需要进行调参的工作，我们分别让$\\eta = 0.1$和$\\eta = 0.0001$来训练，同时绘制迭代次数和代价函数的图像，观察Adaline通过数据训练进行学习的效果。 1234567891011121314fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8,4))ada1 = AdalineGD(n_iter=10, eta=0.1).fit(X, y)ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker=&#x27;o&#x27;)ax[0].set_xlabel(&#x27;Epochs&#x27;)ax[0].set_ylabel(&#x27;log(SSE)&#x27;)ax[0].set_title(&#x27;Adaline - 0.1&#x27;)ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker=&#x27;o&#x27;)ax[1].set_xlabel(&#x27;Epochs&#x27;)ax[1].set_ylabel(&#x27;log(SSE)&#x27;)ax[1].set_title(&#x27;Adaline - 0.0001&#x27;)plt.show() 图像如下： img 从图中可以看出，我们面临着这两种问题：左边图像显示了学习速率过大可能会导致并没有使得代价函数尽可能低，反而因为算法跳过了全局最优解，导致误差越来越大；右边图像虽然代价函数逐渐减少，但是学习速率太小，使得到算法收敛的目标需要更多次数的迭代。 为了提高算法优化的性能，我们将使用特征缩放的方法，也就是：$$x_j^{‘}=\\frac{x_j - \\mu_j}{\\sigma_j}$$标准化可以通过Numpy的mean和std方法实现： 123X_std = np.copy(X)X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std() 接下来，按照$\\eta = 0.01$的学习速率来对Adaline进行训练： 123456789101112ada = AdalineGD(n_iter=15, eta=0.01)ada.fit(X_std, y)plot_decision_regions(X_std, y, classifier=ada)plt.title(&#x27;Adaline - 0.01&#x27;)plt.xlabel(&#x27;sepal length&#x27;)plt.ylabel(&#x27;petal length&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show()plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker=&#x27;o&#x27;)plt.xlabel(&#x27;Epochs&#x27;)plt.ylabel(&#x27;SSE&#x27;)plt.show() 得到的图像如下： img img 如上图，虽然所有样本都被正确分类，但是误差平方和（SSE）的值仍然不为零。 大规模机器学习和随机梯度下降上一节中，我们对所有的数据进行计算，利用计算出来的结果来实现权重的更新。但是，机器学习面临的数据往往是包含着几百万数据的巨大数据集，这种情况下使用批量梯度下降的计算成本很高。 为了解决这个问题，我们引入随机梯度下降，和基于所有样本的累计误差更新权重的策略不同，我们每次使用一个训练样本渐进地更新权重：$$\\eta(y^i-\\phi(z^i))x^i$$ 当实现梯度随即下降时，通常规定学习速率如下：$$\\eta = \\frac{c_1}{c_2 + [迭代次数]}$$ 为了让随机梯度下降得到更加准确的结果，让数据以随机的方式提供给算法时很重要的，因此，我们需要在每次迭代的时候打乱训练集。 随机梯度下降的另外一个优势是我们可以将其用于在线学习。通过在线学习，当有新的数据输入时模型会被实时训练。 小批次学习：介于梯度下降和随机梯度下降之间的一种技术。将数据分成一组组的训练数据，在对每组数进行训练。 随机梯度下降的Adaline算法如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859from numpy.random import seedclass AdalineSGD(object): def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None): self.eta = eta self.n_iter = n_iter self.w_initialized = False self.shuffle = shuffle if random_state: seed(random_state) def fit(self, X, y): self._initialize_weights(X.shape[1]) self.cost_ = [] for i in range(self.n_iter): if self.shuffle: X, y = self._shuffle(X, y) cost = [] for xi, target in zip(X, y): cost.append(self._update_weights(xi, target)) avg_cost = sum(cost) / len(y) self.cost_.append(avg_cost) return self def partial_fit(self, X, y): if not self.w_initialized: self._initialize_weights(X.shape[1]) if y.ravel().shape[0] &gt; 1: for xi, target in zip(X, y): self._update_weights(xi, target) else: self._update_weights(X, y) return self def _shuffle(self, X, y): r = np.random.permutation(len(y)) return X[r], y[r] def _initialize_weights(self, m): self.w_ = np.zeros(1 + m) self.w_initialized = True def _update_weights(self, xi, target): output = self.net_input(xi) errors = (target - output) self.w_[1:] += self.eta * xi.dot(errors) self.w_[0] += self.eta * errors cost = 0.5 * errors**2 return cost def net_input(self, X): return np.dot(X, self.w_[1:]) + self.w_[0] def activation(self, X): return self.net_input(X) def predict(self, X): return np.where(self.activation(X) &gt;= 0.0, 1, -1) 绘图程序如下： 123456789101112ada = AdalineSGD(n_iter=15, eta=0.01, random_state=1)ada.fit(X_std, y)plot_decision_regions(X_std, y, classifier=ada)plt.title(&#x27;Adaline - 0.01&#x27;)plt.xlabel(&#x27;sepal length&#x27;)plt.ylabel(&#x27;petal length&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show()plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker=&#x27;o&#x27;)plt.xlabel(&#x27;Epochs&#x27;)plt.ylabel(&#x27;Average Cost&#x27;)plt.show() 图像如下： img img 可以发现，代价函数的均值下降得很快，经过15次迭代后，基本趋势和梯度下降得到的图像类似。 如果改进模型，如用于数据流的在线学习，可以对单个样本简单调用partial_fit方法，如：ada.partial_fit(X_std[0, :], y[0])。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"赋予计算机学习数据的能力","slug":"赋予计算机学习数据的能力","date":"2020-01-24T04:07:05.000Z","updated":"2020-12-17T10:23:43.741Z","comments":true,"path":"2020/01/24/赋予计算机学习数据的能力/","link":"","permalink":"http://blog.zsstrike.top/2020/01/24/%E8%B5%8B%E4%BA%88%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE%E7%9A%84%E8%83%BD%E5%8A%9B/","excerpt":"","text":"主要了解机器学习的主要概念和几种不同类型的学习算法。 机器学习的三种方法监督学习使用有类标的数据构建模型，使用经训练得到的模型对未来的数据进行预测。预测方法主要有两种： 使用分类进行预测 分类是监督学习的一个子类，目的是对过往已知示例的观察与学习，实现对新样本类标的预测。检测垃圾邮件的例子就是一种二类标的方法，当然还有多类别分类的例子，比如字母表中每个字母的识别。 使用回归预测连续输出值 这种方法用于对连续型输出变量进行预测，比如学习成绩分数和自习时间多少之间进行预测。 强化（半监督）学习强化学习的目标在于构建一个系统，在于环境的交互过程中逐步提高系统的性能。环境的当前状态中通常包含者一个反馈信号。常用的例子是象棋对弈的例子，在这个例子中，系统根据棋盘上的局势（环境）来决定落子的位置，而游戏结束的输赢可以当做是反馈信号。 无监督学习在这种学习方法下，我们将会处理无类标数据或者是总体趋势不明朗的数据，来提取出有效信息探索数据的整体结构。 通过聚类发现数据的子群 聚类是一种探索性的数据分析技术，在没有任何相关先验信息的情况下，它可以帮我们将数据分成有意义的小组别。 数据压缩中的降维 讲高维数据压缩，是之转变为相对容易处理的维度数据。 机器学习系统的蓝图数据预处理为了尽可能发回机器学习算法的性能，往往需要对原始数据进行处理使得它能达到算法要求的标准，同时选择较高关联的属性作为训练数据。 选择预测模型类型并进行训练和校正选择合适的机器学习算法来对训练数据集进行学习得到模型，同时利用反馈信号来对模型进行校正。 使用未知数据进行预测对未来的数据进行预测。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"git 常见错误","slug":"git-常见错误","date":"2019-10-27T12:32:26.000Z","updated":"2020-12-17T10:23:43.556Z","comments":true,"path":"2019/10/27/git-常见错误/","link":"","permalink":"http://blog.zsstrike.top/2019/10/27/git-%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/","excerpt":"","text":"整理在使用 git 过程中遇到的问题以及解决方法。 shallow update not allowed这个问题的产生原因是在克隆远程仓库的时候采用了以下命令： 1git clone --depth=&lt;num&gt; &lt;remote-url&gt; 这将会导致shallow clone(浅复制)。这将会使得这个仓库不能向远程仓库进行push。通过以下命令可修复： 1git fetch --unshallow &lt;remote-repo&gt;","categories":[],"tags":[{"name":"git","slug":"git","permalink":"http://blog.zsstrike.top/tags/git/"}]}],"categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://blog.zsstrike.top/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"MySQL","slug":"MySQL","permalink":"http://blog.zsstrike.top/tags/MySQL/"},{"name":"Java","slug":"Java","permalink":"http://blog.zsstrike.top/tags/Java/"},{"name":"分布式","slug":"分布式","permalink":"http://blog.zsstrike.top/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.zsstrike.top/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Hexo","slug":"Hexo","permalink":"http://blog.zsstrike.top/tags/Hexo/"},{"name":"Typora","slug":"Typora","permalink":"http://blog.zsstrike.top/tags/Typora/"},{"name":"Redis","slug":"Redis","permalink":"http://blog.zsstrike.top/tags/Redis/"},{"name":"CI/CD","slug":"CI-CD","permalink":"http://blog.zsstrike.top/tags/CI-CD/"},{"name":"Nodejs","slug":"Nodejs","permalink":"http://blog.zsstrike.top/tags/Nodejs/"},{"name":"Vim","slug":"Vim","permalink":"http://blog.zsstrike.top/tags/Vim/"},{"name":"Linux","slug":"Linux","permalink":"http://blog.zsstrike.top/tags/Linux/"},{"name":"Python","slug":"Python","permalink":"http://blog.zsstrike.top/tags/Python/"},{"name":"Nginx","slug":"Nginx","permalink":"http://blog.zsstrike.top/tags/Nginx/"},{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://blog.zsstrike.top/tags/TensorFlow/"},{"name":"SSR","slug":"SSR","permalink":"http://blog.zsstrike.top/tags/SSR/"},{"name":"git","slug":"git","permalink":"http://blog.zsstrike.top/tags/git/"}]}